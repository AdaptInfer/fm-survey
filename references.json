[
  {
    "id": "jbY4pWUH",
    "type": "paper-conference",
    "title": "Early Drift Detection Method",
    "author": [
      {
        "family": "Baena-García",
        "given": "Manuel"
      },
      {
        "family": "del Campo-Ávila",
        "given": "José"
      },
      {
        "family": "Fidalgo",
        "given": "Raul"
      },
      {
        "family": "Bifet",
        "given": "Albert"
      },
      {
        "family": "Gavalda",
        "given": "Ricard"
      },
      {
        "family": "Morales-Bueno",
        "given": "Rafael"
      }
    ],
    "container-title": "Fourth International Workshop on Knowledge Discovery from Data Streams",
    "volume": "6",
    "page": "77–86",
    "issued": {
      "date-parts": [
        [
          2006
        ]
      ]
    },
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: Baena2006EarlyDrift"
  },
  {
    "id": "SldOo5zA",
    "type": "article-journal",
    "title": "A Markovian Decision Process",
    "author": [
      {
        "family": "Bellman",
        "given": "Richard"
      }
    ],
    "container-title": "Journal of Mathematics and Mechanics",
    "page": "679–684",
    "publisher": "JSTOR",
    "issued": {
      "date-parts": [
        [
          1957
        ]
      ]
    },
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: Bellman1957Markovian"
  },
  {
    "id": "18GHyIdqs",
    "type": "book",
    "title": "Statistical Methods for Research Workers",
    "author": [
      {
        "family": "Fisher",
        "given": "Ronald A."
      }
    ],
    "publisher": "Oliver & Boyd",
    "publisher-place": "Edinburgh",
    "issued": {
      "date-parts": [
        [
          1925
        ]
      ]
    },
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: Fisher1925Research"
  },
  {
    "id": "17ryvnoPR",
    "type": "book",
    "title": "The Design of Experiments",
    "author": [
      {
        "family": "Fisher",
        "given": "Ronald A."
      }
    ],
    "publisher": "Oliver & Boyd",
    "issued": {
      "date-parts": [
        [
          1949
        ]
      ]
    },
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: Fisher1949Design"
  },
  {
    "id": "vKtb27Ba",
    "type": "book",
    "title": "Theoria motus corporum coelestium in sectionibus conicis solem ambientium",
    "author": [
      {
        "family": "Gauss",
        "given": "Carl Friedrich"
      }
    ],
    "publisher": "Perthes & Besser",
    "publisher-place": "Hamburg",
    "issued": {
      "date-parts": [
        [
          1809
        ]
      ]
    },
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: Gauss1809Ambientium"
  },
  {
    "id": "1GKMYvnqB",
    "type": "article-journal",
    "title": "Estimation of Genetic Parameters",
    "author": [
      {
        "family": "Henderson",
        "given": "Charles R."
      }
    ],
    "container-title": "Annals of Mathematical Statistics",
    "volume": "21",
    "issue": "2",
    "page": "309–310",
    "issued": {
      "date-parts": [
        [
          1950
        ]
      ]
    },
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: Henderson1950Genetic"
  },
  {
    "id": "8BCLzHII",
    "type": "book",
    "title": "Nouvelles méthodes pour la détermination des orbites des comètes",
    "author": [
      {
        "family": "Legendre",
        "given": "Adrien-Marie"
      }
    ],
    "publisher": "Courcier",
    "publisher-place": "Paris",
    "issued": {
      "date-parts": [
        [
          1805
        ]
      ]
    },
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: Legendre1805Comets"
  },
  {
    "id": "jlS72VrS",
    "type": "book",
    "title": "Reinforcement Learning: An Introduction",
    "author": [
      {
        "family": "Sutton",
        "given": "Richard S."
      },
      {
        "family": "Barto",
        "given": "Andrew G."
      }
    ],
    "publisher": "MIT Press",
    "publisher-place": "Cambridge, MA",
    "ISBN": "978-0-262-19398-6",
    "issued": {
      "date-parts": [
        [
          1998
        ]
      ]
    },
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: Sutton1998RLIntroduction"
  },
  {
    "id": "jggKMQFt",
    "type": "thesis",
    "title": "Learning from Delayed Rewards",
    "author": [
      {
        "family": "Watkins",
        "given": "Christopher John Cornish Hellaby"
      }
    ],
    "publisher": "King's College, University of Cambridge",
    "genre": "PhD thesis",
    "issued": {
      "date-parts": [
        [
          1989
        ]
      ]
    },
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: Watkins1989Learning"
  },
  {
    "id": "NLVTJ9Lj",
    "URL": "https://arxiv.org/abs/1312.6114",
    "number": "1312.6114",
    "title": "Auto-Encoding Variational Bayes",
    "issued": {
      "date-parts": [
        [
          2022,
          12,
          13
        ]
      ]
    },
    "author": [
      {
        "given": "Diederik P",
        "family": "Kingma"
      },
      {
        "given": "Max",
        "family": "Welling"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions are two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.",
    "note": "license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/\nThis CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: arxiv:1312.6114"
  },
  {
    "id": "JhS4KMR7",
    "URL": "https://arxiv.org/abs/1501.01332",
    "number": "1501.01332",
    "title": "Causal inference using invariant prediction: identification and confidence intervals",
    "issued": {
      "date-parts": [
        [
          2024,
          4,
          26
        ]
      ]
    },
    "author": [
      {
        "given": "Jonas",
        "family": "Peters"
      },
      {
        "given": "Peter",
        "family": "BÃ¼hlmann"
      },
      {
        "given": "Nicolai",
        "family": "Meinshausen"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "What is the difference of a prediction that is made with a causal model and a non-causal model? Suppose we intervene on the predictor variables or change the whole environment. The predictions from a causal model will in general work as well under interventions as for observational data. In contrast, predictions from a non-causal model can potentially be very wrong if we actively intervene on variables. Here, we propose to exploit this invariance of a prediction under a causal model for causal inference: given different experimental settings (for example various interventions) we collect all models that do show invariance in their predictive accuracy across settings and interventions. The causal model will be a member of this set of models with high probability. This approach yields valid confidence intervals for the causal relationships in quite general scenarios. We examine the example of structural equation models in more detail and provide sufficient assumptions under which the set of causal predictors becomes identifiable. We further investigate robustness properties of our approach under model misspecification and discuss possible extensions. The empirical properties are studied for various data sets, including large-scale gene perturbation experiments.",
    "note": "license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/\nThis CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: arxiv:1501.01332"
  },
  {
    "id": "1HqcEkGab",
    "URL": "https://arxiv.org/abs/1702.08734",
    "number": "1702.08734",
    "title": "Billion-scale similarity search with GPUs",
    "issued": {
      "date-parts": [
        [
          2018,
          6,
          7
        ]
      ]
    },
    "author": [
      {
        "given": "Jeff",
        "family": "Johnson"
      },
      {
        "given": "Matthijs",
        "family": "Douze"
      },
      {
        "given": "HervÃ©",
        "family": "JÃ©gou"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "Similarity search finds application in specialized database systems handling complex data such as images or videos, which are typically represented by high-dimensional features and require specific indexing structures. This paper tackles the problem of better utilizing GPUs for this task. While GPUs excel at data-parallel tasks, prior approaches are bottlenecked by algorithms that expose less parallelism, such as k-min selection, or make poor use of the memory hierarchy.\n  We propose a design for k-selection that operates at up to 55% of theoretical peak performance, enabling a nearest neighbor implementation that is 8.5x faster than prior GPU state of the art. We apply it in different similarity search scenarios, by proposing optimized design for brute-force, approximate and compressed-domain search based on product quantization. In all these setups, we outperform the state of the art by large margins. Our implementation enables the construction of a high accuracy k-NN graph on 95 million images from the Yfcc100M dataset in 35 minutes, and of a graph connecting 1 billion vectors in less than 12 hours on 4 Maxwell Titan X GPUs. We have open-sourced our approach for the sake of comparison and reproducibility.",
    "note": "license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/\nThis CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: arxiv:1702.08734"
  },
  {
    "id": "B0L8KJ8W",
    "URL": "https://arxiv.org/abs/1706.06083",
    "number": "1706.06083",
    "title": "Towards Deep Learning Models Resistant to Adversarial Attacks",
    "issued": {
      "date-parts": [
        [
          2019,
          9,
          6
        ]
      ]
    },
    "author": [
      {
        "given": "Aleksander",
        "family": "Madry"
      },
      {
        "given": "Aleksandar",
        "family": "Makelov"
      },
      {
        "given": "Ludwig",
        "family": "Schmidt"
      },
      {
        "given": "Dimitris",
        "family": "Tsipras"
      },
      {
        "given": "Adrian",
        "family": "Vladu"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "Recent work has demonstrated that deep neural networks are vulnerable to adversarial examples---inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. Code and pre-trained models are available at https://github.com/MadryLab/mnist_challenge and https://github.com/MadryLab/cifar10_challenge.",
    "note": "license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/\nThis CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: arxiv:1706.06083"
  },
  {
    "id": "L6xa6qzg",
    "URL": "https://arxiv.org/abs/1710.11469",
    "number": "1710.11469",
    "title": "Conditional Variance Penalties and Domain Shift Robustness",
    "issued": {
      "date-parts": [
        [
          2019,
          4,
          16
        ]
      ]
    },
    "author": [
      {
        "given": "Christina",
        "family": "Heinze-Deml"
      },
      {
        "given": "Nicolai",
        "family": "Meinshausen"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "When training a deep neural network for image classification, one can broadly distinguish between two types of latent features of images that will drive the classification. We can divide latent features into (i) \"core\" or \"conditionally invariant\" features $X^\\text{core}$ whose distribution $X^\\text{core}\\vert Y$, conditional on the class $Y$, does not change substantially across domains and (ii) \"style\" features $X^{\\text{style}}$ whose distribution $X^{\\text{style}} \\vert Y$ can change substantially across domains. Examples for style features include position, rotation, image quality or brightness but also more complex ones like hair color, image quality or posture for images of persons. Our goal is to minimize a loss that is robust under changes in the distribution of these style features. In contrast to previous work, we assume that the domain itself is not observed and hence a latent variable.\n  We do assume that we can sometimes observe a typically discrete identifier or \"$\\mathrm{ID}$ variable\". In some applications we know, for example, that two images show the same person, and $\\mathrm{ID}$ then refers to the identity of the person. The proposed method requires only a small fraction of images to have $\\mathrm{ID}$ information. We group observations if they share the same class and identifier $(Y,\\mathrm{ID})=(y,\\mathrm{id})$ and penalize the conditional variance of the prediction or the loss if we condition on $(Y,\\mathrm{ID})$. Using a causal framework, this conditional variance regularization (CoRe) is shown to protect asymptotically against shifts in the distribution of the style variables. Empirically, we show that the CoRe penalty improves predictive accuracy substantially in settings where domain changes occur in terms of image quality, brightness and color while we also look at more complex changes such as changes in movement and posture.",
    "note": "license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/\nThis CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: arxiv:1710.11469"
  },
  {
    "id": "ylSNfYug",
    "URL": "https://arxiv.org/abs/1805.12152",
    "number": "1805.12152",
    "title": "Robustness May Be at Odds with Accuracy",
    "issued": {
      "date-parts": [
        [
          2019,
          9,
          10
        ]
      ]
    },
    "author": [
      {
        "given": "Dimitris",
        "family": "Tsipras"
      },
      {
        "given": "Shibani",
        "family": "Santurkar"
      },
      {
        "given": "Logan",
        "family": "Engstrom"
      },
      {
        "given": "Alexander",
        "family": "Turner"
      },
      {
        "given": "Aleksander",
        "family": "Madry"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "We show that there may exist an inherent tension between the goal of adversarial robustness and that of standard generalization. Specifically, training robust models may not only be more resource-consuming, but also lead to a reduction of standard accuracy. We demonstrate that this trade-off between the standard accuracy of a model and its robustness to adversarial perturbations provably exists in a fairly simple and natural setting. These findings also corroborate a similar phenomenon observed empirically in more complex settings. Further, we argue that this phenomenon is a consequence of robust classifiers learning fundamentally different feature representations than standard classifiers. These differences, in particular, seem to result in unexpected benefits: the representations learned by robust models tend to align better with salient data characteristics and human perception.",
    "note": "license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/\nThis CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: arxiv:1805.12152"
  },
  {
    "id": "dJTnXBAv",
    "URL": "https://arxiv.org/abs/1806.07572",
    "number": "1806.07572",
    "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
    "issued": {
      "date-parts": [
        [
          2020,
          2,
          11
        ]
      ]
    },
    "author": [
      {
        "given": "Arthur",
        "family": "Jacot"
      },
      {
        "given": "Franck",
        "family": "Gabriel"
      },
      {
        "given": "ClÃ©ment",
        "family": "Hongler"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "At initialization, artificial neural networks (ANNs) are equivalent to Gaussian processes in the infinite-width limit, thus connecting them to kernel methods. We prove that the evolution of an ANN during training can also be described by a kernel: during gradient descent on the parameters of an ANN, the network function $f_Î¸$ (which maps input vectors to output vectors) follows the kernel gradient of the functional cost (which is convex, in contrast to the parameter cost) w.r.t. a new kernel: the Neural Tangent Kernel (NTK). This kernel is central to describe the generalization features of ANNs. While the NTK is random at initialization and varies during training, in the infinite-width limit it converges to an explicit limiting kernel and it stays constant during training. This makes it possible to study the training of ANNs in function space instead of parameter space. Convergence of the training can then be related to the positive-definiteness of the limiting NTK. We prove the positive-definiteness of the limiting NTK when the data is supported on the sphere and the non-linearity is non-polynomial. We then focus on the setting of least-squares regression and show that in the infinite-width limit, the network function $f_Î¸$ follows a linear differential equation during training. The convergence is fastest along the largest kernel principal components of the input data with respect to the NTK, hence suggesting a theoretical motivation for early stopping. Finally we study the NTK numerically, observe its behavior for wide networks, and compare it to the infinite-width limit.",
    "note": "license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/\nThis CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: arxiv:1806.07572"
  },
  {
    "id": "l5C1P3il",
    "URL": "https://arxiv.org/abs/1905.02175",
    "number": "1905.02175",
    "title": "Adversarial Examples Are Not Bugs, They Are Features",
    "issued": {
      "date-parts": [
        [
          2019,
          8,
          13
        ]
      ]
    },
    "author": [
      {
        "given": "Andrew",
        "family": "Ilyas"
      },
      {
        "given": "Shibani",
        "family": "Santurkar"
      },
      {
        "given": "Dimitris",
        "family": "Tsipras"
      },
      {
        "given": "Logan",
        "family": "Engstrom"
      },
      {
        "given": "Brandon",
        "family": "Tran"
      },
      {
        "given": "Aleksander",
        "family": "Madry"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "Adversarial examples have attracted significant attention in machine learning, but the reasons for their existence and pervasiveness remain unclear. We demonstrate that adversarial examples can be directly attributed to the presence of non-robust features: features derived from patterns in the data distribution that are highly predictive, yet brittle and incomprehensible to humans. After capturing these features within a theoretical framework, we establish their widespread existence in standard datasets. Finally, we present a simple setting where we can rigorously tie the phenomena we observe in practice to a misalignment between the (human-specified) notion of robustness and the inherent geometry of the data.",
    "note": "license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/\nThis CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: arxiv:1905.02175"
  },
  {
    "id": "1DeCFT6PA",
    "URL": "https://arxiv.org/abs/1907.02893",
    "number": "1907.02893",
    "title": "Invariant Risk Minimization",
    "issued": {
      "date-parts": [
        [
          2020,
          3,
          31
        ]
      ]
    },
    "author": [
      {
        "given": "Martin",
        "family": "Arjovsky"
      },
      {
        "given": "LÃ©on",
        "family": "Bottou"
      },
      {
        "given": "Ishaan",
        "family": "Gulrajani"
      },
      {
        "given": "David",
        "family": "Lopez-Paz"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "We introduce Invariant Risk Minimization (IRM), a learning paradigm to estimate invariant correlations across multiple training distributions. To achieve this goal, IRM learns a data representation such that the optimal classifier, on top of that data representation, matches for all training distributions. Through theory and experiments, we show how the invariances learned by IRM relate to the causal structures governing the data and enable out-of-distribution generalization.",
    "note": "license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/\nThis CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: arxiv:1907.02893"
  },
  {
    "id": "Jm8Kx8HW",
    "URL": "https://arxiv.org/abs/1911.08731",
    "number": "1911.08731",
    "title": "Distributionally Robust Neural Networks for Group Shifts: On the Importance of Regularization for Worst-Case Generalization",
    "issued": {
      "date-parts": [
        [
          2020,
          4,
          3
        ]
      ]
    },
    "author": [
      {
        "given": "Shiori",
        "family": "Sagawa"
      },
      {
        "given": "Pang Wei",
        "family": "Koh"
      },
      {
        "given": "Tatsunori B.",
        "family": "Hashimoto"
      },
      {
        "given": "Percy",
        "family": "Liang"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "Overparameterized neural networks can be highly accurate on average on an i.i.d. test set yet consistently fail on atypical groups of the data (e.g., by learning spurious correlations that hold on average but not in such groups). Distributionally robust optimization (DRO) allows us to learn models that instead minimize the worst-case training loss over a set of pre-defined groups. However, we find that naively applying group DRO to overparameterized neural networks fails: these models can perfectly fit the training data, and any model with vanishing average training loss also already has vanishing worst-case training loss. Instead, the poor worst-case performance arises from poor generalization on some groups. By coupling group DRO models with increased regularization---a stronger-than-typical L2 penalty or early stopping---we achieve substantially higher worst-group accuracies, with 10-40 percentage point improvements on a natural language inference task and two image tasks, while maintaining high average accuracies. Our results suggest that regularization is important for worst-group generalization in the overparameterized regime, even if it is not needed for average generalization. Finally, we introduce a stochastic optimization algorithm, with convergence guarantees, to efficiently train group DRO models.",
    "note": "license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/\nThis CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: arxiv:1911.08731"
  },
  {
    "id": "ELrcyIqv",
    "URL": "https://arxiv.org/abs/1912.02803",
    "number": "1912.02803",
    "title": "Neural Tangents: Fast and Easy Infinite Neural Networks in Python",
    "issued": {
      "date-parts": [
        [
          2019,
          12,
          6
        ]
      ]
    },
    "author": [
      {
        "given": "Roman",
        "family": "Novak"
      },
      {
        "given": "Lechao",
        "family": "Xiao"
      },
      {
        "given": "Jiri",
        "family": "Hron"
      },
      {
        "given": "Jaehoon",
        "family": "Lee"
      },
      {
        "given": "Alexander A.",
        "family": "Alemi"
      },
      {
        "given": "Jascha",
        "family": "Sohl-Dickstein"
      },
      {
        "given": "Samuel S.",
        "family": "Schoenholz"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "Neural Tangents is a library designed to enable research into infinite-width neural networks. It provides a high-level API for specifying complex and hierarchical neural network architectures. These networks can then be trained and evaluated either at finite-width as usual or in their infinite-width limit. Infinite-width networks can be trained analytically using exact Bayesian inference or using gradient descent via the Neural Tangent Kernel. Additionally, Neural Tangents provides tools to study gradient descent training dynamics of wide but finite networks in either function space or weight space.\n  The entire library runs out-of-the-box on CPU, GPU, or TPU. All computations can be automatically distributed over multiple accelerators with near-linear scaling in the number of devices. Neural Tangents is available at www.github.com/google/neural-tangents. We also provide an accompanying interactive Colab notebook.",
    "note": "license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/\nThis CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: arxiv:1912.02803"
  },
  {
    "id": "11IMWGprl",
    "URL": "https://arxiv.org/abs/2003.00688",
    "number": "2003.00688",
    "title": "Out-of-Distribution Generalization via Risk Extrapolation (REx)",
    "issued": {
      "date-parts": [
        [
          2021,
          2,
          26
        ]
      ]
    },
    "author": [
      {
        "given": "David",
        "family": "Krueger"
      },
      {
        "given": "Ethan",
        "family": "Caballero"
      },
      {
        "given": "Joern-Henrik",
        "family": "Jacobsen"
      },
      {
        "given": "Amy",
        "family": "Zhang"
      },
      {
        "given": "Jonathan",
        "family": "Binas"
      },
      {
        "given": "Dinghuai",
        "family": "Zhang"
      },
      {
        "given": "Remi Le",
        "family": "Priol"
      },
      {
        "given": "Aaron",
        "family": "Courville"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "Distributional shift is one of the major obstacles when transferring machine learning prediction systems from the lab to the real world. To tackle this problem, we assume that variation across training domains is representative of the variation we might encounter at test time, but also that shifts at test time may be more extreme in magnitude. In particular, we show that reducing differences in risk across training domains can reduce a model's sensitivity to a wide range of extreme distributional shifts, including the challenging setting where the input contains both causal and anti-causal elements. We motivate this approach, Risk Extrapolation (REx), as a form of robust optimization over a perturbation set of extrapolated domains (MM-REx), and propose a penalty on the variance of training risks (V-REx) as a simpler variant. We prove that variants of REx can recover the causal mechanisms of the targets, while also providing some robustness to changes in the input distribution (\"covariate shift\"). By appropriately trading-off robustness to causally induced distributional shifts and covariate shift, REx is able to outperform alternative methods such as Invariant Risk Minimization in situations where these types of shift co-occur.",
    "note": "license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/\nThis CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: arxiv:2003.00688"
  },
  {
    "id": "1FLMzrLE9",
    "URL": "https://arxiv.org/abs/2010.05761",
    "number": "2010.05761",
    "title": "The Risks of Invariant Risk Minimization",
    "issued": {
      "date-parts": [
        [
          2021,
          3,
          30
        ]
      ]
    },
    "author": [
      {
        "given": "Elan",
        "family": "Rosenfeld"
      },
      {
        "given": "Pradeep",
        "family": "Ravikumar"
      },
      {
        "given": "Andrej",
        "family": "Risteski"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "Invariant Causal Prediction (Peters et al., 2016) is a technique for out-of-distribution generalization which assumes that some aspects of the data distribution vary across the training set but that the underlying causal mechanisms remain constant. Recently, Arjovsky et al. (2019) proposed Invariant Risk Minimization (IRM), an objective based on this idea for learning deep, invariant features of data which are a complex function of latent variables; many alternatives have subsequently been suggested. However, formal guarantees for all of these works are severely lacking. In this paper, we present the first analysis of classification under the IRM objective--as well as these recently proposed alternatives--under a fairly natural and general model. In the linear case, we show simple conditions under which the optimal solution succeeds or, more often, fails to recover the optimal invariant predictor. We furthermore present the very first results in the non-linear regime: we demonstrate that IRM can fail catastrophically unless the test data are sufficiently similar to the training distribution--this is precisely the issue that it was intended to solve. Thus, in this setting we find that IRM and its alternatives fundamentally do not improve over standard Empirical Risk Minimization.",
    "note": "license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/\nThis CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: arxiv:2010.05761"
  },
  {
    "id": "10151coVE",
    "URL": "https://arxiv.org/abs/2010.07249",
    "number": "2010.07249",
    "title": "Environment Inference for Invariant Learning",
    "issued": {
      "date-parts": [
        [
          2021,
          7,
          16
        ]
      ]
    },
    "author": [
      {
        "given": "Elliot",
        "family": "Creager"
      },
      {
        "given": "JÃ¶rn-Henrik",
        "family": "Jacobsen"
      },
      {
        "given": "Richard",
        "family": "Zemel"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "Learning models that gracefully handle distribution shifts is central to research on domain generalization, robust optimization, and fairness. A promising formulation is domain-invariant learning, which identifies the key issue of learning which features are domain-specific versus domain-invariant. An important assumption in this area is that the training examples are partitioned into \"domains\" or \"environments\". Our focus is on the more common setting where such partitions are not provided. We propose EIIL, a general framework for domain-invariant learning that incorporates Environment Inference to directly infer partitions that are maximally informative for downstream Invariant Learning. We show that EIIL outperforms invariant learning methods on the CMNIST benchmark without using environment labels, and significantly outperforms ERM on worst-group performance in the Waterbirds and CivilComments datasets. Finally, we establish connections between EIIL and algorithmic fairness, which enables EIIL to improve accuracy and calibration in a fair prediction problem.",
    "note": "license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/\nThis CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: arxiv:2010.07249"
  },
  {
    "id": "1CkxORTSX",
    "URL": "https://arxiv.org/abs/2103.00315",
    "number": "2103.00315",
    "title": "Time-Varying Coefficient Model Estimation Through Radial Basis Functions",
    "issued": {
      "date-parts": [
        [
          2021,
          3,
          2
        ]
      ]
    },
    "author": [
      {
        "given": "Juan",
        "family": "Sosa"
      },
      {
        "given": "Lina",
        "family": "Buitrago"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "In this paper we estimate the dynamic parameters of a time-varying coefficient model through radial kernel functions in the context of a longitudinal study. Our proposal is based on a linear combination of weighted kernel functions involving a bandwidth, centered around a given set of time points. In addition, we study different alternatives of estimation and inference including a Frequentist approach using weighted least squares along with bootstrap methods, and a Bayesian approach through both Markov chain Monte Carlo and variational methods. We compare the estimation strategies mention above with each other, and our radial kernel functions proposal with an expansion based on regression spline, by means of an extensive simulation study considering multiples scenarios in terms of sample size, number of repeated measurements, and subject-specific correlation. Our experiments show that the capabilities of our proposal based on radial kernel functions are indeed comparable with or even better than those obtained from regression splines. We illustrate our methodology by analyzing data from two AIDS clinical studies.",
    "note": "license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/\nThis CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: arxiv:2103.00315"
  },
  {
    "id": "HzzgJQN0",
    "URL": "https://arxiv.org/abs/2106.04486",
    "number": "2106.04486",
    "title": "Sketch-Based Anomaly Detection in Streaming Graphs",
    "issued": {
      "date-parts": [
        [
          2023,
          7,
          18
        ]
      ]
    },
    "author": [
      {
        "given": "Siddharth",
        "family": "Bhatia"
      },
      {
        "given": "Mohit",
        "family": "Wadhwa"
      },
      {
        "given": "Kenji",
        "family": "Kawaguchi"
      },
      {
        "given": "Neil",
        "family": "Shah"
      },
      {
        "given": "Philip S.",
        "family": "Yu"
      },
      {
        "given": "Bryan",
        "family": "Hooi"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "Given a stream of graph edges from a dynamic graph, how can we assign anomaly scores to edges and subgraphs in an online manner, for the purpose of detecting unusual behavior, using constant time and memory? For example, in intrusion detection, existing work seeks to detect either anomalous edges or anomalous subgraphs, but not both. In this paper, we first extend the count-min sketch data structure to a higher-order sketch. This higher-order sketch has the useful property of preserving the dense subgraph structure (dense subgraphs in the input turn into dense submatrices in the data structure). We then propose 4 online algorithms that utilize this enhanced data structure, which (a) detect both edge and graph anomalies; (b) process each edge and graph in constant memory and constant update time per newly arriving edge, and; (c) outperform state-of-the-art baselines on 4 real-world datasets. Our method is the first streaming approach that incorporates dense subgraph search to detect graph anomalies in constant memory and time.",
    "note": "license: http://creativecommons.org/licenses/by/4.0/\nThis CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: arxiv:2106.04486"
  },
  {
    "id": "EXaywYVO",
    "URL": "https://arxiv.org/abs/2107.09044",
    "number": "2107.09044",
    "title": "Just Train Twice: Improving Group Robustness without Training Group Information",
    "issued": {
      "date-parts": [
        [
          2021,
          9,
          28
        ]
      ]
    },
    "author": [
      {
        "given": "Evan Zheran",
        "family": "Liu"
      },
      {
        "given": "Behzad",
        "family": "Haghgoo"
      },
      {
        "given": "Annie S.",
        "family": "Chen"
      },
      {
        "given": "Aditi",
        "family": "Raghunathan"
      },
      {
        "given": "Pang Wei",
        "family": "Koh"
      },
      {
        "given": "Shiori",
        "family": "Sagawa"
      },
      {
        "given": "Percy",
        "family": "Liang"
      },
      {
        "given": "Chelsea",
        "family": "Finn"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "Standard training via empirical risk minimization (ERM) can produce models that achieve high accuracy on average but low accuracy on certain groups, especially in the presence of spurious correlations between the input and label. Prior approaches that achieve high worst-group accuracy, like group distributionally robust optimization (group DRO) require expensive group annotations for each training point, whereas approaches that do not use such group annotations typically achieve unsatisfactory worst-group accuracy. In this paper, we propose a simple two-stage approach, JTT, that first trains a standard ERM model for several epochs, and then trains a second model that upweights the training examples that the first model misclassified. Intuitively, this upweights examples from groups on which standard ERM models perform poorly, leading to improved worst-group performance. Averaged over four image classification and natural language processing tasks with spurious correlations, JTT closes 75% of the gap in worst-group accuracy between standard ERM and group DRO, while only requiring group annotations on a small validation set in order to tune hyperparameters.",
    "note": "license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/\nThis CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: arxiv:2107.09044"
  },
  {
    "id": "fo6ln654",
    "URL": "https://arxiv.org/abs/2208.01066",
    "number": "2208.01066",
    "title": "What Can Transformers Learn In-Context? A Case Study of Simple Function Classes",
    "issued": {
      "date-parts": [
        [
          2023,
          8,
          15
        ]
      ]
    },
    "author": [
      {
        "given": "Shivam",
        "family": "Garg"
      },
      {
        "given": "Dimitris",
        "family": "Tsipras"
      },
      {
        "given": "Percy",
        "family": "Liang"
      },
      {
        "given": "Gregory",
        "family": "Valiant"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "In-context learning refers to the ability of a model to condition on a prompt sequence consisting of in-context examples (input-output pairs corresponding to some task) along with a new query input, and generate the corresponding output. Crucially, in-context learning happens only at inference time without any parameter updates to the model. While large language models such as GPT-3 exhibit some ability to perform in-context learning, it is unclear what the relationship is between tasks on which this succeeds and what is present in the training data. To make progress towards understanding in-context learning, we consider the well-defined problem of training a model to in-context learn a function class (e.g., linear functions): that is, given data derived from some functions in the class, can we train a model to in-context learn \"most\" functions from this class? We show empirically that standard Transformers can be trained from scratch to perform in-context learning of linear functions -- that is, the trained model is able to learn unseen linear functions from in-context examples with performance comparable to the optimal least squares estimator. In fact, in-context learning is possible even under two forms of distribution shift: (i) between the training data of the model and inference-time prompts, and (ii) between the in-context examples and the query input during inference. We also show that we can train Transformers to in-context learn more complex function classes -- namely sparse linear functions, two-layer neural networks, and decision trees -- with performance that matches or exceeds task-specific learning algorithms. Our code and models are available at https://github.com/dtsip/in-context-learning .",
    "note": "license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/\nThis CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: arxiv:2208.01066"
  },
  {
    "id": "Humo13Op",
    "URL": "https://arxiv.org/abs/2212.07677",
    "number": "2212.07677",
    "title": "Transformers learn in-context by gradient descent",
    "issued": {
      "date-parts": [
        [
          2023,
          6,
          1
        ]
      ]
    },
    "author": [
      {
        "given": "Johannes",
        "family": "von Oswald"
      },
      {
        "given": "Eyvind",
        "family": "Niklasson"
      },
      {
        "given": "Ettore",
        "family": "Randazzo"
      },
      {
        "given": "JoÃ£o",
        "family": "Sacramento"
      },
      {
        "given": "Alexander",
        "family": "Mordvintsev"
      },
      {
        "given": "Andrey",
        "family": "Zhmoginov"
      },
      {
        "given": "Max",
        "family": "Vladymyrov"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "At present, the mechanisms of in-context learning in Transformers are not well understood and remain mostly an intuition. In this paper, we suggest that training Transformers on auto-regressive objectives is closely related to gradient-based meta-learning formulations. We start by providing a simple weight construction that shows the equivalence of data transformations induced by 1) a single linear self-attention layer and by 2) gradient-descent (GD) on a regression loss. Motivated by that construction, we show empirically that when training self-attention-only Transformers on simple regression tasks either the models learned by GD and Transformers show great similarity or, remarkably, the weights found by optimization match the construction. Thus we show how trained Transformers become mesa-optimizers i.e. learn models by gradient descent in their forward pass. This allows us, at least in the domain of regression problems, to mechanistically understand the inner workings of in-context learning in optimized Transformers. Building on this insight, we furthermore identify how Transformers surpass the performance of plain gradient descent by learning an iterative curvature correction and learn linear models on deep data representations to solve non-linear regression tasks. Finally, we discuss intriguing parallels to a mechanism identified to be crucial for in-context learning termed induction-head (Olsson et al., 2022) and show how it could be understood as a specific case of in-context learning by gradient descent learning within Transformers. Code to reproduce the experiments can be found at https://github.com/google-research/self-organising-systems/tree/master/transformers_learn_icl_by_gd .",
    "note": "license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/\nThis CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: arxiv:2212.07677"
  },
  {
    "id": "PZHprt8G",
    "URL": "https://arxiv.org/abs/2212.10559",
    "number": "2212.10559",
    "title": "Why Can GPT Learn In-Context? Language Models Implicitly Perform Gradient Descent as Meta-Optimizers",
    "issued": {
      "date-parts": [
        [
          2023,
          5,
          16
        ]
      ]
    },
    "author": [
      {
        "given": "Damai",
        "family": "Dai"
      },
      {
        "given": "Yutao",
        "family": "Sun"
      },
      {
        "given": "Li",
        "family": "Dong"
      },
      {
        "given": "Yaru",
        "family": "Hao"
      },
      {
        "given": "Shuming",
        "family": "Ma"
      },
      {
        "given": "Zhifang",
        "family": "Sui"
      },
      {
        "given": "Furu",
        "family": "Wei"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "Large pretrained language models have shown surprising in-context learning (ICL) ability. With a few demonstration input-label pairs, they can predict the label for an unseen input without parameter updates. Despite the great success in performance, its working mechanism still remains an open question. In this paper, we explain language models as meta-optimizers and understand in-context learning as implicit finetuning. Theoretically, we figure out that Transformer attention has a dual form of gradient descent. On top of it, we understand ICL as follows: GPT first produces meta-gradients according to the demonstration examples, and then these meta-gradients are applied to the original GPT to build an ICL model. We comprehensively compare the behaviors of in-context learning and explicit finetuning on real tasks to provide empirical evidence that supports our understanding. Experimental results show that in-context learning behaves similarly to explicit finetuning from multiple perspectives. Inspired by the dual form between Transformer attention and gradient descent, we design a momentum-based attention by analogy with gradient descent with momentum. The improved performance over vanilla attention further supports our understanding from another perspective, and more importantly, shows the potential to utilize our understanding for future model design. The code is available at \\url{https://aka.ms/icl}.",
    "note": "license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/\nThis CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: arxiv:2212.10559"
  },
  {
    "id": "xoOsuDeH",
    "URL": "https://arxiv.org/abs/2303.02781v1",
    "number": "2303.02781v1",
    "version": "v1",
    "title": "Robustness, Evaluation and Adaptation of Machine Learning Models in the Wild",
    "issued": {
      "date-parts": [
        [
          2023,
          3,
          5
        ]
      ]
    },
    "author": [
      {
        "literal": "Vihari Piratla"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "Our goal is to improve reliability of Machine Learning (ML) systems deployed in the wild. ML models perform exceedingly well when test examples are similar to train examples. However, real-world applications are required to perform on any distribution of test examples. Current ML systems can fail silently on test examples with distribution shifts. In order to improve reliability of ML models due to covariate or domain shift, we propose algorithms that enable models to: (a) generalize to a larger family of test distributions, (b) evaluate accuracy under distribution shifts, (c) adapt to a target distribution. We study causes of impaired robustness to domain shifts and present algorithms for training domain robust models. A key source of model brittleness is due to domain overfitting, which our new training algorithms suppress and instead encourage domain-general hypotheses. While we improve robustness over standard training methods for certain problem settings, performance of ML systems can still vary drastically with domain shifts. It is crucial for developers and stakeholders to understand model vulnerabilities and operational ranges of input, which could be assessed on the fly during the deployment, albeit at a great cost. Instead, we advocate for proactively estimating accuracy surfaces over any combination of prespecified and interpretable domain shifts for performance forecasting. We present a label-efficient estimation to address estimation over a combinatorial space of domain shifts. Further, when a model's performance on a target domain is found to be poor, traditional approaches adapt the model using the target domain's resources. Standard adaptation methods assume access to sufficient labeled resources, which may be impractical for deployed models. We initiate a study of lightweight adaptation techniques with only unlabeled data resources with a focus on language applications.",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: arxiv:2303.02781v1"
  },
  {
    "id": "PF464FGD",
    "URL": "https://arxiv.org/abs/2309.06180",
    "number": "2309.06180",
    "title": "Efficient Memory Management for Large Language Model Serving with PagedAttention",
    "issued": {
      "date-parts": [
        [
          2023,
          9,
          13
        ]
      ]
    },
    "author": [
      {
        "given": "Woosuk",
        "family": "Kwon"
      },
      {
        "given": "Zhuohan",
        "family": "Li"
      },
      {
        "given": "Siyuan",
        "family": "Zhuang"
      },
      {
        "given": "Ying",
        "family": "Sheng"
      },
      {
        "given": "Lianmin",
        "family": "Zheng"
      },
      {
        "given": "Cody Hao",
        "family": "Yu"
      },
      {
        "given": "Joseph E.",
        "family": "Gonzalez"
      },
      {
        "given": "Hao",
        "family": "Zhang"
      },
      {
        "given": "Ion",
        "family": "Stoica"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "High throughput serving of large language models (LLMs) requires batching sufficiently many requests at a time. However, existing systems struggle because the key-value cache (KV cache) memory for each request is huge and grows and shrinks dynamically. When managed inefficiently, this memory can be significantly wasted by fragmentation and redundant duplication, limiting the batch size. To address this problem, we propose PagedAttention, an attention algorithm inspired by the classical virtual memory and paging techniques in operating systems. On top of it, we build vLLM, an LLM serving system that achieves (1) near-zero waste in KV cache memory and (2) flexible sharing of KV cache within and across requests to further reduce memory usage. Our evaluations show that vLLM improves the throughput of popular LLMs by 2-4$\\times$ with the same level of latency compared to the state-of-the-art systems, such as FasterTransformer and Orca. The improvement is more pronounced with longer sequences, larger models, and more complex decoding algorithms. vLLM's source code is publicly available at https://github.com/vllm-project/vllm",
    "note": "license: http://creativecommons.org/licenses/by/4.0/\nThis CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: arxiv:2309.06180"
  },
  {
    "id": "CYfSdHYg",
    "URL": "https://arxiv.org/abs/2309.07864",
    "number": "2309.07864",
    "title": "The Rise and Potential of Large Language Model Based Agents: A Survey",
    "issued": {
      "date-parts": [
        [
          2023,
          9,
          20
        ]
      ]
    },
    "author": [
      {
        "given": "Zhiheng",
        "family": "Xi"
      },
      {
        "given": "Wenxiang",
        "family": "Chen"
      },
      {
        "given": "Xin",
        "family": "Guo"
      },
      {
        "given": "Wei",
        "family": "He"
      },
      {
        "given": "Yiwen",
        "family": "Ding"
      },
      {
        "given": "Boyang",
        "family": "Hong"
      },
      {
        "given": "Ming",
        "family": "Zhang"
      },
      {
        "given": "Junzhe",
        "family": "Wang"
      },
      {
        "given": "Senjie",
        "family": "Jin"
      },
      {
        "given": "Enyu",
        "family": "Zhou"
      },
      {
        "given": "Rui",
        "family": "Zheng"
      },
      {
        "given": "Xiaoran",
        "family": "Fan"
      },
      {
        "given": "Xiao",
        "family": "Wang"
      },
      {
        "given": "Limao",
        "family": "Xiong"
      },
      {
        "given": "Yuhao",
        "family": "Zhou"
      },
      {
        "given": "Weiran",
        "family": "Wang"
      },
      {
        "given": "Changhao",
        "family": "Jiang"
      },
      {
        "given": "Yicheng",
        "family": "Zou"
      },
      {
        "given": "Xiangyang",
        "family": "Liu"
      },
      {
        "given": "Zhangyue",
        "family": "Yin"
      },
      {
        "given": "Shihan",
        "family": "Dou"
      },
      {
        "given": "Rongxiang",
        "family": "Weng"
      },
      {
        "given": "Wensen",
        "family": "Cheng"
      },
      {
        "given": "Qi",
        "family": "Zhang"
      },
      {
        "given": "Wenjuan",
        "family": "Qin"
      },
      {
        "given": "Yongyan",
        "family": "Zheng"
      },
      {
        "given": "Xipeng",
        "family": "Qiu"
      },
      {
        "given": "Xuanjing",
        "family": "Huang"
      },
      {
        "given": "Tao",
        "family": "Gui"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "For a long time, humanity has pursued artificial intelligence (AI) equivalent to or surpassing the human level, with AI agents considered a promising vehicle for this pursuit. AI agents are artificial entities that sense their environment, make decisions, and take actions. Many efforts have been made to develop intelligent agents, but they mainly focus on advancement in algorithms or training strategies to enhance specific capabilities or performance on particular tasks. Actually, what the community lacks is a general and powerful model to serve as a starting point for designing AI agents that can adapt to diverse scenarios. Due to the versatile capabilities they demonstrate, large language models (LLMs) are regarded as potential sparks for Artificial General Intelligence (AGI), offering hope for building general AI agents. Many researchers have leveraged LLMs as the foundation to build AI agents and have achieved significant progress. In this paper, we perform a comprehensive survey on LLM-based agents. We start by tracing the concept of agents from its philosophical origins to its development in AI, and explain why LLMs are suitable foundations for agents. Building upon this, we present a general framework for LLM-based agents, comprising three main components: brain, perception, and action, and the framework can be tailored for different applications. Subsequently, we explore the extensive applications of LLM-based agents in three aspects: single-agent scenarios, multi-agent scenarios, and human-agent cooperation. Following this, we delve into agent societies, exploring the behavior and personality of LLM-based agents, the social phenomena that emerge from an agent society, and the insights they offer for human society. Finally, we discuss several key topics and open problems within the field. A repository for the related papers at https://github.com/WooooDyy/LLM-Agent-Paper-List.",
    "note": "license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/\nThis CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: arxiv:2309.07864"
  },
  {
    "id": "D4deReQm",
    "URL": "https://arxiv.org/abs/2309.17453",
    "number": "2309.17453",
    "title": "Efficient Streaming Language Models with Attention Sinks",
    "issued": {
      "date-parts": [
        [
          2024,
          4,
          9
        ]
      ]
    },
    "author": [
      {
        "given": "Guangxuan",
        "family": "Xiao"
      },
      {
        "given": "Yuandong",
        "family": "Tian"
      },
      {
        "given": "Beidi",
        "family": "Chen"
      },
      {
        "given": "Song",
        "family": "Han"
      },
      {
        "given": "Mike",
        "family": "Lewis"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "Deploying Large Language Models (LLMs) in streaming applications such as multi-round dialogue, where long interactions are expected, is urgently needed but poses two major challenges. Firstly, during the decoding stage, caching previous tokens' Key and Value states (KV) consumes extensive memory. Secondly, popular LLMs cannot generalize to longer texts than the training sequence length. Window attention, where only the most recent KVs are cached, is a natural approach -- but we show that it fails when the text length surpasses the cache size. We observe an interesting phenomenon, namely attention sink, that keeping the KV of initial tokens will largely recover the performance of window attention. In this paper, we first demonstrate that the emergence of attention sink is due to the strong attention scores towards initial tokens as a \"sink\" even if they are not semantically important. Based on the above analysis, we introduce StreamingLLM, an efficient framework that enables LLMs trained with a finite length attention window to generalize to infinite sequence lengths without any fine-tuning. We show that StreamingLLM can enable Llama-2, MPT, Falcon, and Pythia to perform stable and efficient language modeling with up to 4 million tokens and more. In addition, we discover that adding a placeholder token as a dedicated attention sink during pre-training can further improve streaming deployment. In streaming settings, StreamingLLM outperforms the sliding window recomputation baseline by up to 22.2x speedup. Code and datasets are provided at https://github.com/mit-han-lab/streaming-llm.",
    "note": "license: http://creativecommons.org/licenses/by/4.0/\nThis CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: arxiv:2309.17453"
  },
  {
    "id": "1BPbcGQGq",
    "URL": "https://arxiv.org/abs/2402.19473",
    "number": "2402.19473",
    "title": "Retrieval-Augmented Generation for AI-Generated Content: A Survey",
    "issued": {
      "date-parts": [
        [
          2024,
          6,
          24
        ]
      ]
    },
    "author": [
      {
        "given": "Penghao",
        "family": "Zhao"
      },
      {
        "given": "Hailin",
        "family": "Zhang"
      },
      {
        "given": "Qinhan",
        "family": "Yu"
      },
      {
        "given": "Zhengren",
        "family": "Wang"
      },
      {
        "given": "Yunteng",
        "family": "Geng"
      },
      {
        "given": "Fangcheng",
        "family": "Fu"
      },
      {
        "given": "Ling",
        "family": "Yang"
      },
      {
        "given": "Wentao",
        "family": "Zhang"
      },
      {
        "given": "Jie",
        "family": "Jiang"
      },
      {
        "given": "Bin",
        "family": "Cui"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "Advancements in model algorithms, the growth of foundational models, and access to high-quality datasets have propelled the evolution of Artificial Intelligence Generated Content (AIGC). Despite its notable successes, AIGC still faces hurdles such as updating knowledge, handling long-tail data, mitigating data leakage, and managing high training and inference costs. Retrieval-Augmented Generation (RAG) has recently emerged as a paradigm to address such challenges. In particular, RAG introduces the information retrieval process, which enhances the generation process by retrieving relevant objects from available data stores, leading to higher accuracy and better robustness. In this paper, we comprehensively review existing efforts that integrate RAG technique into AIGC scenarios. We first classify RAG foundations according to how the retriever augments the generator, distilling the fundamental abstractions of the augmentation methodologies for various retrievers and generators. This unified perspective encompasses all RAG scenarios, illuminating advancements and pivotal technologies that help with potential future progress. We also summarize additional enhancements methods for RAG, facilitating effective engineering and implementation of RAG systems. Then from another view, we survey on practical applications of RAG across different modalities and tasks, offering valuable references for researchers and practitioners. Furthermore, we introduce the benchmarks for RAG, discuss the limitations of current RAG systems, and suggest potential directions for future research. Github: https://github.com/PKU-DAIR/RAG-Survey.",
    "note": "license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/\nThis CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: arxiv:2402.19473"
  },
  {
    "id": "gfPki3PM",
    "URL": "https://arxiv.org/abs/2406.12274",
    "number": "2406.12274",
    "title": "SafeInfer: Context Adaptive Decoding Time Safety Alignment for Large Language Models",
    "issued": {
      "date-parts": [
        [
          2024,
          12,
          17
        ]
      ]
    },
    "author": [
      {
        "given": "Somnath",
        "family": "Banerjee"
      },
      {
        "given": "Sayan",
        "family": "Layek"
      },
      {
        "given": "Soham",
        "family": "Tripathy"
      },
      {
        "given": "Shanu",
        "family": "Kumar"
      },
      {
        "given": "Animesh",
        "family": "Mukherjee"
      },
      {
        "given": "Rima",
        "family": "Hazra"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "Safety-aligned language models often exhibit fragile and imbalanced safety mechanisms, increasing the likelihood of generating unsafe content. In addition, incorporating new knowledge through editing techniques to language models can further compromise safety. To address these issues, we propose SafeInfer, a context-adaptive, decoding-time safety alignment strategy for generating safe responses to user queries. SafeInfer comprises two phases: the safety amplification phase, which employs safe demonstration examples to adjust the model's hidden states and increase the likelihood of safer outputs, and the safety-guided decoding phase, which influences token selection based on safety-optimized distributions, ensuring the generated content complies with ethical guidelines. Further, we present HarmEval, a novel benchmark for extensive safety evaluations, designed to address potential misuse scenarios in accordance with the policies of leading AI tech giants.",
    "note": "license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/\nThis CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: arxiv:2406.12274"
  },
  {
    "id": "RkqVE8TP",
    "URL": "https://arxiv.org/abs/2409.08354",
    "number": "2409.08354",
    "title": "Bayesian Dynamic Factor Models for High-dimensional Matrix-valued Time Series",
    "issued": {
      "date-parts": [
        [
          2025,
          8,
          11
        ]
      ]
    },
    "author": [
      {
        "given": "Wei",
        "family": "Zhang"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "We introduce a class of Bayesian matrix dynamic factor models that accommodates time-varying volatility, outliers, and cross-sectional correlation in the idiosyncratic components. For model comparison, we employ an importance-sampling estimator of the marginal likelihood based on the cross-entropy method to determine: (1) the optimal dimension of the factor matrix; (2) whether a vector- or matrix-valued structure is more suitable; and (3) whether an approximate or exact factor model is favored by the data. Through a series of Monte Carlo experiments, we demonstrate the accuracy of the factor estimates and the effectiveness of the marginal likelihood estimator in correctly identifying the true model. Applications to macroeconomic and financial datasets illustrate the model's ability to capture key features in matrix-valued time series.",
    "note": "license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/\nThis CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: arxiv:2409.08354"
  },
  {
    "id": "yApXu0Vp",
    "URL": "https://arxiv.org/abs/2410.12837",
    "number": "2410.12837",
    "title": "A Comprehensive Survey of Retrieval-Augmented Generation (RAG): Evolution, Current Landscape and Future Directions",
    "issued": {
      "date-parts": [
        [
          2024,
          10,
          18
        ]
      ]
    },
    "author": [
      {
        "given": "Shailja",
        "family": "Gupta"
      },
      {
        "given": "Rajesh",
        "family": "Ranjan"
      },
      {
        "given": "Surya Narayan",
        "family": "Singh"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "This paper presents a comprehensive study of Retrieval-Augmented Generation (RAG), tracing its evolution from foundational concepts to the current state of the art. RAG combines retrieval mechanisms with generative language models to enhance the accuracy of outputs, addressing key limitations of LLMs. The study explores the basic architecture of RAG, focusing on how retrieval and generation are integrated to handle knowledge-intensive tasks. A detailed review of the significant technological advancements in RAG is provided, including key innovations in retrieval-augmented language models and applications across various domains such as question-answering, summarization, and knowledge-based tasks. Recent research breakthroughs are discussed, highlighting novel methods for improving retrieval efficiency. Furthermore, the paper examines ongoing challenges such as scalability, bias, and ethical concerns in deployment. Future research directions are proposed, focusing on improving the robustness of RAG models, expanding the scope of application of RAG models, and addressing societal implications. This survey aims to serve as a foundational resource for researchers and practitioners in understanding the potential of RAG and its trajectory in natural language processing.",
    "note": "license: http://creativecommons.org/licenses/by-nc-nd/4.0/\nThis CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: arxiv:2410.12837"
  },
  {
    "id": "EsknayJ2",
    "URL": "https://arxiv.org/abs/2504.15965",
    "number": "2504.15965",
    "title": "From Human Memory to AI Memory: A Survey on Memory Mechanisms in the Era of LLMs",
    "issued": {
      "date-parts": [
        [
          2025,
          4,
          24
        ]
      ]
    },
    "author": [
      {
        "given": "Yaxiong",
        "family": "Wu"
      },
      {
        "given": "Sheng",
        "family": "Liang"
      },
      {
        "given": "Chen",
        "family": "Zhang"
      },
      {
        "given": "Yichao",
        "family": "Wang"
      },
      {
        "given": "Yongyue",
        "family": "Zhang"
      },
      {
        "given": "Huifeng",
        "family": "Guo"
      },
      {
        "given": "Ruiming",
        "family": "Tang"
      },
      {
        "given": "Yong",
        "family": "Liu"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "Memory is the process of encoding, storing, and retrieving information, allowing humans to retain experiences, knowledge, skills, and facts over time, and serving as the foundation for growth and effective interaction with the world. It plays a crucial role in shaping our identity, making decisions, learning from past experiences, building relationships, and adapting to changes. In the era of large language models (LLMs), memory refers to the ability of an AI system to retain, recall, and use information from past interactions to improve future responses and interactions. Although previous research and reviews have provided detailed descriptions of memory mechanisms, there is still a lack of a systematic review that summarizes and analyzes the relationship between the memory of LLM-driven AI systems and human memory, as well as how we can be inspired by human memory to construct more powerful memory systems. To achieve this, in this paper, we propose a comprehensive survey on the memory of LLM-driven AI systems. In particular, we first conduct a detailed analysis of the categories of human memory and relate them to the memory of AI systems. Second, we systematically organize existing memory-related work and propose a categorization method based on three dimensions (object, form, and time) and eight quadrants. Finally, we illustrate some open problems regarding the memory of current AI systems and outline possible future directions for memory in the era of large language models.",
    "note": "license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/\nThis CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: arxiv:2504.15965"
  },
  {
    "id": "QciOeUHY",
    "URL": "https://arxiv.org/abs/2506.06326",
    "number": "2506.06326",
    "title": "Memory OS of AI Agent",
    "issued": {
      "date-parts": [
        [
          2025,
          6,
          10
        ]
      ]
    },
    "author": [
      {
        "given": "Jiazheng",
        "family": "Kang"
      },
      {
        "given": "Mingming",
        "family": "Ji"
      },
      {
        "given": "Zhe",
        "family": "Zhao"
      },
      {
        "given": "Ting",
        "family": "Bai"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "Large Language Models (LLMs) face a crucial challenge from fixed context windows and inadequate memory management, leading to a severe shortage of long-term memory capabilities and limited personalization in the interactive experience with AI agents. To overcome this challenge, we innovatively propose a Memory Operating System, i.e., MemoryOS, to achieve comprehensive and efficient memory management for AI agents. Inspired by the memory management principles in operating systems, MemoryOS designs a hierarchical storage architecture and consists of four key modules: Memory Storage, Updating, Retrieval, and Generation. Specifically, the architecture comprises three levels of storage units: short-term memory, mid-term memory, and long-term personal memory. Key operations within MemoryOS include dynamic updates between storage units: short-term to mid-term updates follow a dialogue-chain-based FIFO principle, while mid-term to long-term updates use a segmented page organization strategy. Our pioneering MemoryOS enables hierarchical memory integration and dynamic updating. Extensive experiments on the LoCoMo benchmark show an average improvement of 49.11% on F1 and 46.18% on BLEU-1 over the baselines on GPT-4o-mini, showing contextual coherence and personalized memory retention in long conversations. The implementation code is open-sourced at https://github.com/BAI-LAB/MemoryOS.",
    "note": "license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/\nThis CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: arxiv:2506.06326"
  },
  {
    "id": "nTk9nk8h",
    "URL": "https://arxiv.org/abs/2508.10055",
    "number": "2508.10055",
    "title": "Bayesian Models for Joint Selection of Features and Auto-Regressive Lags: Theory and Applications in Environmental and Financial Forecasting",
    "issued": {
      "date-parts": [
        [
          2025,
          8,
          18
        ]
      ]
    },
    "author": [
      {
        "given": "Alokesh",
        "family": "Manna"
      },
      {
        "given": "Sujit K.",
        "family": "Ghosh"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "We develop a Bayesian framework for variable selection in linear regression with autocorrelated errors, accommodating lagged covariates and autoregressive structures. This setting occurs in time series applications where responses depend on contemporaneous or past explanatory variables and persistent stochastic shocks, including financial modeling, hydrological forecasting, and meteorological applications requiring temporal dependency capture. Our methodology uses hierarchical Bayesian models with spike-and-slab priors to simultaneously select relevant covariates and lagged error terms. We propose an efficient two-stage MCMC algorithm separating sampling of variable inclusion indicators and model parameters to address high-dimensional computational challenges. Theoretical analysis establishes posterior selection consistency under mild conditions, even when candidate predictors grow exponentially with sample size, common in modern time series with many potential lagged variables. Through simulations and real applications (groundwater depth prediction, S&P 500 log returns modeling), we demonstrate substantial gains in variable selection accuracy and predictive performance. Compared to existing methods, our framework achieves lower MSPE, improved true model component identification, and greater robustness with autocorrelated noise, underscoring practical utility for model interpretation and forecasting in autoregressive settings.",
    "note": "license: http://creativecommons.org/publicdomain/zero/1.0/\nThis CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: arxiv:2508.10055"
  },
  {
    "id": "1C0w0XTvj",
    "URL": "https://arxiv.org/abs/2509.01794",
    "number": "2509.01794",
    "title": "A Multi-target Bayesian Transformer Framework for Predicting Cardiovascular Disease Biomarkers during Pandemics",
    "issued": {
      "date-parts": [
        [
          2025,
          11,
          7
        ]
      ]
    },
    "author": [
      {
        "given": "Trusting",
        "family": "Inekwe"
      },
      {
        "given": "Winnie",
        "family": "Mkandawire"
      },
      {
        "given": "Emmanuel",
        "family": "Agu"
      },
      {
        "given": "Andres",
        "family": "Colubri"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "The COVID-19 pandemic disrupted healthcare systems worldwide, disproportionately impacting individuals with chronic conditions such as cardiovascular disease (CVD). These disruptions -- through delayed care and behavioral changes, affected key CVD biomarkers, including LDL cholesterol (LDL-C), HbA1c, BMI, and systolic blood pressure (SysBP). Accurate modeling of these changes is crucial for predicting disease progression and guiding preventive care. However, prior work has not addressed multi-target prediction of CVD biomarker from Electronic Health Records (EHRs) using machine learning (ML), while jointly capturing biomarker interdependencies, temporal patterns, and predictive uncertainty. In this paper, we propose MBT-CB, a Multi-target Bayesian Transformer (MBT) with pre-trained BERT-based transformer framework to jointly predict LDL-C, HbA1c, BMI and SysBP CVD biomarkers from EHR data. The model leverages Bayesian Variational Inference to estimate uncertainties, embeddings to capture temporal relationships and a DeepMTR model to capture biomarker inter-relationships. We evaluate MBT-CT on retrospective EHR data from 3,390 CVD patient records (304 unique patients) in Central Massachusetts during the Covid-19 pandemic. MBT-CB outperformed a comprehensive set of baselines including other BERT-based ML models, achieving an MAE of 0.00887, RMSE of 0.0135 and MSE of 0.00027, while effectively capturing data and model uncertainty, patient biomarker inter-relationships, and temporal dynamics via its attention and embedding mechanisms. MBT-CB's superior performance highlights its potential to improve CVD biomarker prediction and support clinical decision-making during pandemics.",
    "note": "license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/\nThis CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: arxiv:2509.01794"
  },
  {
    "id": "138XfoxXm",
    "URL": "https://arxiv.org/abs/2509.08183",
    "number": "2509.08183",
    "title": "Chaotic Bayesian Inference: Strange Attractors as Risk Models for Black Swan Events",
    "issued": {
      "date-parts": [
        [
          2025,
          9,
          11
        ]
      ]
    },
    "author": [
      {
        "given": "Crystal",
        "family": "Rust"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "We introduce a new risk modeling framework where chaotic attractors shape the geometry of Bayesian inference. By combining heavy-tailed priors with Lorenz and Rossler dynamics, the models naturally generate volatility clustering, fat tails, and extreme events. We compare two complementary approaches: Model A, which emphasizes geometric stability, and Model B, which highlights rare bursts using Fibonacci diagnostics. Together, they provide a dual perspective for systemic risk analysis, linking Black Swan theory to practical tools for stress testing and volatility monitoring.",
    "note": "license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/\nThis CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: arxiv:2509.08183"
  },
  {
    "publisher": "Wiley",
    "abstract": "<jats:title>Abstract</jats:title>\n          <jats:p>Functional data analysis (FDA) models data using functions or functional parameters. The complexity of the functions is not assumed to be known in advance, so that methods are used for approximating these with as much flexibility as the data require.</jats:p>",
    "DOI": "10.1002/0470013192.bsa239",
    "source": "Crossref",
    "title": "Functional Data Analysis",
    "author": [
      {
        "given": "James",
        "family": "Ramsay"
      }
    ],
    "container-title": "Encyclopedia of Statistics in Behavioral Science",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2005,
          4,
          15
        ]
      ]
    },
    "URL": "https://doi.org/b9tmj6",
    "id": "OXCuJonU",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.1002/0470013192.bsa239",
    "type": "entry"
  },
  {
    "publisher": "Wiley",
    "DOI": "10.1002/9780470973394",
    "source": "Crossref",
    "title": "Multilevel Statistical Models",
    "author": [
      {
        "given": "Harvey",
        "family": "Goldstein"
      }
    ],
    "container-title": "Wiley Series in Probability and Statistics",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2010,
          10,
          29
        ]
      ]
    },
    "URL": "https://doi.org/cj8tgk",
    "id": "UqtGdFFv",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.1002/9780470973394",
    "type": "entry"
  },
  {
    "publisher": "Wiley",
    "issue": "29",
    "abstract": "<jats:p>Over 782 000 individuals in the United States have end‐stage kidney disease with about 72% of patients on dialysis, a life‐sustaining treatment. Dialysis patients experience high mortality and frequent hospitalizations, at about twice per year. These poor outcomes are exacerbated at key time periods, such as the fragile period after transition to dialysis. In order to study the time‐varying effects of modifiable patient and dialysis facility risk factors on hospitalization and mortality, we propose a novel Bayesian multilevel time‐varying joint model. Efficient estimation and inference is achieved within the Bayesian framework using Markov chain Monte Carlo, where multilevel (patient‐ and dialysis facility‐level) varying coefficient functions are targeted via Bayesian P‐splines. Applications to the United States Renal Data System, a national database which contains data on nearly all patients on dialysis in the United States, highlight significant time‐varying effects of patient‐ and facility‐level risk factors on hospitalization risk and mortality. Finite sample performance of the proposed methodology is studied through simulations.</jats:p>",
    "DOI": "10.1002/sim.9582",
    "type": "article-journal",
    "page": "5597-5611",
    "source": "Crossref",
    "title": "A Bayesian multilevel time‐varying framework for joint modeling of hospitalization and survival in patients on dialysis",
    "volume": "41",
    "author": [
      {
        "given": "Esra",
        "family": "Kürüm"
      },
      {
        "given": "Danh V.",
        "family": "Nguyen"
      },
      {
        "given": "Sudipto",
        "family": "Banerjee"
      },
      {
        "given": "Yihao",
        "family": "Li"
      },
      {
        "given": "Connie M.",
        "family": "Rhee"
      },
      {
        "given": "Damla",
        "family": "Şentürk"
      }
    ],
    "container-title": "Statistics in Medicine",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2022,
          10
        ]
      ]
    },
    "URL": "https://doi.org/g96dmg",
    "container-title-short": "Statistics in Medicine",
    "PMCID": "PMC9931182",
    "PMID": "36181392",
    "id": "ow7VHsto",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.1002/sim.9582"
  },
  {
    "publisher": "Elsevier BV",
    "issue": "2",
    "DOI": "10.1006/inco.1994.1009",
    "type": "article-journal",
    "page": "212-261",
    "source": "Crossref",
    "title": "The Weighted Majority Algorithm",
    "volume": "108",
    "author": [
      {
        "given": "N.",
        "family": "Littlestone"
      },
      {
        "given": "M.K.",
        "family": "Warmuth"
      }
    ],
    "container-title": "Information and Computation",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          1994,
          2
        ]
      ]
    },
    "URL": "https://doi.org/c8hw9h",
    "container-title-short": "Information and Computation",
    "id": "dSHoyg5K",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.1006/inco.1994.1009"
  },
  {
    "publisher": "Springer US",
    "DOI": "10.1007/978-1-4899-3242-6",
    "type": "book",
    "source": "Crossref",
    "title": "Generalized Linear Models",
    "author": [
      {
        "given": "P.",
        "family": "McCullagh"
      },
      {
        "given": "J. A.",
        "family": "Nelder"
      }
    ],
    "language": "en",
    "issued": {
      "date-parts": [
        [
          1989
        ]
      ]
    },
    "URL": "https://doi.org/brhr",
    "id": "1HWqsAX0j",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.1007/978-1-4899-3242-6"
  },
  {
    "publisher": "Springer Berlin Heidelberg",
    "DOI": "10.1007/978-3-540-28645-5_29",
    "type": "chapter",
    "page": "286-295",
    "source": "Crossref",
    "title": "Learning with Drift Detection",
    "author": [
      {
        "given": "João",
        "family": "Gama"
      },
      {
        "given": "Pedro",
        "family": "Medas"
      },
      {
        "given": "Gladys",
        "family": "Castillo"
      },
      {
        "given": "Pedro",
        "family": "Rodrigues"
      }
    ],
    "container-title": "Lecture Notes in Computer Science",
    "issued": {
      "date-parts": [
        [
          2004
        ]
      ]
    },
    "URL": "https://doi.org/ckzcm4",
    "id": "QbZy0vLM",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.1007/978-3-540-28645-5_29"
  },
  {
    "publisher": "Elsevier BV",
    "issue": "4",
    "DOI": "10.1016/j.chaos.2008.07.022",
    "type": "article-journal",
    "page": "1764-1772",
    "source": "Crossref",
    "title": "Dynamic effects of increasing heterogeneity in financial markets",
    "volume": "41",
    "author": [
      {
        "given": "Ahmad K.",
        "family": "Naimzada"
      },
      {
        "given": "Giorgio",
        "family": "Ricchiuti"
      }
    ],
    "container-title": "Chaos, Solitons &amp; Fractals",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2009,
          8
        ]
      ]
    },
    "URL": "https://doi.org/bfbqxn",
    "container-title-short": "Chaos, Solitons &amp; Fractals",
    "id": "1HdkTgLul",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.1016/j.chaos.2008.07.022"
  },
  {
    "publisher": "Elsevier BV",
    "DOI": "10.1016/j.cie.2020.106889",
    "type": "article-journal",
    "page": "106889",
    "source": "Crossref",
    "title": "Predictive maintenance in the Industry 4.0: A systematic literature review",
    "volume": "150",
    "author": [
      {
        "given": "Tiago",
        "family": "Zonta"
      },
      {
        "given": "Cristiano André",
        "family": "da Costa"
      },
      {
        "given": "Rodrigo",
        "family": "da Rosa Righi"
      },
      {
        "given": "Miromar José",
        "family": "de Lima"
      },
      {
        "given": "Eduardo Silveira",
        "family": "da Trindade"
      },
      {
        "given": "Guann Pyng",
        "family": "Li"
      }
    ],
    "container-title": "Computers &amp; Industrial Engineering",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2020,
          12
        ]
      ]
    },
    "URL": "https://doi.org/ghtwvv",
    "container-title-short": "Computers &amp; Industrial Engineering",
    "id": "2CxZoj5J",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.1016/j.cie.2020.106889"
  },
  {
    "publisher": "Elsevier BV",
    "DOI": "10.1016/j.jbi.2022.104086",
    "type": "article-journal",
    "page": "104086",
    "source": "Crossref",
    "title": "Automated interpretable discovery of heterogeneous treatment effectiveness: A COVID-19 case study",
    "volume": "130",
    "author": [
      {
        "given": "Benjamin J.",
        "family": "Lengerich"
      },
      {
        "given": "Mark E.",
        "family": "Nunnally"
      },
      {
        "given": "Yin",
        "family": "Aphinyanaphongs"
      },
      {
        "given": "Caleb",
        "family": "Ellington"
      },
      {
        "given": "Rich",
        "family": "Caruana"
      }
    ],
    "container-title": "Journal of Biomedical Informatics",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2022,
          6
        ]
      ]
    },
    "URL": "https://doi.org/gt68h5",
    "container-title-short": "Journal of Biomedical Informatics",
    "PMCID": "PMC9055753",
    "PMID": "35504543",
    "id": "esxxcr9l",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.1016/j.jbi.2022.104086"
  },
  {
    "publisher": "Cambridge University Press",
    "abstract": "<jats:p>Data Analysis Using Regression and Multilevel/Hierarchical Models, first published in 2007, is a comprehensive manual for the applied researcher who wants to perform data analysis using linear and nonlinear regression and multilevel models. The book introduces a wide variety of models, whilst at the same time instructing the reader in how to fit these models using available software packages. The book illustrates the concepts by working through scores of real data examples that have arisen from the authors' own applied research, with programming codes provided for each one. Topics covered include causal inference, including regression, poststratification, matching, regression discontinuity, and instrumental variables, as well as multilevel logistic regression and missing-data imputation. Practical tips regarding building, fitting, and understanding are provided throughout.</jats:p>",
    "DOI": "10.1017/cbo9780511790942",
    "source": "Crossref",
    "title": "Data Analysis Using Regression and Multilevel/Hierarchical Models",
    "author": [
      {
        "given": "Andrew",
        "family": "Gelman"
      },
      {
        "given": "Jennifer",
        "family": "Hill"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2006,
          12,
          18
        ]
      ]
    },
    "URL": "https://doi.org/dbrqk6",
    "id": "PWhr4ijC",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.1017/cbo9780511790942",
    "type": "entry"
  },
  {
    "publisher": "Springer Science and Business Media LLC",
    "issue": "1",
    "DOI": "10.1023/a:1007379606734",
    "type": "article-journal",
    "page": "41-75",
    "source": "Crossref",
    "title": "Multitask Learning",
    "volume": "28",
    "author": [
      {
        "given": "Rich",
        "family": "Caruana"
      }
    ],
    "container-title": "Machine Learning",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          1997,
          7
        ]
      ]
    },
    "URL": "https://doi.org/d3gsgj",
    "container-title-short": "Machine Learning",
    "id": "9k4OKrXL",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.1023/a:1007379606734"
  },
  {
    "publisher": "Springer Science and Business Media LLC",
    "issue": "2-3",
    "DOI": "10.1023/a:1013689704352",
    "type": "article-journal",
    "page": "235-256",
    "source": "Crossref",
    "title": "Finite-time Analysis of the Multiarmed Bandit Problem",
    "volume": "47",
    "author": [
      {
        "given": "Peter",
        "family": "Auer"
      },
      {
        "given": "Nicolò",
        "family": "Cesa-Bianchi"
      },
      {
        "given": "Paul",
        "family": "Fischer"
      }
    ],
    "container-title": "Machine Learning",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2002,
          5
        ]
      ]
    },
    "URL": "https://doi.org/dxkxgv",
    "container-title-short": "Machine Learning",
    "id": "17mBgrHWB",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.1023/a:1013689704352"
  },
  {
    "publisher": "Springer Science and Business Media LLC",
    "issue": "3-4",
    "DOI": "10.1023/a:1022672621406",
    "type": "article-journal",
    "page": "229-256",
    "source": "Crossref",
    "title": "Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning",
    "volume": "8",
    "author": [
      {
        "given": "Ronald J.",
        "family": "Williams"
      }
    ],
    "container-title": "Machine Learning",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          1992,
          5
        ]
      ]
    },
    "URL": "https://doi.org/b762gd",
    "container-title-short": "Machine Learning",
    "id": "5ivkUecW",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.1023/a:1022672621406"
  },
  {
    "publisher": "American Psychological Association (APA)",
    "issue": "4",
    "DOI": "10.1037/h0040957",
    "type": "article-journal",
    "page": "281-302",
    "source": "Crossref",
    "title": "Construct validity in psychological tests.",
    "volume": "52",
    "author": [
      {
        "given": "Lee J.",
        "family": "Cronbach"
      },
      {
        "given": "Paul E.",
        "family": "Meehl"
      }
    ],
    "container-title": "Psychological Bulletin",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          1955,
          7
        ]
      ]
    },
    "URL": "https://doi.org/dcsjjf",
    "container-title-short": "Psychological Bulletin",
    "PMID": "13245896",
    "id": "3oGysJy4",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.1037/h0040957"
  },
  {
    "publisher": "Springer Science and Business Media LLC",
    "issue": "7540",
    "DOI": "10.1038/nature14236",
    "type": "article-journal",
    "page": "529-533",
    "source": "Crossref",
    "title": "Human-level control through deep reinforcement learning",
    "volume": "518",
    "author": [
      {
        "given": "Volodymyr",
        "family": "Mnih"
      },
      {
        "given": "Koray",
        "family": "Kavukcuoglu"
      },
      {
        "given": "David",
        "family": "Silver"
      },
      {
        "given": "Andrei A.",
        "family": "Rusu"
      },
      {
        "given": "Joel",
        "family": "Veness"
      },
      {
        "given": "Marc G.",
        "family": "Bellemare"
      },
      {
        "given": "Alex",
        "family": "Graves"
      },
      {
        "given": "Martin",
        "family": "Riedmiller"
      },
      {
        "given": "Andreas K.",
        "family": "Fidjeland"
      },
      {
        "given": "Georg",
        "family": "Ostrovski"
      },
      {
        "given": "Stig",
        "family": "Petersen"
      },
      {
        "given": "Charles",
        "family": "Beattie"
      },
      {
        "given": "Amir",
        "family": "Sadik"
      },
      {
        "given": "Ioannis",
        "family": "Antonoglou"
      },
      {
        "given": "Helen",
        "family": "King"
      },
      {
        "given": "Dharshan",
        "family": "Kumaran"
      },
      {
        "given": "Daan",
        "family": "Wierstra"
      },
      {
        "given": "Shane",
        "family": "Legg"
      },
      {
        "given": "Demis",
        "family": "Hassabis"
      }
    ],
    "container-title": "Nature",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2015,
          2,
          25
        ]
      ]
    },
    "URL": "https://doi.org/gc3h75",
    "container-title-short": "Nature",
    "PMID": "25719670",
    "id": "BLbPVL8P",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.1038/nature14236"
  },
  {
    "publisher": "Springer Science and Business Media LLC",
    "issue": "1",
    "abstract": "<jats:title>Abstract</jats:title><jats:p>Recent large language models (LLMs), such as ChatGPT, have demonstrated remarkable prediction performance for a growing array of tasks. However, their proliferation into high-stakes domains and compute-limited settings has created a burgeoning need for interpretability and efficiency. We address this need by proposing Aug-imodels, a framework for leveraging the knowledge learned by LLMs to build extremely efficient and interpretable prediction models. Aug-imodels use LLMs during fitting but not during inference, allowing complete transparency and often a speed/memory improvement of greater than 1000x for inference compared to LLMs. We explore two instantiations of Aug-imodels in natural-language processing: Aug-Linear, which augments a linear model with decoupled embeddings from an LLM and Aug-Tree, which augments a decision tree with LLM feature expansions. Across a variety of text-classification datasets, both outperform their non-augmented, interpretable counterparts. Aug-Linear can even outperform much larger models, e.g. a 6-billion parameter GPT-J model, despite having 10,000x fewer parameters and being fully transparent. We further explore Aug-imodels in a natural-language fMRI study, where they generate interesting interpretations from scientific data.</jats:p>",
    "DOI": "10.1038/s41467-023-43713-1",
    "type": "article-journal",
    "source": "Crossref",
    "title": "Augmenting interpretable models with large language models during training",
    "volume": "14",
    "author": [
      {
        "given": "Chandan",
        "family": "Singh"
      },
      {
        "given": "Armin",
        "family": "Askari"
      },
      {
        "given": "Rich",
        "family": "Caruana"
      },
      {
        "given": "Jianfeng",
        "family": "Gao"
      }
    ],
    "container-title": "Nature Communications",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2023,
          11,
          30
        ]
      ]
    },
    "URL": "https://doi.org/g9t2z9",
    "container-title-short": "Nat Commun",
    "PMCID": "PMC10689442",
    "PMID": "38036543",
    "id": "HQXzkG4Q",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.1038/s41467-023-43713-1"
  },
  {
    "publisher": "Springer Science and Business Media LLC",
    "issue": "3",
    "abstract": "<jats:title>Abstract</jats:title><jats:p>Post-hoc interpretability methods are critical tools to explain neural-network results. Several post-hoc methods have emerged in recent years but they produce different results when applied to a given task, raising the question of which method is the most suitable to provide accurate post-hoc interpretability. To understand the performance of each method, quantitative evaluation of interpretability methods is essential; however, currently available frameworks have several drawbacks that hinder the adoption of post-hoc interpretability methods, especially in high-risk sectors. In this work we propose a framework with quantitative metrics to assess the performance of existing post-hoc interpretability methods, particularly in time-series classification. We show that several drawbacks identified in the literature are addressed, namely, the dependence on human judgement, retraining and the shift in the data distribution when occluding samples. We also design a synthetic dataset with known discriminative features and tunable complexity. The proposed methodology and quantitative metrics can be used to understand the reliability of interpretability methods results obtained in practical applications. In turn, they can be embedded within operational workflows in critical fields that require accurate interpretability results for, example, regulatory policies.</jats:p>",
    "DOI": "10.1038/s42256-023-00620-w",
    "type": "article-journal",
    "page": "250-260",
    "source": "Crossref",
    "title": "Evaluation of post-hoc interpretability methods in time-series classification",
    "volume": "5",
    "author": [
      {
        "given": "Hugues",
        "family": "Turbé"
      },
      {
        "given": "Mina",
        "family": "Bjelogrlic"
      },
      {
        "given": "Christian",
        "family": "Lovis"
      },
      {
        "given": "Gianmarco",
        "family": "Mengaldo"
      }
    ],
    "container-title": "Nature Machine Intelligence",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2023,
          3,
          13
        ]
      ]
    },
    "URL": "https://doi.org/grzdnk",
    "container-title-short": "Nat Mach Intell",
    "id": "RzmVBDLx",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.1038/s42256-023-00620-w"
  },
  {
    "publisher": "Wiley",
    "issue": "1",
    "DOI": "10.1080/00288233.1981.10420865",
    "type": "article-journal",
    "page": "11-20",
    "source": "Crossref",
    "title": "Herd effects on the growth of beef bulls from different sources tested together under grazing conditions",
    "volume": "24",
    "author": [
      {
        "given": "C.A.",
        "family": "Morris"
      }
    ],
    "container-title": "New Zealand Journal of Agricultural Research",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          1981,
          1
        ]
      ]
    },
    "URL": "https://doi.org/fxp28n",
    "container-title-short": "New Zealand Journal of Agricultural Research",
    "id": "1FmrthAK7",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.1080/00288233.1981.10420865"
  },
  {
    "publisher": "Informa UK Limited",
    "issue": "227",
    "DOI": "10.1080/01621459.1944.10500699",
    "type": "article-journal",
    "page": "357-365",
    "source": "Crossref",
    "title": "Application of the Logistic Function to Bio-Assay",
    "volume": "39",
    "author": [
      {
        "given": "Joseph",
        "family": "Berkson"
      }
    ],
    "container-title": "Journal of the American Statistical Association",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          1944,
          9
        ]
      ]
    },
    "URL": "https://doi.org/gn82f3",
    "container-title-short": "Journal of the American Statistical Association",
    "id": "15YaJttEf",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.1080/01621459.1944.10500699"
  },
  {
    "publisher": "Informa UK Limited",
    "issue": "421",
    "DOI": "10.1080/01621459.1993.10594284",
    "type": "article-journal",
    "page": "9-25",
    "source": "Crossref",
    "title": "Approximate Inference in Generalized Linear Mixed Models",
    "volume": "88",
    "author": [
      {
        "given": "N. E.",
        "family": "Breslow"
      },
      {
        "given": "D. G.",
        "family": "Clayton"
      }
    ],
    "container-title": "Journal of the American Statistical Association",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          1993,
          3
        ]
      ]
    },
    "URL": "https://doi.org/ggnhwn",
    "container-title-short": "Journal of the American Statistical Association",
    "id": "sPnUGQ8l",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.1080/01621459.1993.10594284"
  },
  {
    "publisher": "Informa UK Limited",
    "issue": "509",
    "DOI": "10.1080/01621459.2014.896806",
    "type": "article-journal",
    "page": "159-174",
    "source": "Crossref",
    "title": "Bayesian Inference of Multiple Gaussian Graphical Models",
    "volume": "110",
    "author": [
      {
        "given": "Christine",
        "family": "Peterson"
      },
      {
        "given": "Francesco C.",
        "family": "Stingo"
      },
      {
        "given": "Marina",
        "family": "Vannucci"
      }
    ],
    "container-title": "Journal of the American Statistical Association",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2015,
          1,
          2
        ]
      ]
    },
    "URL": "https://doi.org/f69dnj",
    "container-title-short": "Journal of the American Statistical Association",
    "PMCID": "PMC4465207",
    "PMID": "26078481",
    "id": "1Da8QJneg",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.1080/01621459.2014.896806"
  },
  {
    "publisher": "Informa UK Limited",
    "issue": "538",
    "DOI": "10.1080/01621459.2021.2000866",
    "type": "article-journal",
    "page": "533-546",
    "source": "Crossref",
    "title": "Bayesian Edge Regression in Undirected Graphical Models to Characterize Interpatient Heterogeneity in Cancer",
    "volume": "117",
    "author": [
      {
        "given": "Zeya",
        "family": "Wang"
      },
      {
        "given": "Veerabhadran",
        "family": "Baladandayuthapani"
      },
      {
        "given": "Ahmed O.",
        "family": "Kaseb"
      },
      {
        "given": "Hesham M.",
        "family": "Amin"
      },
      {
        "given": "Manal M.",
        "family": "Hassan"
      },
      {
        "given": "Wenyi",
        "family": "Wang"
      },
      {
        "given": "Jeffrey S.",
        "family": "Morris"
      }
    ],
    "container-title": "Journal of the American Statistical Association",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2022,
          1,
          5
        ]
      ]
    },
    "URL": "https://doi.org/gt68hr",
    "container-title-short": "Journal of the American Statistical Association",
    "PMCID": "PMC9454401",
    "PMID": "36090952",
    "id": "TULLRYDp",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.1080/01621459.2021.2000866"
  },
  {
    "publisher": "Informa UK Limited",
    "issue": "552",
    "DOI": "10.1080/01621459.2025.2470481",
    "type": "article-journal",
    "page": "2498-2509",
    "source": "Crossref",
    "title": "Network Varying Coefficient Model",
    "volume": "120",
    "author": [
      {
        "given": "Xinyan",
        "family": "Fan"
      },
      {
        "given": "Kuangnan",
        "family": "Fang"
      },
      {
        "given": "Wei",
        "family": "Lan"
      },
      {
        "given": "Chih-Ling",
        "family": "Tsai"
      }
    ],
    "container-title": "Journal of the American Statistical Association",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2025,
          4,
          11
        ]
      ]
    },
    "URL": "https://doi.org/g9t2rm",
    "container-title-short": "Journal of the American Statistical Association",
    "id": "10QS2bf0y",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.1080/01621459.2025.2470481"
  },
  {
    "publisher": "Informa UK Limited",
    "issue": "14",
    "DOI": "10.1080/03610920801931887",
    "type": "article-journal",
    "page": "2249-2261",
    "source": "Crossref",
    "title": "Penalized Spline Estimation for Varying-Coefficient Models",
    "volume": "37",
    "author": [
      {
        "given": "Yiqiang",
        "family": "Lu"
      },
      {
        "given": "Riquan",
        "family": "Zhang"
      },
      {
        "given": "Liping",
        "family": "Zhu"
      }
    ],
    "container-title": "Communications in Statistics - Theory and Methods",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2008,
          5,
          27
        ]
      ]
    },
    "URL": "https://doi.org/fpj5gm",
    "container-title-short": "Communications in Statistics - Theory and Methods",
    "id": "Z6951tJe",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.1080/03610920801931887"
  },
  {
    "publisher": "Informa UK Limited",
    "DOI": "10.1080/13675567.2025.2566806",
    "type": "article-journal",
    "page": "1-28",
    "source": "Crossref",
    "title": "A data-driven and context-aware approach for demand forecasting in the beverage industry",
    "author": [
      {
        "given": "Benedict Jun",
        "family": "Ma"
      },
      {
        "given": "Ilya",
        "family": "Jackson"
      },
      {
        "given": "Maggie",
        "family": "Huang"
      },
      {
        "given": "Sebastian",
        "family": "Villegas"
      },
      {
        "given": "Jaime",
        "family": "Macias-Aguayo"
      }
    ],
    "container-title": "International Journal of Logistics Research and Applications",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2025,
          10,
          10
        ]
      ]
    },
    "URL": "https://doi.org/g96qvd",
    "container-title-short": "International Journal of Logistics Research and Applications",
    "id": "1C8sIEO7D",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.1080/13675567.2025.2566806"
  },
  {
    "publisher": "IOP Publishing",
    "issue": "7",
    "DOI": "10.1088/2040-8978/17/7/073001",
    "type": "article-journal",
    "page": "073001",
    "source": "Crossref",
    "title": "Compression, restoration, resampling, ‘compressive sensing’: fast transforms in digital imaging",
    "volume": "17",
    "author": [
      {
        "given": "L P",
        "family": "Yaroslavsky"
      }
    ],
    "container-title": "Journal of Optics",
    "issued": {
      "date-parts": [
        [
          2015,
          7,
          1
        ]
      ]
    },
    "URL": "https://doi.org/g96wr9",
    "container-title-short": "J. Opt.",
    "id": "13xFsnpVI",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.1088/2040-8978/17/7/073001"
  },
  {
    "publisher": "American Mathematical Society (AMS)",
    "issue": "5",
    "DOI": "10.1090/s0002-9904-1952-09620-8",
    "type": "article-journal",
    "page": "527-535",
    "source": "Crossref",
    "title": "Some aspects of the sequential design of experiments",
    "volume": "58",
    "author": [
      {
        "given": "Herbert",
        "family": "Robbins"
      }
    ],
    "container-title": "Bulletin of the American Mathematical Society",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          1952
        ]
      ]
    },
    "URL": "https://doi.org/c325nh",
    "container-title-short": "Bull. Amer. Math. Soc.",
    "id": "18RlJp2YO",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.1090/s0002-9904-1952-09620-8"
  },
  {
    "publisher": "Oxford University Press (OUP)",
    "issue": "3",
    "DOI": "10.1093/biomet/58.3.545",
    "type": "article-journal",
    "page": "545-554",
    "source": "Crossref",
    "title": "Recovery of inter-block information when block sizes are unequal",
    "volume": "58",
    "author": [
      {
        "given": "H. D.",
        "family": "PATTERSON"
      },
      {
        "given": "R.",
        "family": "THOMPSON"
      }
    ],
    "container-title": "Biometrika",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          1971
        ]
      ]
    },
    "URL": "https://doi.org/c473hg",
    "container-title-short": "Biometrika",
    "id": "xPNSAM4v",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.1093/biomet/58.3.545"
  },
  {
    "publisher": "Oxford University Press (OUP)",
    "issue": "4",
    "DOI": "10.1093/biomet/78.4.719",
    "type": "article-journal",
    "page": "719-727",
    "source": "Crossref",
    "title": "Estimation in generalized linear models with random effects",
    "volume": "78",
    "author": [
      {
        "given": "ROBERT",
        "family": "SCHALL"
      }
    ],
    "container-title": "Biometrika",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          1991
        ]
      ]
    },
    "URL": "https://doi.org/bhxfht",
    "container-title-short": "Biometrika",
    "id": "iS2cDavY",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.1093/biomet/78.4.719"
  },
  {
    "publisher": "Oxford University Press (OUP)",
    "issue": "1",
    "DOI": "10.1093/biomet/asq060",
    "type": "article-journal",
    "page": "1-15",
    "source": "Crossref",
    "title": "Joint estimation of multiple graphical models",
    "volume": "98",
    "author": [
      {
        "given": "J.",
        "family": "Guo"
      },
      {
        "given": "E.",
        "family": "Levina"
      },
      {
        "given": "G.",
        "family": "Michailidis"
      },
      {
        "given": "J.",
        "family": "Zhu"
      }
    ],
    "container-title": "Biometrika",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2011,
          2,
          9
        ]
      ]
    },
    "URL": "https://doi.org/fqvbh2",
    "container-title-short": "Biometrika",
    "PMCID": "PMC3412604",
    "PMID": "23049124",
    "id": "hxZIBmjM",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.1093/biomet/asq060"
  },
  {
    "publisher": "Oxford University Press (OUP)",
    "issue": "2",
    "abstract": "<jats:title>ABSTRACT</jats:title>\n               <jats:p>Covariate-dependent graph learning has gained increasing interest in the graphical modeling literature for the analysis of heterogeneous data. This task, however, poses challenges to modeling, computational efficiency, and interpretability. The parameter of interest can be naturally represented as a 3-dimensional array with elements that can be grouped according to 2 directions, corresponding to node level and covariate level, respectively. In this article, we propose a novel dual group spike-and-slab prior that enables multi-level selection at covariate-level and node-level, as well as individual (local) level sparsity. We introduce a nested strategy with specific choices to address distinct challenges posed by the various grouping directions. For posterior inference, we develop a full Gibbs sampler for all parameters, which mitigates the difficulties of parameter tuning often encountered in high-dimensional graphical models and facilitates routine implementation. Through simulation studies, we demonstrate that the proposed model outperforms existing methods in its accuracy of graph recovery. We show the practical utility of our model via an application to microbiome data where we seek to better understand the interactions among microbes as well as how these are affected by relevant covariates.</jats:p>",
    "DOI": "10.1093/biomtc/ujaf053",
    "type": "article-journal",
    "source": "Crossref",
    "title": "Bayesian covariate-dependent graph learning with a dual group spike-and-slab prior",
    "volume": "81",
    "author": [
      {
        "given": "Zijian",
        "family": "Zeng"
      },
      {
        "given": "Meng",
        "family": "Li"
      },
      {
        "given": "Marina",
        "family": "Vannucci"
      }
    ],
    "container-title": "Biometrics",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2025,
          4,
          2
        ]
      ]
    },
    "URL": "https://doi.org/g95bkg",
    "PMID": "40322851",
    "id": "x5cbX2Cv",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.1093/biomtc/ujaf053"
  },
  {
    "publisher": "Oxford University Press (OUP)",
    "issue": "3",
    "abstract": "<jats:title>Abstract</jats:title>\n               <jats:p>We consider the problem of estimating sparse graphs by a lasso penalty applied to the inverse covariance matrix. Using a coordinate descent procedure for the lasso, we develop a simple algorithm—the graphical lasso—that is remarkably fast: It solves a 1000-node problem (∼500000 parameters) in at most a minute and is 30–4000 times faster than competing methods. It also provides a conceptual link between the exact problem and the approximation suggested by Meinshausen and Bühlmann (2006). We illustrate the method on some cell-signaling data from proteomics.</jats:p>",
    "DOI": "10.1093/biostatistics/kxm045",
    "type": "article-journal",
    "page": "432-441",
    "source": "Crossref",
    "title": "Sparse inverse covariance estimation with the graphical lasso",
    "volume": "9",
    "author": [
      {
        "given": "Jerome",
        "family": "Friedman"
      },
      {
        "given": "Trevor",
        "family": "Hastie"
      },
      {
        "given": "Robert",
        "family": "Tibshirani"
      }
    ],
    "container-title": "Biostatistics",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2007,
          12,
          12
        ]
      ]
    },
    "URL": "https://doi.org/db7svr",
    "PMCID": "PMC3019769",
    "PMID": "18079126",
    "id": "m4KsbXUW",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.1093/biostatistics/kxm045"
  },
  {
    "publisher": "Oxford University Press (OUP)",
    "DOI": "10.1093/jnci/11.6.1269",
    "type": "article-journal",
    "source": "Crossref",
    "title": "A Method of Estimating Comparative Rates from Clinical Data. Applications to Cancer of the Lung, Breast, and Cervix",
    "container-title": "JNCI: Journal of the National Cancer Institute",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          1951,
          6
        ]
      ]
    },
    "URL": "https://doi.org/g96wsb",
    "id": "pC7P4enS",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.1093/jnci/11.6.1269"
  },
  {
    "publisher": "Oxford University PressOxford",
    "abstract": "<jats:title>Abstract</jats:title>\n                  <jats:p>The idea of modelling systems using graph theory has its origin in several scientific areas: in statistical physics (the study of large particle systems), in genetics (studying inheritable properties of natural species), and in interactions in contingency tables. The use of graphical models in statistics has increased considerably over recent years and the theory has been greatly developed and extended. This book provides the first comprehensive and authoritative account of the theory of graphical models and is written by a leading expert in the field. It contains the fundamental graph theory required and a thorough study of Markov properties associated with various type of graphs. The statistical theory of log-linear and graphical models for contingency tables, covariance selection models, and graphical models with mixed discrete-continous variables in developed detail. Special topics, such as the application of graphical models to probabilistic expert systems, are described briefly, and appendices give details of the multivarate normal distribution and of the theory of regular exponential families. The author has recently been awarded the RSS Guy Medal in Silver 1996 for his innovative contributions to statistical theory and practice, and especially for his work on graphical models.</jats:p>",
    "DOI": "10.1093/oso/9780198522195.001.0001",
    "type": "book",
    "source": "Crossref",
    "title": "Graphical Models",
    "author": [
      {
        "given": "Steffen L",
        "family": "Lauritzen"
      }
    ],
    "language": "en",
    "issued": {
      "date-parts": [
        [
          1996,
          5,
          2
        ]
      ]
    },
    "URL": "https://doi.org/g958zb",
    "id": "moNSKvBk",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.1093/oso/9780198522195.001.0001"
  },
  {
    "publisher": "Oxford University Press (OUP)",
    "issue": "4",
    "DOI": "10.1093/rfs/15.4.1137",
    "type": "article-journal",
    "page": "1137-1187",
    "source": "Crossref",
    "title": "International Asset Allocation With Regime Shifts",
    "volume": "15",
    "author": [
      {
        "given": "Andrew",
        "family": "Ang"
      },
      {
        "given": "Geert",
        "family": "Bekaert"
      }
    ],
    "container-title": "Review of Financial Studies",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2002,
          7
        ]
      ]
    },
    "URL": "https://doi.org/b535qr",
    "container-title-short": "Rev. Financ. Stud.",
    "id": "E4f9DQPu",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.1093/rfs/15.4.1137"
  },
  {
    "publisher": "Cold Spring Harbor Laboratory",
    "abstract": "<jats:title>Abstract</jats:title><jats:p>Summarizing multiple data modalities into a parsimonious cancer “subtype” is difficult because the most informative representation of each patient’s disease is not observed. We propose to model these latent summaries as<jats:italic>discriminative subtypes</jats:italic>: sample representations which induce accurate and interpretable sample-specific models for downstream predictions. In this way, discriminative subtypes, which are shared between data modalities, can be estimated from one data modality and optimized according to the predictions induced in another modality. We apply this approach to lung cancer by training a deep neural network to predict discriminative subtypes from histopathology images, and use these predicted subtypes to generate models which classify adenocarcinoma, squamous cell carcinoma, and healthy tissue based on transcriptomic signatures. In this way, we optimize the latent discriminative subtypes through induced prediction loss, and the discriminative subtypes are interpreted with standard interpretation of transcriptomic predictive models. Our framework achieves state-of-the-art classification accuracy (F1-score of 0.97) and identifies discriminative subtypes which link histopathology images to transcriptomic explanations without requiring pre-specification of morphological patterns or transcriptomic processes.</jats:p>",
    "DOI": "10.1101/2020.06.25.20140053",
    "type": "manuscript",
    "source": "Crossref",
    "title": "Discriminative Subtyping of Lung Cancers from Histopathology Images via Contextual Deep Learning",
    "author": [
      {
        "given": "Benjamin J.",
        "family": "Lengerich"
      },
      {
        "given": "Maruan",
        "family": "Al-Shedivat"
      },
      {
        "given": "Amir",
        "family": "Alavi"
      },
      {
        "given": "Jennifer",
        "family": "Williams"
      },
      {
        "given": "Sami",
        "family": "Labbaki"
      },
      {
        "given": "Eric P.",
        "family": "Xing"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2020,
          6,
          26
        ]
      ]
    },
    "URL": "https://doi.org/gt68h6",
    "id": "O1UU4a5P",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.1101/2020.06.25.20140053"
  },
  {
    "publisher": "Cold Spring Harbor Laboratory",
    "abstract": "<jats:p>Cancers are shaped by somatic mutations, microenvironment, and patient background, each altering gene expression and regulation in complex ways, resulting in heterogeneous cellular states and dynamics. Inferring gene regulatory networks (GRNs) from expression data can help characterize this regulation-driven heterogeneity, but network inference requires many statistical samples, limiting GRNs to cluster-level analyses that ignore intra-cluster heterogeneity. We propose to move beyond coarse analyses of pre-defined subgroups by using<jats:italic>contextualized</jats:italic>learning, a multi-task learning paradigm that uses multi-view contexts including phenotypic, molecular, and environmental information to infer personalized models. With sample-specific contexts, contextualization enables sample-specific models and even generalizes at test time to predict network models for entirely unseen contexts. We unify three network model classes (Correlation, Markov, Neighborhood Selection) and estimate context-specific GRNs for 7997 tumors across 25 tumor types, using copy number and driver mutation profiles, tumor microenvironment, and patient demographics as model context. Our generative modeling approach allows us to predict GRNs for unseen tumor types based on a pan-cancer model of how somatic mutations affect gene regulation. Finally, contextualized networks enable GRN-based precision oncology by providing a structured view of expression dynamics at sample-specific resolution, explaining known biomarkers in terms of network-mediated effects and leading to novel subtypings that improve survival prognosis. We provide a SKLearn-style Python package<jats:ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://contextualized.ml\">https://contextualized.ml</jats:ext-link>for learning and analyzing contextualized models, as well as interactive plotting tools for pan-cancer data exploration at<jats:ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://github.com/cnellington/CancerContextualized\">https://github.com/cnellington/CancerContextualized</jats:ext-link>.</jats:p><jats:sec><jats:title>Significance Statement</jats:title><jats:p>Network estimation is essential for understanding the structure and function of biological systems, but current statistical approaches fail to capture inter-subject heterogeneity or cross-modality information flow, both of which are needed for understanding complex phenotypes and pathologies. We introduce contextualized network inference, leveraging multi-view contextual metadata to capture similarities and differences among heterogeneous observations during network estimation. Sharing information across contexts enables inference at sample-specific resolution, thus quantifying variation between subjects and revealing context-specific network rewiring. Applied to tumor-specific transcriptional network inference using clinical, molecular, and multi-omic data, contextualized networks improve accuracy, generalize to unseen cancer types, and discover novel prognostic tumor subtypes. By tailoring disease models to each sample, contextualized networks promise to enable precision medicine at unprecedented resolution.</jats:p></jats:sec>",
    "DOI": "10.1101/2023.12.01.569658",
    "type": "manuscript",
    "source": "Crossref",
    "title": "Learning to Estimate Sample-specific Transcriptional Networks for 7000 Tumors",
    "author": [
      {
        "given": "Caleb N.",
        "family": "Ellington"
      },
      {
        "given": "Benjamin J.",
        "family": "Lengerich"
      },
      {
        "given": "Thomas B.K.",
        "family": "Watkins"
      },
      {
        "given": "Jiekun",
        "family": "Yang"
      },
      {
        "given": "Abhinav",
        "family": "Adduri"
      },
      {
        "given": "Sazan",
        "family": "Mahbub"
      },
      {
        "given": "Hanxi",
        "family": "Xiao"
      },
      {
        "given": "Manolis",
        "family": "Kellis"
      },
      {
        "given": "Eric P.",
        "family": "Xing"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2023,
          12,
          4
        ]
      ]
    },
    "URL": "https://doi.org/gt68h7",
    "id": "Rt6voTFN",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.1101/2023.12.01.569658"
  },
  {
    "publisher": "Institute of Electrical and Electronics Engineers (IEEE)",
    "issue": "5",
    "DOI": "10.1109/72.788640",
    "type": "article-journal",
    "page": "988-999",
    "source": "Crossref",
    "title": "An overview of statistical learning theory",
    "volume": "10",
    "author": [
      {
        "given": "V.N.",
        "family": "Vapnik"
      }
    ],
    "container-title": "IEEE Transactions on Neural Networks",
    "issued": {
      "date-parts": [
        [
          1999
        ]
      ]
    },
    "URL": "https://doi.org/fdvhmd",
    "container-title-short": "IEEE Trans. Neural Netw.",
    "PMID": "18252602",
    "id": "bWiWgPrK",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.1109/72.788640"
  },
  {
    "publisher": "Institute of Electrical and Electronics Engineers (IEEE)",
    "issue": "10",
    "DOI": "10.1109/tkde.2009.191",
    "type": "article-journal",
    "page": "1345-1359",
    "source": "Crossref",
    "title": "A Survey on Transfer Learning",
    "volume": "22",
    "author": [
      {
        "given": "Sinno Jialin",
        "family": "Pan"
      },
      {
        "given": "Qiang",
        "family": "Yang"
      }
    ],
    "container-title": "IEEE Transactions on Knowledge and Data Engineering",
    "issued": {
      "date-parts": [
        [
          2010,
          10
        ]
      ]
    },
    "URL": "https://doi.org/bc4vws",
    "container-title-short": "IEEE Trans. Knowl. Data Eng.",
    "id": "12JtL2o6T",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.1109/tkde.2009.191"
  },
  {
    "publisher": "Institute of Electrical and Electronics Engineers (IEEE)",
    "issue": "6",
    "DOI": "10.1109/tpami.1984.4767596",
    "type": "article-journal",
    "page": "721-741",
    "source": "Crossref",
    "title": "Stochastic Relaxation, Gibbs Distributions, and the Bayesian Restoration of Images",
    "volume": "PAMI-6",
    "author": [
      {
        "given": "Stuart",
        "family": "Geman"
      },
      {
        "given": "Donald",
        "family": "Geman"
      }
    ],
    "container-title": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
    "issued": {
      "date-parts": [
        [
          1984,
          11
        ]
      ]
    },
    "URL": "https://doi.org/bpv4j5",
    "container-title-short": "IEEE Trans. Pattern Anal. Mach. Intell.",
    "PMID": "22499653",
    "id": "SXlx1fmn",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.1109/tpami.1984.4767596"
  },
  {
    "publisher": "Institute of Electrical and Electronics Engineers (IEEE)",
    "issue": "8",
    "DOI": "10.1109/tpami.2013.50",
    "type": "article-journal",
    "page": "1798-1828",
    "source": "Crossref",
    "title": "Representation Learning: A Review and New Perspectives",
    "volume": "35",
    "author": [
      {
        "given": "Y.",
        "family": "Bengio"
      },
      {
        "given": "A.",
        "family": "Courville"
      },
      {
        "given": "P.",
        "family": "Vincent"
      }
    ],
    "container-title": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
    "issued": {
      "date-parts": [
        [
          2013,
          8
        ]
      ]
    },
    "URL": "https://doi.org/f42hw4",
    "container-title-short": "IEEE Trans. Pattern Anal. Mach. Intell.",
    "PMID": "23787338",
    "id": "SVoGevDg",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.1109/tpami.2013.50"
  },
  {
    "publisher": "Institute of Electrical and Electronics Engineers (IEEE)",
    "issue": "3",
    "DOI": "10.1109/tr.2016.2570568",
    "type": "article-journal",
    "page": "1314-1326",
    "source": "Crossref",
    "title": "A Model-Based Method for Remaining Useful Life Prediction of Machinery",
    "volume": "65",
    "author": [
      {
        "given": "Yaguo",
        "family": "Lei"
      },
      {
        "given": "Naipeng",
        "family": "Li"
      },
      {
        "given": "Szymon",
        "family": "Gontarz"
      },
      {
        "given": "Jing",
        "family": "Lin"
      },
      {
        "given": "Stanislaw",
        "family": "Radkowski"
      },
      {
        "given": "Jacek",
        "family": "Dybala"
      }
    ],
    "container-title": "IEEE Transactions on Reliability",
    "issued": {
      "date-parts": [
        [
          2016,
          9
        ]
      ]
    },
    "URL": "https://doi.org/f82bw2",
    "container-title-short": "IEEE Trans. Rel.",
    "id": "jH6SpM8M",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.1109/tr.2016.2570568"
  },
  {
    "publisher": "Institute of Electrical and Electronics Engineers (IEEE)",
    "issue": "5",
    "DOI": "10.1109/tsmc.1983.6313077",
    "type": "article-journal",
    "page": "834-846",
    "source": "Crossref",
    "title": "Neuronlike adaptive elements that can solve difficult learning control problems",
    "volume": "SMC-13",
    "author": [
      {
        "given": "Andrew G.",
        "family": "Barto"
      },
      {
        "given": "Richard S.",
        "family": "Sutton"
      },
      {
        "given": "Charles W.",
        "family": "Anderson"
      }
    ],
    "container-title": "IEEE Transactions on Systems, Man, and Cybernetics",
    "issued": {
      "date-parts": [
        [
          1983,
          9
        ]
      ]
    },
    "URL": "https://doi.org/gddhk6",
    "container-title-short": "IEEE Trans. Syst., Man, Cybern.",
    "id": "YAPw7zCk",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.1109/tsmc.1983.6313077"
  },
  {
    "publisher": "IEEE",
    "DOI": "10.1109/wetice57085.2023.10477842",
    "type": "paper-conference",
    "page": "1-6",
    "source": "Crossref",
    "title": "Predictive Maintenance Approaches in Industry 4.0: A Systematic Literature Review",
    "author": [
      {
        "given": "Fidma Mohamed",
        "family": "Abdelillah"
      },
      {
        "given": "Hamour",
        "family": "Nora"
      },
      {
        "given": "Ouchani",
        "family": "Samir"
      },
      {
        "given": "Sidi Mohamed",
        "family": "Benslimane"
      }
    ],
    "event": "2023 IEEE International Conference on Enabling Technologies: Infrastructure for Collaborative Enterprises (WETICE)",
    "container-title": "2023 IEEE International Conference on Enabling Technologies: Infrastructure for Collaborative Enterprises (WETICE)",
    "issued": {
      "date-parts": [
        [
          2023,
          12,
          14
        ]
      ]
    },
    "URL": "https://doi.org/g96qvf",
    "id": "8MSb5kh8",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.1109/wetice57085.2023.10477842"
  },
  {
    "publisher": "Oxford University Press (OUP)",
    "issue": "1",
    "abstract": "<jats:title>Summary</jats:title><jats:p>We consider the problem of selecting grouped variables (factors) for accurate prediction in regression. Such a problem arises naturally in many practical situations with the multifactor analysis-of-variance problem as the most important and well-known example. Instead of selecting factors by stepwise backward elimination, we focus on the accuracy of estimation and consider extensions of the lasso, the LARS algorithm and the non-negative garrotte for factor selection. The lasso, the LARS algorithm and the non-negative garrotte are recently proposed regression methods that can be used to select individual variables. We study and propose efficient algorithms for the extensions of these methods for factor selection and show that these extensions give superior performance to the traditional stepwise backward elimination method in factor selection problems. We study the similarities and the differences between these methods. Simulations and real examples are used to illustrate the methods.</jats:p>",
    "DOI": "10.1111/j.1467-9868.2005.00532.x",
    "type": "article-journal",
    "page": "49-67",
    "source": "Crossref",
    "title": "Model Selection and Estimation in Regression with Grouped Variables",
    "volume": "68",
    "author": [
      {
        "given": "Ming",
        "family": "Yuan"
      },
      {
        "given": "Yi",
        "family": "Lin"
      }
    ],
    "container-title": "Journal of the Royal Statistical Society Series B: Statistical Methodology",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2005,
          12,
          21
        ]
      ]
    },
    "URL": "https://doi.org/fvntrn",
    "id": "kfKaakAe",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.1111/j.1467-9868.2005.00532.x"
  },
  {
    "publisher": "Oxford University Press (OUP)",
    "issue": "2",
    "abstract": "<jats:title>Summary</jats:title>\n               <jats:p>A sequence of 0’s and 1’s is observed and it is suspected that the chance that a particular trial is a 1 depends on the value of one or more independent variables. Tests and estimates for such situations are considered, dealing first with problems in which the independent variable is preassigned and then with independent variables that are functions of the sequence. There is a considerable amount of earlier work, which is reviewed.</jats:p>",
    "DOI": "10.1111/j.2517-6161.1958.tb00292.x",
    "type": "article-journal",
    "page": "215-232",
    "source": "Crossref",
    "title": "The Regression Analysis of Binary Sequences",
    "volume": "20",
    "author": [
      {
        "given": "D. R.",
        "family": "Cox"
      }
    ],
    "container-title": "Journal of the Royal Statistical Society Series B: Statistical Methodology",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          1958,
          7,
          1
        ]
      ]
    },
    "URL": "https://doi.org/gfzf66",
    "id": "fzXXj6RE",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.1111/j.2517-6161.1958.tb00292.x"
  },
  {
    "publisher": "Oxford University Press (OUP)",
    "issue": "1",
    "abstract": "<jats:title>Summary</jats:title>\n               <jats:p>The usual linear statistical model is reanalyzed using Bayesian methods and the concept of exchangeability. The general method is illustrated by applications to two-factor experimental designs and multiple regression.</jats:p>",
    "DOI": "10.1111/j.2517-6161.1972.tb00885.x",
    "type": "article-journal",
    "page": "1-18",
    "source": "Crossref",
    "title": "Bayes Estimates for the Linear Model",
    "volume": "34",
    "author": [
      {
        "given": "D. V.",
        "family": "Lindley"
      },
      {
        "given": "A. F. M.",
        "family": "Smith"
      }
    ],
    "container-title": "Journal of the Royal Statistical Society Series B: Statistical Methodology",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          1972,
          9,
          1
        ]
      ]
    },
    "URL": "https://doi.org/gg2vv3",
    "id": "mLMS4pAF",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.1111/j.2517-6161.1972.tb00885.x"
  },
  {
    "publisher": "Oxford University Press (OUP)",
    "issue": "4",
    "abstract": "<jats:title>SUMMARY</jats:title>\n               <jats:p>We explore a class of regression and generalized regression models in which the coefficients are allowed to vary as smooth functions of other variables. General algorithms are presented for estimating the models flexibly and some examples are given. This class of models ties together generalized additive models and dynamic generalized linear models into one common framework. When applied to the proportional hazards model for survival data, this approach provides a new way of modelling departures from the proportional hazards assumption.</jats:p>",
    "DOI": "10.1111/j.2517-6161.1993.tb01939.x",
    "type": "article-journal",
    "page": "757-779",
    "source": "Crossref",
    "title": "Varying-Coefficient Models",
    "volume": "55",
    "author": [
      {
        "given": "Trevor",
        "family": "Hastie"
      },
      {
        "given": "Robert",
        "family": "Tibshirani"
      }
    ],
    "container-title": "Journal of the Royal Statistical Society Series B: Statistical Methodology",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          1993,
          9,
          1
        ]
      ]
    },
    "URL": "https://doi.org/gmfvmb",
    "id": "ugXwusl0",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.1111/j.2517-6161.1993.tb01939.x"
  },
  {
    "publisher": "Oxford University Press (OUP)",
    "issue": "1",
    "abstract": "<jats:title>SUMMARY</jats:title>\n                  <jats:p>We propose a new method for estimation in linear models. The ‘lasso’ minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described.</jats:p>",
    "DOI": "10.1111/j.2517-6161.1996.tb02080.x",
    "type": "article-journal",
    "page": "267-288",
    "source": "Crossref",
    "title": "Regression Shrinkage and Selection Via the Lasso",
    "volume": "58",
    "author": [
      {
        "given": "Robert",
        "family": "Tibshirani"
      }
    ],
    "container-title": "Journal of the Royal Statistical Society Series B: Statistical Methodology",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          1996,
          1,
          1
        ]
      ]
    },
    "URL": "https://doi.org/gfn45m",
    "id": "kX2zf6UE",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.1111/j.2517-6161.1996.tb02080.x"
  },
  {
    "publisher": "Oxford University Press (OUP)",
    "issue": "2",
    "abstract": "<jats:title>Summary</jats:title><jats:p>We consider the problem of estimating multiple related Gaussian graphical models from a high dimensional data set with observations belonging to distinct classes. We propose the joint graphical lasso, which borrows strength across the classes to estimate multiple graphical models that share certain characteristics, such as the locations or weights of non-zero edges. Our approach is based on maximizing a penalized log-likelihood. We employ generalized fused lasso or group lasso penalties and implement a fast alternating directions method of multipliers algorithm to solve the corresponding convex optimization problems. The performance of the method proposed is illustrated through simulated and real data examples.</jats:p>",
    "DOI": "10.1111/rssb.12033",
    "type": "article-journal",
    "page": "373-397",
    "source": "Crossref",
    "title": "The Joint Graphical Lasso for Inverse Covariance Estimation Across Multiple Classes",
    "volume": "76",
    "author": [
      {
        "given": "Patrick",
        "family": "Danaher"
      },
      {
        "given": "Pei",
        "family": "Wang"
      },
      {
        "given": "Daniela M.",
        "family": "Witten"
      }
    ],
    "container-title": "Journal of the Royal Statistical Society Series B: Statistical Methodology",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2013,
          8,
          12
        ]
      ]
    },
    "URL": "https://doi.org/f5sj9g",
    "PMCID": "PMC4012833",
    "PMID": "24817823",
    "id": "JDoK9thg",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.1111/rssb.12033"
  },
  {
    "publisher": "ASME International",
    "issue": "1",
    "abstract": "<jats:p>The classical filtering and prediction problem is re-examined using the Bode-Shannon representation of random processes and the “state-transition” method of analysis of dynamic systems. New results are: (1) The formulation and methods of solution of the problem apply without modification to stationary and nonstationary statistics and to growing-memory and infinite-memory filters. (2) A nonlinear difference (or differential) equation is derived for the covariance matrix of the optimal estimation error. From the solution of this equation the co-efficients of the difference (or differential) equation of the optimal linear filter are obtained without further calculations. (3) The filtering problem is shown to be the dual of the noise-free regulator problem. The new method developed here is applied to two well-known problems, confirming and extending earlier results. The discussion is largely self-contained and proceeds from first principles; basic concepts of the theory of random processes are reviewed in the Appendix.</jats:p>",
    "DOI": "10.1115/1.3662552",
    "type": "article-journal",
    "page": "35-45",
    "source": "Crossref",
    "title": "A New Approach to Linear Filtering and Prediction Problems",
    "volume": "82",
    "author": [
      {
        "given": "R. E.",
        "family": "Kalman"
      }
    ],
    "container-title": "Journal of Basic Engineering",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          1960,
          3,
          1
        ]
      ]
    },
    "URL": "https://doi.org/dmftj3",
    "id": "RauXHY7u",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.1115/1.3662552"
  },
  {
    "publisher": "Acoustical Society of America (ASA)",
    "issue": "2B",
    "abstract": "<jats:p>We describe a procedure for efficient encoding of the speech wave by representing it in terms of time-varying parameters related to the transfer function of the vocal tract and the characteristics of the excitation. The speech wave, sampled at 10 kHz, is analyzed by predicting the present speech sample as a linear combination of the 12 previous samples. The 12 predictor coefficients are determined by minimizing the mean-squared error between the actual and the predicted values of the speech samples. Fifteen parameters—namely, the 12 predictor coefficients, the pitch period, a binary parameter indicating whether the speech is voiced or unvoiced, and the rms value of the speech samples—are derived by analysis of the speech wave, encoded and transmitted to the synthesizer. The speech wave is synthesized as the output of a linear recursive filter excited by either a sequence of quasiperiodic pulses or a white-noise source. Application of this method for efficient transmission and storage of speech signals as well as procedures for determining other speech characteristics, such as formant frequencies and bandwidths, the spectral envelope, and the autocorrelation function, are discussed.</jats:p>",
    "DOI": "10.1121/1.1912679",
    "type": "article-journal",
    "page": "637-655",
    "source": "Crossref",
    "title": "Speech Analysis and Synthesis by Linear Prediction of the Speech Wave",
    "volume": "50",
    "author": [
      {
        "given": "B. S.",
        "family": "Atal"
      },
      {
        "given": "Suzanne L.",
        "family": "Hanauer"
      }
    ],
    "container-title": "The Journal of the Acoustical Society of America",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          1971,
          8,
          1
        ]
      ]
    },
    "URL": "https://doi.org/cc657m",
    "PMID": "4106390",
    "id": "1F4l9zw7H",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.1121/1.1912679"
  },
  {
    "publisher": "ACM",
    "DOI": "10.1145/1557019.1557041",
    "type": "paper-conference",
    "page": "139-148",
    "source": "Crossref",
    "title": "New ensemble methods for evolving data streams",
    "author": [
      {
        "given": "Albert",
        "family": "Bifet"
      },
      {
        "given": "Geoff",
        "family": "Holmes"
      },
      {
        "given": "Bernhard",
        "family": "Pfahringer"
      },
      {
        "given": "Richard",
        "family": "Kirkby"
      },
      {
        "given": "Ricard",
        "family": "Gavaldà"
      }
    ],
    "event": "KDD09: The 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
    "container-title": "Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining",
    "issued": {
      "date-parts": [
        [
          2009,
          6,
          28
        ]
      ]
    },
    "URL": "https://doi.org/dkxcrj",
    "id": "yml0Vh7r",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.1145/1557019.1557041"
  },
  {
    "publisher": "ACM",
    "DOI": "10.1145/1772690.1772758",
    "type": "paper-conference",
    "page": "661-670",
    "source": "Crossref",
    "title": "A contextual-bandit approach to personalized news article recommendation",
    "author": [
      {
        "given": "Lihong",
        "family": "Li"
      },
      {
        "given": "Wei",
        "family": "Chu"
      },
      {
        "given": "John",
        "family": "Langford"
      },
      {
        "given": "Robert E.",
        "family": "Schapire"
      }
    ],
    "event": "WWW '10: The 19th International World Wide Web Conference",
    "container-title": "Proceedings of the 19th international conference on World wide web",
    "issued": {
      "date-parts": [
        [
          2010,
          4,
          26
        ]
      ]
    },
    "URL": "https://doi.org/bpkcx9",
    "id": "On9K3pxW",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.1145/1772690.1772758"
  },
  {
    "publisher": "Association for Computing Machinery (ACM)",
    "issue": "4",
    "abstract": "<jats:p>Concept drift primarily refers to an online supervised learning scenario when the relation between the input data and the target variable changes over time. Assuming a general knowledge of supervised learning in this article, we characterize adaptive learning processes; categorize existing strategies for handling concept drift; overview the most representative, distinct, and popular techniques and algorithms; discuss evaluation methodology of adaptive algorithms; and present a set of illustrative applications. The survey covers the different facets of concept drift in an integrated way to reflect on the existing scattered state of the art. Thus, it aims at providing a comprehensive introduction to the concept drift adaptation for researchers, industry analysts, and practitioners.</jats:p>",
    "DOI": "10.1145/2523813",
    "type": "article-journal",
    "page": "1-37",
    "source": "Crossref",
    "title": "A survey on concept drift adaptation",
    "volume": "46",
    "author": [
      {
        "given": "João",
        "family": "Gama"
      },
      {
        "given": "Indrė",
        "family": "Žliobaitė"
      },
      {
        "given": "Albert",
        "family": "Bifet"
      },
      {
        "given": "Mykola",
        "family": "Pechenizkiy"
      },
      {
        "given": "Abdelhamid",
        "family": "Bouchachia"
      }
    ],
    "container-title": "ACM Computing Surveys",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2014,
          3
        ]
      ]
    },
    "URL": "https://doi.org/gd893p",
    "container-title-short": "ACM Comput. Surv.",
    "id": "BkxeQdkH",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.1145/2523813"
  },
  {
    "publisher": "ACM",
    "DOI": "10.1145/2939672.2939785",
    "type": "paper-conference",
    "page": "785-794",
    "source": "Crossref",
    "title": "XGBoost",
    "author": [
      {
        "given": "Tianqi",
        "family": "Chen"
      },
      {
        "given": "Carlos",
        "family": "Guestrin"
      }
    ],
    "event": "KDD '16: The 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
    "container-title": "Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
    "issued": {
      "date-parts": [
        [
          2016,
          8,
          13
        ]
      ]
    },
    "URL": "https://doi.org/gdp84q",
    "id": "8w9fI63O",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.1145/2939672.2939785"
  },
  {
    "publisher": "ACM",
    "DOI": "10.1145/3097983.3098066",
    "type": "paper-conference",
    "page": "275-284",
    "source": "Crossref",
    "title": "The Selective Labels Problem",
    "author": [
      {
        "given": "Himabindu",
        "family": "Lakkaraju"
      },
      {
        "given": "Jon",
        "family": "Kleinberg"
      },
      {
        "given": "Jure",
        "family": "Leskovec"
      },
      {
        "given": "Jens",
        "family": "Ludwig"
      },
      {
        "given": "Sendhil",
        "family": "Mullainathan"
      }
    ],
    "event": "KDD '17: The 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
    "container-title": "Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
    "issued": {
      "date-parts": [
        [
          2017,
          8,
          4
        ]
      ]
    },
    "URL": "https://doi.org/ggd7hz",
    "PMCID": "PMC5958915",
    "PMID": "29780658",
    "id": "MGkiKe9y",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.1145/3097983.3098066"
  },
  {
    "publisher": "Association for Computing Machinery (ACM)",
    "issue": "3",
    "abstract": "<jats:p>Machine learning has been highly successful in data-intensive applications but is often hampered when the data set is small. Recently, Few-shot Learning (FSL) is proposed to tackle this problem. Using prior knowledge, FSL can rapidly generalize to new tasks containing only a few samples with supervised information. In this article, we conduct a thorough survey to fully understand FSL. Starting from a formal definition of FSL, we distinguish FSL from several relevant machine learning problems. We then point out that the core issue in FSL is that the empirical risk minimizer is unreliable. Based on how prior knowledge can be used to handle this core issue, we categorize FSL methods from three perspectives: (i) data, which uses prior knowledge to augment the supervised experience; (ii) model, which uses prior knowledge to reduce the size of the hypothesis space; and (iii) algorithm, which uses prior knowledge to alter the search for the best hypothesis in the given hypothesis space. With this taxonomy, we review and discuss the pros and cons of each category. Promising directions, in the aspects of the FSL problem setups, techniques, applications, and theories, are also proposed to provide insights for future research.<jats:sup>1</jats:sup></jats:p>",
    "DOI": "10.1145/3386252",
    "type": "article-journal",
    "page": "1-34",
    "source": "Crossref",
    "title": "Generalizing from a Few Examples",
    "volume": "53",
    "author": [
      {
        "given": "Yaqing",
        "family": "Wang"
      },
      {
        "given": "Quanming",
        "family": "Yao"
      },
      {
        "given": "James T.",
        "family": "Kwok"
      },
      {
        "given": "Lionel M.",
        "family": "Ni"
      }
    ],
    "container-title": "ACM Computing Surveys",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2020,
          6,
          12
        ]
      ]
    },
    "URL": "https://doi.org/gg37m2",
    "container-title-short": "ACM Comput. Surv.",
    "id": "S1Lim9f9",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.1145/3386252"
  },
  {
    "publisher": "ACM",
    "DOI": "10.1145/3442188.3445922",
    "type": "paper-conference",
    "page": "610-623",
    "source": "Crossref",
    "title": "On the Dangers of Stochastic Parrots",
    "author": [
      {
        "given": "Emily M.",
        "family": "Bender"
      },
      {
        "given": "Timnit",
        "family": "Gebru"
      },
      {
        "given": "Angelina",
        "family": "McMillan-Major"
      },
      {
        "given": "Shmargaret",
        "family": "Shmitchell"
      }
    ],
    "event": "FAccT '21: 2021 ACM Conference on Fairness, Accountability, and Transparency",
    "container-title": "Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",
    "issued": {
      "date-parts": [
        [
          2021,
          3
        ]
      ]
    },
    "URL": "https://doi.org/gh677h",
    "id": "At7PZDaG",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.1145/3442188.3445922"
  },
  {
    "publisher": "Association for Computing Machinery (ACM)",
    "issue": "9",
    "abstract": "<jats:p>\n            This article surveys and organizes research works in a new paradigm in natural language processing, which we dub “prompt-based learning.” Unlike traditional supervised learning, which trains a model to take in an input\n            <jats:bold>\n              <jats:italic>x</jats:italic>\n            </jats:bold>\n            and predict an output\n            <jats:bold>\n              <jats:italic>y</jats:italic>\n            </jats:bold>\n            as\n            <jats:italic>P</jats:italic>\n            (\n            <jats:bold>\n              <jats:italic>y|x</jats:italic>\n            </jats:bold>\n            ), prompt-based learning is based on language models that model the probability of text directly. To use these models to perform prediction tasks, the original input\n            <jats:bold>\n              <jats:italic>x</jats:italic>\n            </jats:bold>\n            is modified using a\n            <jats:italic>template</jats:italic>\n            into a textual string\n            <jats:italic>prompt</jats:italic>\n            <jats:bold>\n              <jats:italic>x′</jats:italic>\n            </jats:bold>\n            that has some unfilled slots, and then the language model is used to probabilistically fill the unfilled information to obtain a final string\n            <jats:bold>\n              <jats:italic>x̂</jats:italic>\n            </jats:bold>\n            , from which the final output\n            <jats:bold>\n              <jats:italic>y</jats:italic>\n            </jats:bold>\n            can be derived. This framework is powerful and attractive for a number of reasons: It allows the language model to be\n            <jats:italic>pre-trained</jats:italic>\n            on massive amounts of raw text, and by defining a new prompting function the model is able to perform\n            <jats:italic>few-shot</jats:italic>\n            or even\n            <jats:italic>zero-shot</jats:italic>\n            learning, adapting to new scenarios with few or no labeled data. In this article, we introduce the basics of this promising paradigm, describe a unified set of mathematical notations that can cover a wide variety of existing work, and organize existing work along several dimensions, e.g., the choice of pre-trained language models, prompts, and tuning strategies. To make the field more accessible to interested beginners, we not only make a systematic review of existing works and a highly structured typology of prompt-based concepts but also release other resources, e.g., a website\n            <jats:ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"url\" xlink:href=\"http://pretrain.nlpedia.ai/\">NLPedia–Pretrain</jats:ext-link>\n            including constantly updated survey and paperlist.\n          </jats:p>",
    "DOI": "10.1145/3560815",
    "type": "article-journal",
    "page": "1-35",
    "source": "Crossref",
    "title": "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing",
    "volume": "55",
    "author": [
      {
        "given": "Pengfei",
        "family": "Liu"
      },
      {
        "given": "Weizhe",
        "family": "Yuan"
      },
      {
        "given": "Jinlan",
        "family": "Fu"
      },
      {
        "given": "Zhengbao",
        "family": "Jiang"
      },
      {
        "given": "Hiroaki",
        "family": "Hayashi"
      },
      {
        "given": "Graham",
        "family": "Neubig"
      }
    ],
    "container-title": "ACM Computing Surveys",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2023,
          1,
          16
        ]
      ]
    },
    "URL": "https://doi.org/gq5fh2",
    "container-title-short": "ACM Comput. Surv.",
    "id": "12nAa0T4v",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.1145/3560815"
  },
  {
    "publisher": "Association for Computing Machinery (ACM)",
    "issue": "11",
    "abstract": "<jats:p>In a document retrieval, or other pattern matching environment where stored entities (documents) are compared with each other or with incoming patterns (search requests), it appears that the best indexing (property) space is one where each entity lies as far away from the others as possible; in these circumstances the value of an indexing system may be expressible as a function of the density of the object space; in particular, retrieval performance may correlate inversely with space density. An approach based on space density computations is used to choose an optimum indexing vocabulary for a collection of documents. Typical evaluation results are shown, demonstating the usefulness of the model.</jats:p>",
    "DOI": "10.1145/361219.361220",
    "type": "article-journal",
    "page": "613-620",
    "source": "Crossref",
    "title": "A vector space model for automatic indexing",
    "volume": "18",
    "author": [
      {
        "given": "G.",
        "family": "Salton"
      },
      {
        "given": "A.",
        "family": "Wong"
      },
      {
        "given": "C. S.",
        "family": "Yang"
      }
    ],
    "container-title": "Communications of the ACM",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          1975,
          11
        ]
      ]
    },
    "URL": "https://doi.org/fw8vv8",
    "container-title-short": "Commun. ACM",
    "id": "u6KGkrtr",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.1145/361219.361220"
  },
  {
    "publisher": "MIT Press",
    "issue": "1",
    "abstract": "<jats:p> We present a new supervised learning procedure for systems composed of many separate networks, each of which learns to handle a subset of the complete set of training cases. The new procedure can be viewed either as a modular version of a multilayer supervised network, or as an associative version of competitive learning. It therefore provides a new link between these two apparently different approaches. We demonstrate that the learning procedure divides up a vowel discrimination task into appropriate subtasks, each of which can be solved by a very simple expert network. </jats:p>",
    "DOI": "10.1162/neco.1991.3.1.79",
    "type": "article-journal",
    "page": "79-87",
    "source": "Crossref",
    "title": "Adaptive Mixtures of Local Experts",
    "volume": "3",
    "author": [
      {
        "given": "Robert A.",
        "family": "Jacobs"
      },
      {
        "given": "Michael I.",
        "family": "Jordan"
      },
      {
        "given": "Steven J.",
        "family": "Nowlan"
      },
      {
        "given": "Geoffrey E.",
        "family": "Hinton"
      }
    ],
    "container-title": "Neural Computation",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          1991,
          2
        ]
      ]
    },
    "URL": "https://doi.org/cnsnqg",
    "container-title-short": "Neural Computation",
    "PMID": "31141872",
    "id": "ljL7YbZD",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.1162/neco.1991.3.1.79"
  },
  {
    "publisher": "SAGE Publications",
    "issue": "11",
    "abstract": "<jats:sec><jats:title>Aim</jats:title><jats:p> To present a flexible model for repeated measures longitudinal growth data within individuals that allows trends over time to incorporate individual-specific random effects. These may reflect the timing of growth events and characterise within-individual variability which can be modelled as a function of age. </jats:p></jats:sec><jats:sec><jats:title>Subjects and methods</jats:title><jats:p> A Bayesian model is developed that includes random effects for the mean growth function, an individual age-alignment random effect and random effects for the within-individual variance function. This model is applied to data on boys’ heights from the Edinburgh longitudinal growth study and to repeated weight measurements of a sample of pregnant women in the Avon Longitudinal Study of Parents and Children cohort. </jats:p></jats:sec><jats:sec><jats:title>Results</jats:title><jats:p> The mean age at which the growth curves for individual boys are aligned is 11.4 years, corresponding to the mean ‘take off’ age for pubertal growth. The within-individual variance (standard deviation) is found to decrease from 0.24 cm<jats:sup>2</jats:sup> (0.50 cm) at 9 years for the ‘average’ boy to 0.07 cm<jats:sup>2</jats:sup> (0.25 cm) at 16 years. Change in weight during pregnancy can be characterised by regression splines with random effects that include a large woman-specific random effect for the within-individual variation, which is also correlated with overall weight and weight gain. </jats:p></jats:sec><jats:sec><jats:title>Conclusions</jats:title><jats:p> The proposed model provides a useful extension to existing approaches, allowing considerable flexibility in describing within- and between-individual differences in growth patterns. </jats:p></jats:sec>",
    "DOI": "10.1177/0962280217706728",
    "type": "article-journal",
    "page": "3478-3491",
    "source": "Crossref",
    "title": "Multilevel growth curve models that incorporate a random coefficient model for the level 1 variance function",
    "volume": "27",
    "author": [
      {
        "given": "Harvey",
        "family": "Goldstein"
      },
      {
        "given": "George",
        "family": "Leckie"
      },
      {
        "given": "Christopher",
        "family": "Charlton"
      },
      {
        "given": "Kate",
        "family": "Tilling"
      },
      {
        "given": "William J",
        "family": "Browne"
      }
    ],
    "container-title": "Statistical Methods in Medical Research",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2017,
          5,
          1
        ]
      ]
    },
    "URL": "https://doi.org/f95xmx",
    "container-title-short": "Stat Methods Med Res",
    "PMID": "28459180",
    "id": "1BXylhj0T",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.1177/0962280217706728"
  },
  {
    "publisher": "Informa UK Limited",
    "issue": "496",
    "DOI": "10.1198/jasa.2011.tm10465",
    "type": "article-journal",
    "page": "1418-1433",
    "source": "Crossref",
    "title": "Bayesian Inference for General Gaussian Graphical Models With Application to Multivariate Lattice Data",
    "volume": "106",
    "author": [
      {
        "given": "Adrian",
        "family": "Dobra"
      },
      {
        "given": "Alex",
        "family": "Lenkoski"
      },
      {
        "given": "Abel",
        "family": "Rodriguez"
      }
    ],
    "container-title": "Journal of the American Statistical Association",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2011,
          12
        ]
      ]
    },
    "URL": "https://doi.org/fxq5wh",
    "container-title-short": "Journal of the American Statistical Association",
    "PMCID": "PMC4767185",
    "PMID": "26924867",
    "id": "721WoKJr",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.1198/jasa.2011.tm10465"
  },
  {
    "publisher": "Chapman and Hall/CRC",
    "DOI": "10.1201/b16018",
    "type": "book",
    "source": "Crossref",
    "title": "Bayesian Data Analysis",
    "author": [
      {
        "given": "Andrew",
        "family": "Gelman"
      },
      {
        "given": "John B.",
        "family": "Carlin"
      },
      {
        "given": "Hal S.",
        "family": "Stern"
      },
      {
        "given": "David B.",
        "family": "Dunson"
      },
      {
        "given": "Aki",
        "family": "Vehtari"
      },
      {
        "given": "Donald B.",
        "family": "Rubin"
      }
    ],
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2013,
          11,
          27
        ]
      ]
    },
    "URL": "https://doi.org/gqfx8g",
    "id": "Jk46kTvA",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.1201/b16018"
  },
  {
    "publisher": "Institute of Mathematical Statistics",
    "issue": "3",
    "DOI": "10.1214/009053606000000281",
    "type": "article-journal",
    "source": "Crossref",
    "title": "High-dimensional graphs and variable selection with the Lasso",
    "volume": "34",
    "author": [
      {
        "given": "Nicolai",
        "family": "Meinshausen"
      },
      {
        "given": "Peter",
        "family": "Bühlmann"
      }
    ],
    "container-title": "The Annals of Statistics",
    "issued": {
      "date-parts": [
        [
          2006,
          6,
          1
        ]
      ]
    },
    "URL": "https://doi.org/fwt5kt",
    "container-title-short": "Ann. Statist.",
    "id": "eFgbj4Kw",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.1214/009053606000000281"
  },
  {
    "publisher": "Institute of Mathematical Statistics",
    "issue": "1",
    "DOI": "10.1214/09-aoas308",
    "type": "article-journal",
    "source": "Crossref",
    "title": "Estimating time-varying networks",
    "volume": "4",
    "author": [
      {
        "given": "Mladen",
        "family": "Kolar"
      },
      {
        "given": "Le",
        "family": "Song"
      },
      {
        "given": "Amr",
        "family": "Ahmed"
      },
      {
        "given": "Eric P.",
        "family": "Xing"
      }
    ],
    "container-title": "The Annals of Applied Statistics",
    "issued": {
      "date-parts": [
        [
          2010,
          3,
          1
        ]
      ]
    },
    "URL": "https://doi.org/b3rn6q",
    "container-title-short": "Ann. Appl. Stat.",
    "id": "lAsTg3IH",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.1214/09-aoas308"
  },
  {
    "publisher": "Institute of Mathematical Statistics",
    "issue": "-1",
    "DOI": "10.1214/24-ba1470",
    "type": "article-journal",
    "source": "Crossref",
    "title": "VCBART: Bayesian Trees for Varying Coefficients",
    "volume": "-1",
    "author": [
      {
        "given": "Sameer K.",
        "family": "Deshpande"
      },
      {
        "given": "Ray",
        "family": "Bai"
      },
      {
        "given": "Cecilia",
        "family": "Balocchi"
      },
      {
        "given": "Jennifer E.",
        "family": "Starling"
      },
      {
        "given": "Jordan",
        "family": "Weiss"
      }
    ],
    "container-title": "Bayesian Analysis",
    "issued": {
      "date-parts": [
        [
          2024,
          1,
          1
        ]
      ]
    },
    "URL": "https://doi.org/g977nk",
    "container-title-short": "Bayesian Anal.",
    "id": "pxvmdyAr",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.1214/24-ba1470"
  },
  {
    "publisher": "Institute of Mathematical Statistics",
    "issue": "5",
    "DOI": "10.1214/aos/1017939139",
    "type": "article-journal",
    "source": "Crossref",
    "title": "Statistical estimation in varying coefficient models",
    "volume": "27",
    "author": [
      {
        "given": "Jianqing",
        "family": "Fan"
      },
      {
        "given": "Wenyang",
        "family": "Zhang"
      }
    ],
    "container-title": "The Annals of Statistics",
    "issued": {
      "date-parts": [
        [
          1999,
          10,
          1
        ]
      ]
    },
    "URL": "https://doi.org/dsxd4s",
    "container-title-short": "Ann. Statist.",
    "id": "l6vMkIsa",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.1214/aos/1017939139"
  },
  {
    "publisher": "Institute of Mathematical Statistics",
    "issue": "6",
    "DOI": "10.1214/aos/1030741083",
    "type": "article-journal",
    "source": "Crossref",
    "title": "Optimal pointwise adaptive methods in nonparametric estimation",
    "volume": "25",
    "author": [
      {
        "given": "O. V.",
        "family": "Lepski"
      },
      {
        "given": "V. G.",
        "family": "Spokoiny"
      }
    ],
    "container-title": "The Annals of Statistics",
    "issued": {
      "date-parts": [
        [
          1997,
          12,
          1
        ]
      ]
    },
    "URL": "https://doi.org/fwzw52",
    "container-title-short": "Ann. Statist.",
    "id": "TBU9RF9F",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.1214/aos/1030741083"
  },
  {
    "publisher": "Institute of Mathematical Statistics",
    "issue": "3",
    "DOI": "10.1214/ss/1177013604",
    "type": "article-journal",
    "source": "Crossref",
    "title": "Generalized Additive Models",
    "volume": "1",
    "author": [
      {
        "given": "Trevor",
        "family": "Hastie"
      },
      {
        "given": "Robert",
        "family": "Tibshirani"
      }
    ],
    "container-title": "Statistical Science",
    "issued": {
      "date-parts": [
        [
          1986,
          8,
          1
        ]
      ]
    },
    "URL": "https://doi.org/cjx3mk",
    "container-title-short": "Statist. Sci.",
    "id": "12q6JvZEW",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.1214/ss/1177013604"
  },
  {
    "publisher": "Emerald",
    "issue": "1",
    "DOI": "10.1561/2200000016",
    "type": "article-journal",
    "page": "1-122",
    "source": "Crossref",
    "title": "Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers",
    "volume": "3",
    "author": [
      {
        "given": "Stephen",
        "family": "Boyd"
      }
    ],
    "container-title": "Foundations and Trends® in Machine Learning",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2010
        ]
      ]
    },
    "URL": "https://doi.org/d3kztk",
    "container-title-short": "FNT in Machine Learning",
    "id": "W3XPrQXH",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.1561/2200000016"
  },
  {
    "publisher": "Emerald",
    "issue": "2",
    "abstract": "<jats:p>Online learning is a well established learning paradigm which has both theoretical and practical appeals. The goal of online learning is to make a sequence of accurate predictions given knowledge of the correct answer to previous prediction tasks and possibly additional available information. Online learning has been studied in several research fields including game theory, information theory, and machine learning. It also became of great interest to practitioners due the recent emergence of large scale applications such as online advertisement placement and online web ranking. In this survey we provide a modern overview of online learning. Our goal is to give the reader a sense of some of the interesting ideas and in particular to underscore the centrality of convexity in deriving efficient online learning algorithms. We do not mean to be comprehensive but rather to give a high-level, rigorous yet easy to follow, survey.</jats:p>",
    "DOI": "10.1561/2200000018",
    "type": "article-journal",
    "page": "107-194",
    "source": "Crossref",
    "title": "Online Learning and Online Convex Optimization",
    "volume": "4",
    "author": [
      {
        "given": "Shai",
        "family": "Shalev-Shwartz"
      }
    ],
    "container-title": "Foundations and Trends® in Machine Learning",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2025,
          12,
          20
        ]
      ]
    },
    "URL": "https://doi.org/gc7rf4",
    "id": "5S8Ulwe8",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.1561/2200000018"
  },
  {
    "publisher": "Association for Computational Linguistics",
    "DOI": "10.18653/v1/2023.emnlp-main.384",
    "type": "paper-conference",
    "page": "6253-6267",
    "source": "Crossref",
    "title": "Tree Prompting: Efficient Task Adaptation without Fine-Tuning",
    "author": [
      {
        "given": "Chandan",
        "family": "Singh"
      },
      {
        "given": "John",
        "family": "Morris"
      },
      {
        "given": "Alexander",
        "family": "Rush"
      },
      {
        "given": "Jianfeng",
        "family": "Gao"
      },
      {
        "given": "Yuntian",
        "family": "Deng"
      }
    ],
    "event": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    "container-title": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    "issued": {
      "date-parts": [
        [
          2023
        ]
      ]
    },
    "URL": "https://doi.org/gtgrkq",
    "id": "kAJDlMwy",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.18653/v1/2023.emnlp-main.384"
  },
  {
    "publisher": "The Open Journal",
    "issue": "97",
    "DOI": "10.21105/joss.06469",
    "type": "article-journal",
    "page": "6469",
    "source": "Crossref",
    "title": "Contextualized: Heterogeneous Modeling Toolbox",
    "volume": "9",
    "author": [
      {
        "given": "Caleb N.",
        "family": "Ellington"
      },
      {
        "given": "Benjamin J.",
        "family": "Lengerich"
      },
      {
        "given": "Wesley",
        "family": "Lo"
      },
      {
        "given": "Aaron",
        "family": "Alvarez"
      },
      {
        "given": "Andrea",
        "family": "Rubbi"
      },
      {
        "given": "Manolis",
        "family": "Kellis"
      },
      {
        "given": "Eric P.",
        "family": "Xing"
      }
    ],
    "container-title": "Journal of Open Source Software",
    "issued": {
      "date-parts": [
        [
          2024,
          5,
          8
        ]
      ]
    },
    "URL": "https://doi.org/gt68h8",
    "container-title-short": "JOSS",
    "id": "4cK1tiec",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.21105/joss.06469"
  },
  {
    "publisher": "Oxford University Press (OUP)",
    "issue": "3",
    "DOI": "10.2307/2337120",
    "type": "article-journal",
    "page": "471-483",
    "source": "Crossref",
    "title": "A class of pattern-mixture models for normal incomplete data",
    "volume": "81",
    "author": [
      {
        "given": "RODERICK J. A.",
        "family": "LITTLE"
      }
    ],
    "container-title": "Biometrika",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          1994
        ]
      ]
    },
    "URL": "https://doi.org/bqw3x9",
    "container-title-short": "Biometrika",
    "id": "DfpD095S",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.2307/2337120"
  },
  {
    "publisher": "JSTOR",
    "issue": "3",
    "DOI": "10.2307/2344614",
    "type": "article-journal",
    "page": "370",
    "source": "Crossref",
    "title": "Generalized Linear Models",
    "volume": "135",
    "author": [
      {
        "given": "J. A.",
        "family": "Nelder"
      },
      {
        "given": "R. W. M.",
        "family": "Wedderburn"
      }
    ],
    "container-title": "Journal of the Royal Statistical Society. Series A (General)",
    "issued": {
      "date-parts": [
        [
          1972
        ]
      ]
    },
    "URL": "https://doi.org/dhq253",
    "container-title-short": "Journal of the Royal Statistical Society. Series A (General)",
    "id": "kXBVO9y8",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.2307/2344614"
  },
  {
    "publisher": "JSTOR",
    "issue": "1",
    "DOI": "10.2307/2528966",
    "type": "article-journal",
    "page": "157",
    "source": "Crossref",
    "title": "Covariance Selection",
    "volume": "28",
    "author": [
      {
        "given": "A. P.",
        "family": "Dempster"
      }
    ],
    "container-title": "Biometrics",
    "issued": {
      "date-parts": [
        [
          1972,
          3
        ]
      ]
    },
    "URL": "https://doi.org/d5t49s",
    "container-title-short": "Biometrics",
    "id": "1AAscRzye",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.2307/2528966"
  },
  {
    "publisher": "JSTOR",
    "issue": "4",
    "DOI": "10.2307/2529876",
    "type": "article-journal",
    "page": "963",
    "source": "Crossref",
    "title": "Random-Effects Models for Longitudinal Data",
    "volume": "38",
    "author": [
      {
        "given": "Nan M.",
        "family": "Laird"
      },
      {
        "given": "James H.",
        "family": "Ware"
      }
    ],
    "container-title": "Biometrics",
    "issued": {
      "date-parts": [
        [
          1982,
          12
        ]
      ]
    },
    "URL": "https://doi.org/b8dmsr",
    "container-title-short": "Biometrics",
    "id": "7EbnZ6mY",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.2307/2529876"
  },
  {
    "publisher": "MDPI AG",
    "issue": "1",
    "abstract": "<jats:p>Recent advances in artificial intelligence (AI) have led to its widespread industrial adoption, with machine learning systems demonstrating superhuman performance in a significant number of tasks. However, this surge in performance, has often been achieved through increased model complexity, turning such systems into “black box” approaches and causing uncertainty regarding the way they operate and, ultimately, the way that they come to decisions. This ambiguity has made it problematic for machine learning systems to be adopted in sensitive yet critical domains, where their value could be immense, such as healthcare. As a result, scientific interest in the field of Explainable Artificial Intelligence (XAI), a field that is concerned with the development of new methods that explain and interpret machine learning models, has been tremendously reignited over recent years. This study focuses on machine learning interpretability methods; more specifically, a literature review and taxonomy of these methods are presented, as well as links to their programming implementations, in the hope that this survey would serve as a reference point for both theorists and practitioners.</jats:p>",
    "DOI": "10.3390/e23010018",
    "type": "article-journal",
    "page": "18",
    "source": "Crossref",
    "title": "Explainable AI: A Review of Machine Learning Interpretability Methods",
    "volume": "23",
    "author": [
      {
        "given": "Pantelis",
        "family": "Linardatos"
      },
      {
        "given": "Vasilis",
        "family": "Papastefanopoulos"
      },
      {
        "given": "Sotiris",
        "family": "Kotsiantis"
      }
    ],
    "container-title": "Entropy",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2020,
          12,
          25
        ]
      ]
    },
    "URL": "https://doi.org/gktm9k",
    "container-title-short": "Entropy",
    "PMCID": "PMC7824368",
    "PMID": "33375658",
    "id": "OsqhaAcF",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.3390/e23010018"
  },
  {
    "publisher": "MDPI AG",
    "issue": "2",
    "abstract": "<jats:p>Numerous papers have demonstrated that by using a varying coefficients model (VCM), researchers can unveil patterns of interactions between variables that could otherwise remain hidden if using the more popular regression model with an interaction term. Hence, one would expect high acceptance of the VCM as a tool for studying statistical interactions in datasets. Yet, the current paper shows that the VCM is still struggling to migrate from journals in which methods are presented to journals in which methods are utilized. First, a search in Google Scholar with the phrase “varying coefficients” returned ~79,200 results in comparison to returning ~2,710,000 results with the phrase “interaction term”. Second, a bibliometric analysis of publications with the VCM showed that in many research domains, there were more publications with the VCM in journals on methods than publications with the VCM in journals for empirical investigations. Economics and environmental studies stood out with many more publications with the VCM in empirical journals than in journals on statistical methods. The gap between the high acclaims of the VCM in the statistical literature and its low utilization rate in practice should be of concern to the research community. The possible reasons for this gap and its potential remedies are discussed.</jats:p>",
    "DOI": "10.3390/publications13020019",
    "type": "article-journal",
    "page": "19",
    "source": "Crossref",
    "title": "Publication Trends on the Varying Coefficients Model: Estimating the Actual (Under)Utilization of a Highly Acclaimed Method for Studying Statistical Interactions",
    "volume": "13",
    "author": [
      {
        "given": "Assaf",
        "family": "Botzer"
      }
    ],
    "container-title": "Publications",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2025,
          4,
          7
        ]
      ]
    },
    "URL": "https://doi.org/g9t2rq",
    "container-title-short": "Publications",
    "id": "hRp04fhf",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.3390/publications13020019"
  },
  {
    "publisher": "International Press of Boston",
    "issue": "1",
    "DOI": "10.4310/sii.2008.v1.n1.a15",
    "type": "article-journal",
    "page": "179-195",
    "source": "Crossref",
    "title": "Statistical methods with varying coefficient models",
    "volume": "1",
    "author": [
      {
        "given": "Jianqing",
        "family": "Fan"
      },
      {
        "given": "Wenyang",
        "family": "Zhang"
      }
    ],
    "container-title": "Statistics and Its Interface",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2008
        ]
      ]
    },
    "URL": "https://doi.org/gkq3kq",
    "PMCID": "PMC2575822",
    "PMID": "18978950",
    "id": "boDjf6IJ",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.4310/sii.2008.v1.n1.a15"
  },
  {
    "type": "article",
    "id": "EOUjThUk",
    "categories": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Kingma",
        "given": "Diederik P"
      },
      {
        "family": "Welling",
        "given": "Max"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2013
        ]
      ]
    },
    "abstract": "How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions are two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.",
    "DOI": "10.48550/arxiv.1312.6114",
    "publisher": "arXiv",
    "title": "Auto-Encoding Variational Bayes",
    "URL": "https://doi.org/gpp5xv",
    "version": "11",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.1312.6114"
  },
  {
    "type": "article",
    "id": "1G9auqG3f",
    "categories": [
      "Machine Learning (cs.LG)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Kingma",
        "given": "Diederik P."
      },
      {
        "family": "Ba",
        "given": "Jimmy"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2014
        ]
      ]
    },
    "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.",
    "DOI": "10.48550/arxiv.1412.6980",
    "publisher": "arXiv",
    "title": "Adam: A Method for Stochastic Optimization",
    "URL": "https://doi.org/hnkr",
    "version": "9",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.1412.6980"
  },
  {
    "type": "article",
    "id": "uXcqmE5X",
    "categories": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Ribeiro",
        "given": "Marco Tulio"
      },
      {
        "family": "Singh",
        "given": "Sameer"
      },
      {
        "family": "Guestrin",
        "given": "Carlos"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2016
        ]
      ]
    },
    "abstract": "Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.",
    "DOI": "10.48550/arxiv.1602.04938",
    "publisher": "arXiv",
    "title": "\"Why Should I Trust You?\": Explaining the Predictions of Any Classifier",
    "URL": "https://doi.org/hsmk",
    "version": "3",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.1602.04938"
  },
  {
    "type": "article-journal",
    "id": "pTzEjoO6",
    "categories": [
      "Machine Learning (cs.LG)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Chen",
        "given": "Tianqi"
      },
      {
        "family": "Guestrin",
        "given": "Carlos"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2016
        ]
      ]
    },
    "abstract": "Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.",
    "container-title": "arXiv",
    "DOI": "10.48550/arxiv.1603.02754",
    "publisher": "arXiv",
    "title": "XGBoost: A Scalable Tree Boosting System",
    "URL": "https://doi.org/g958zc",
    "version": "3",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.1603.02754"
  },
  {
    "type": "article",
    "id": "HZddfIob",
    "categories": [
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Machine Learning (stat.ML)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Che",
        "given": "Zhengping"
      },
      {
        "family": "Purushotham",
        "given": "Sanjay"
      },
      {
        "family": "Cho",
        "given": "Kyunghyun"
      },
      {
        "family": "Sontag",
        "given": "David"
      },
      {
        "family": "Liu",
        "given": "Yan"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2016
        ]
      ]
    },
    "abstract": "Multivariate time series data in practical applications, such as health care, geoscience, and biology, are characterized by a variety of missing values. In time series prediction and other related tasks, it has been noted that missing values and their missing patterns are often correlated with the target labels, a.k.a., informative missingness. There is very limited work on exploiting the missing patterns for effective imputation and improving prediction performance. In this paper, we develop novel deep learning models, namely GRU-D, as one of the early attempts. GRU-D is based on Gated Recurrent Unit (GRU), a state-of-the-art recurrent neural network. It takes two representations of missing patterns, i.e., masking and time interval, and effectively incorporates them into a deep model architecture so that it not only captures the long-term temporal dependencies in time series, but also utilizes the missing patterns to achieve better prediction results. Experiments of time series classification tasks on real-world clinical datasets (MIMIC-III, PhysioNet) and synthetic datasets demonstrate that our models achieve state-of-the-art performance and provides useful insights for better understanding and utilization of missing values in time series analysis.",
    "DOI": "10.48550/arxiv.1606.01865",
    "publisher": "arXiv",
    "title": "Recurrent Neural Networks for Multivariate Time Series with Missing Values",
    "URL": "https://doi.org/g958zd",
    "version": "2",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.1606.01865"
  },
  {
    "type": "article",
    "id": "3wZYBtMr",
    "categories": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Vinyals",
        "given": "Oriol"
      },
      {
        "family": "Blundell",
        "given": "Charles"
      },
      {
        "family": "Lillicrap",
        "given": "Timothy"
      },
      {
        "family": "Kavukcuoglu",
        "given": "Koray"
      },
      {
        "family": "Wierstra",
        "given": "Daan"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2016
        ]
      ]
    },
    "abstract": "Learning from a few examples remains a key challenge in machine learning. Despite recent advances in important domains such as vision and language, the standard supervised deep learning paradigm does not offer a satisfactory solution for learning new concepts rapidly from little data. In this work, we employ ideas from metric learning based on deep neural features and from recent advances that augment neural networks with external memories. Our framework learns a network that maps a small labelled support set and an unlabelled example to its label, obviating the need for fine-tuning to adapt to new class types. We then define one-shot learning problems on vision (using Omniglot, ImageNet) and language tasks. Our algorithm improves one-shot accuracy on ImageNet from 87.6% to 93.2% and from 88.0% to 93.8% on Omniglot compared to competing approaches. We also demonstrate the usefulness of the same model on language modeling by introducing a one-shot task on the Penn Treebank.",
    "DOI": "10.48550/arxiv.1606.04080",
    "publisher": "arXiv",
    "title": "Matching Networks for One Shot Learning",
    "URL": "https://doi.org/g96wsd",
    "version": "2",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.1606.04080"
  },
  {
    "type": "article",
    "id": "45Kr1uvy",
    "categories": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences",
      "FOS: Mathematics",
      "FOS: Mathematics"
    ],
    "author": [
      {
        "family": "Bottou",
        "given": "Léon"
      },
      {
        "family": "Curtis",
        "given": "Frank E."
      },
      {
        "family": "Nocedal",
        "given": "Jorge"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2016
        ]
      ]
    },
    "abstract": "This paper provides a review and commentary on the past, present, and future of numerical optimization algorithms in the context of machine learning applications. Through case studies on text classification and the training of deep neural networks, we discuss how optimization problems arise in machine learning and what makes them challenging. A major theme of our study is that large-scale machine learning represents a distinctive setting in which the stochastic gradient (SG) method has traditionally played a central role while conventional gradient-based nonlinear optimization techniques typically falter. Based on this viewpoint, we present a comprehensive theory of a straightforward, yet versatile SG algorithm, discuss its practical behavior, and highlight opportunities for designing algorithms with improved performance. This leads to a discussion about the next generation of optimization methods for large-scale machine learning, including an investigation of two main streams of research on techniques that diminish noise in the stochastic directions and methods that make use of second-order derivative approximations.",
    "DOI": "10.48550/arxiv.1606.04838",
    "publisher": "arXiv",
    "title": "Optimization Methods for Large-Scale Machine Learning",
    "URL": "https://doi.org/g958zf",
    "version": "3",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.1606.04838"
  },
  {
    "type": "article",
    "id": "y1dj3AHA",
    "categories": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Alain",
        "given": "Guillaume"
      },
      {
        "family": "Bengio",
        "given": "Yoshua"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2016
        ]
      ]
    },
    "abstract": "Neural network models have a reputation for being black boxes. We propose to monitor the features at every layer of a model and measure how suitable they are for classification. We use linear classifiers, which we refer to as \"probes\", trained entirely independently of the model itself. This helps us better understand the roles and dynamics of the intermediate layers. We demonstrate how this can be used to develop a better intuition about models and to diagnose potential problems. We apply this technique to the popular models Inception v3 and Resnet-50. Among other things, we observe experimentally that the linear separability of features increase monotonically along the depth of the model.",
    "DOI": "10.48550/arxiv.1610.01644",
    "publisher": "arXiv",
    "title": "Understanding intermediate layers using linear classifier probes",
    "URL": "https://doi.org/khmg",
    "version": "4",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.1610.01644"
  },
  {
    "type": "article-journal",
    "id": "14BVLTOJq",
    "categories": [
      "Machine Learning (cs.LG)",
      "Information Theory (cs.IT)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Alemi",
        "given": "Alexander A."
      },
      {
        "family": "Fischer",
        "given": "Ian"
      },
      {
        "family": "Dillon",
        "given": "Joshua V."
      },
      {
        "family": "Murphy",
        "given": "Kevin"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2016
        ]
      ]
    },
    "abstract": "We present a variational approximation to the information bottleneck of Tishby et al. (1999). This variational approach allows us to parameterize the information bottleneck model using a neural network and leverage the reparameterization trick for efficient training. We call this method \"Deep Variational Information Bottleneck\", or Deep VIB. We show that models trained with the VIB objective outperform those that are trained with other forms of regularization, in terms of generalization performance and robustness to adversarial attack.",
    "container-title": "arXiv",
    "DOI": "10.48550/arxiv.1612.00410",
    "publisher": "arXiv",
    "title": "Deep Variational Information Bottleneck",
    "URL": "https://doi.org/gq9mrm",
    "version": "7",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.1612.00410"
  },
  {
    "type": "article",
    "id": "mFiH1ERh",
    "categories": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Machine Learning (stat.ML)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Shazeer",
        "given": "Noam"
      },
      {
        "family": "Mirhoseini",
        "given": "Azalia"
      },
      {
        "family": "Maziarz",
        "given": "Krzysztof"
      },
      {
        "family": "Davis",
        "given": "Andy"
      },
      {
        "family": "Le",
        "given": "Quoc"
      },
      {
        "family": "Hinton",
        "given": "Geoffrey"
      },
      {
        "family": "Dean",
        "given": "Jeff"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2017
        ]
      ]
    },
    "abstract": "The capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation. In practice, however, there are significant algorithmic and performance challenges. In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters. We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example. We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora. We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers. On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.",
    "DOI": "10.48550/arxiv.1701.06538",
    "publisher": "arXiv",
    "title": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer",
    "URL": "https://doi.org/g95xv5",
    "version": "1",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.1701.06538"
  },
  {
    "type": "article",
    "id": "Cq4JGOGX",
    "categories": [
      "Machine Learning (stat.ML)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Doshi-Velez",
        "given": "Finale"
      },
      {
        "family": "Kim",
        "given": "Been"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2017
        ]
      ]
    },
    "abstract": "As machine learning systems become ubiquitous, there has been a surge of interest in interpretable machine learning: systems that provide explanation for their outputs. These explanations are often used to qualitatively assess other criteria such as safety or non-discrimination. However, despite the interest in interpretability, there is very little consensus on what interpretable machine learning is and how it should be measured. In this position paper, we first define interpretability and describe when interpretability is needed (and when it is not). Next, we suggest a taxonomy for rigorous evaluation and expose open questions towards a more rigorous science of interpretable machine learning.",
    "DOI": "10.48550/arxiv.1702.08608",
    "publisher": "arXiv",
    "title": "Towards A Rigorous Science of Interpretable Machine Learning",
    "URL": "https://doi.org/h3cz",
    "version": "2",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.1702.08608"
  },
  {
    "type": "article",
    "id": "11Dolfu34",
    "categories": [
      "Machine Learning (cs.LG)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Sundararajan",
        "given": "Mukund"
      },
      {
        "family": "Taly",
        "given": "Ankur"
      },
      {
        "family": "Yan",
        "given": "Qiqi"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2017
        ]
      ]
    },
    "abstract": "We study the problem of attributing the prediction of a deep network to its input features, a problem previously studied by several other works. We identify two fundamental axioms---Sensitivity and Implementation Invariance that attribution methods ought to satisfy. We show that they are not satisfied by most known attribution methods, which we consider to be a fundamental weakness of those methods. We use the axioms to guide the design of a new attribution method called Integrated Gradients. Our method requires no modification to the original network and is extremely simple to implement; it just needs a few calls to the standard gradient operator. We apply this method to a couple of image models, a couple of text models and a chemistry model, demonstrating its ability to debug networks, to extract rules from a network, and to enable users to engage with models better.",
    "DOI": "10.48550/arxiv.1703.01365",
    "publisher": "arXiv",
    "title": "Axiomatic Attribution for Deep Networks",
    "URL": "https://doi.org/grx4kq",
    "version": "2",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.1703.01365"
  },
  {
    "type": "article",
    "id": "JE5FRU4v",
    "categories": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Neural and Evolutionary Computing (cs.NE)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Finn",
        "given": "Chelsea"
      },
      {
        "family": "Abbeel",
        "given": "Pieter"
      },
      {
        "family": "Levine",
        "given": "Sergey"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2017
        ]
      ]
    },
    "abstract": "We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.",
    "DOI": "10.48550/arxiv.1703.03400",
    "publisher": "arXiv",
    "title": "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks",
    "URL": "https://doi.org/g5v2js",
    "version": "3",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.1703.03400"
  },
  {
    "type": "article",
    "id": "167ItAuU7",
    "categories": [
      "Machine Learning (stat.ML)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Koh",
        "given": "Pang Wei"
      },
      {
        "family": "Liang",
        "given": "Percy"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2017
        ]
      ]
    },
    "abstract": "How can we explain the predictions of a black-box model? In this paper, we use influence functions -- a classic technique from robust statistics -- to trace a model's prediction through the learning algorithm and back to its training data, thereby identifying training points most responsible for a given prediction. To scale up influence functions to modern machine learning settings, we develop a simple, efficient implementation that requires only oracle access to gradients and Hessian-vector products. We show that even on non-convex and non-differentiable models where the theory breaks down, approximations to influence functions can still provide valuable information. On linear models and convolutional neural networks, we demonstrate that influence functions are useful for multiple purposes: understanding model behavior, debugging models, detecting dataset errors, and even creating visually-indistinguishable training-set attacks.",
    "DOI": "10.48550/arxiv.1703.04730",
    "publisher": "arXiv",
    "title": "Understanding Black-box Predictions via Influence Functions",
    "URL": "https://doi.org/mcsx",
    "version": "3",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.1703.04730"
  },
  {
    "type": "article",
    "id": "1EGW21dqi",
    "categories": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Snell",
        "given": "Jake"
      },
      {
        "family": "Swersky",
        "given": "Kevin"
      },
      {
        "family": "Zemel",
        "given": "Richard S."
      }
    ],
    "issued": {
      "date-parts": [
        [
          2017
        ]
      ]
    },
    "abstract": "We propose prototypical networks for the problem of few-shot classification, where a classifier must generalize to new classes not seen in the training set, given only a small number of examples of each new class. Prototypical networks learn a metric space in which classification can be performed by computing distances to prototype representations of each class. Compared to recent approaches for few-shot learning, they reflect a simpler inductive bias that is beneficial in this limited-data regime, and achieve excellent results. We provide an analysis showing that some simple design decisions can yield substantial improvements over recent approaches involving complicated architectural choices and meta-learning. We further extend prototypical networks to zero-shot learning and achieve state-of-the-art results on the CU-Birds dataset.",
    "DOI": "10.48550/arxiv.1703.05175",
    "publisher": "arXiv",
    "title": "Prototypical Networks for Few-shot Learning",
    "URL": "https://doi.org/g96wsf",
    "version": "2",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.1703.05175"
  },
  {
    "type": "article-journal",
    "id": "BvgiYaxe",
    "categories": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Shrikumar",
        "given": "Avanti"
      },
      {
        "family": "Greenside",
        "given": "Peyton"
      },
      {
        "family": "Kundaje",
        "given": "Anshul"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2017
        ]
      ]
    },
    "abstract": "The purported \"black box\" nature of neural networks is a barrier to adoption in applications where interpretability is essential. Here we present DeepLIFT (Deep Learning Important FeaTures), a method for decomposing the output prediction of a neural network on a specific input by backpropagating the contributions of all neurons in the network to every feature of the input. DeepLIFT compares the activation of each neuron to its 'reference activation' and assigns contribution scores according to the difference. By optionally giving separate consideration to positive and negative contributions, DeepLIFT can also reveal dependencies which are missed by other approaches. Scores can be computed efficiently in a single backward pass. We apply DeepLIFT to models trained on MNIST and simulated genomic data, and show significant advantages over gradient-based methods. Video tutorial: http://goo.gl/qKb7pL, ICML slides: bit.ly/deeplifticmlslides, ICML talk: https://vimeo.com/238275076, code: http://goo.gl/RM8jvH.",
    "container-title": "arXiv",
    "DOI": "10.48550/arxiv.1704.02685",
    "publisher": "arXiv",
    "title": "Learning Important Features Through Propagating Activation Differences",
    "URL": "https://doi.org/g958zg",
    "version": "2",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.1704.02685"
  },
  {
    "type": "article",
    "id": "rkxTwVjs",
    "categories": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Lundberg",
        "given": "Scott"
      },
      {
        "family": "Lee",
        "given": "Su-In"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2017
        ]
      ]
    },
    "abstract": "Understanding why a model makes a certain prediction can be as crucial as the prediction's accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches.",
    "DOI": "10.48550/arxiv.1705.07874",
    "publisher": "arXiv",
    "title": "A Unified Approach to Interpreting Model Predictions",
    "URL": "https://doi.org/gp6hf4",
    "version": "2",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.1705.07874"
  },
  {
    "type": "article",
    "id": "SfCo6pSp",
    "categories": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Al-Shedivat",
        "given": "Maruan"
      },
      {
        "family": "Dubey",
        "given": "Avinava"
      },
      {
        "family": "Xing",
        "given": "Eric P."
      }
    ],
    "issued": {
      "date-parts": [
        [
          2017
        ]
      ]
    },
    "abstract": "Modern learning algorithms excel at producing accurate but complex models of the data. However, deploying such models in the real-world requires extra care: we must ensure their reliability, robustness, and absence of undesired biases. This motivates the development of models that are equally accurate but can be also easily inspected and assessed beyond their predictive performance. To this end, we introduce contextual explanation networks (CEN)---a class of architectures that learn to predict by generating and utilizing intermediate, simplified probabilistic models. Specifically, CENs generate parameters for intermediate graphical models which are further used for prediction and play the role of explanations. Contrary to the existing post-hoc model-explanation tools, CENs learn to predict and to explain simultaneously. Our approach offers two major advantages: (i) for each prediction valid, instance-specific explanation is generated with no computational overhead and (ii) prediction via explanation acts as a regularizer and boosts performance in data-scarce settings. We analyze the proposed framework theoretically and experimentally. Our results on image and text classification and survival analysis tasks demonstrate that CENs are not only competitive with the state-of-the-art methods but also offer additional insights behind each prediction, that can be valuable for decision support. We also show that while post-hoc methods may produce misleading explanations in certain cases, CENs are consistent and allow to detect such cases systematically.",
    "DOI": "10.48550/arxiv.1705.10301",
    "publisher": "arXiv",
    "title": "Contextual Explanation Networks",
    "URL": "https://doi.org/gt68h9",
    "version": "4",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.1705.10301"
  },
  {
    "type": "article",
    "id": "rh7nCPVE",
    "categories": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Vaswani",
        "given": "Ashish"
      },
      {
        "family": "Shazeer",
        "given": "Noam"
      },
      {
        "family": "Parmar",
        "given": "Niki"
      },
      {
        "family": "Uszkoreit",
        "given": "Jakob"
      },
      {
        "family": "Jones",
        "given": "Llion"
      },
      {
        "family": "Gomez",
        "given": "Aidan N."
      },
      {
        "family": "Kaiser",
        "given": "Lukasz"
      },
      {
        "family": "Polosukhin",
        "given": "Illia"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2017
        ]
      ]
    },
    "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
    "DOI": "10.48550/arxiv.1706.03762",
    "publisher": "arXiv",
    "title": "Attention Is All You Need",
    "URL": "https://doi.org/gpnmtv",
    "version": "7",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.1706.03762"
  },
  {
    "type": "article",
    "id": "zPozLA0K",
    "categories": [
      "Machine Learning (cs.LG)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Guo",
        "given": "Chuan"
      },
      {
        "family": "Pleiss",
        "given": "Geoff"
      },
      {
        "family": "Sun",
        "given": "Yu"
      },
      {
        "family": "Weinberger",
        "given": "Kilian Q."
      }
    ],
    "issued": {
      "date-parts": [
        [
          2017
        ]
      ]
    },
    "abstract": "Confidence calibration -- the problem of predicting probability estimates representative of the true correctness likelihood -- is important for classification models in many applications. We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated. Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors influencing calibration. We evaluate the performance of various post-processing calibration methods on state-of-the-art architectures with image and document classification datasets. Our analysis and experiments not only offer insights into neural network learning, but also provide a simple and straightforward recipe for practical settings: on most datasets, temperature scaling -- a single-parameter variant of Platt Scaling -- is surprisingly effective at calibrating predictions.",
    "DOI": "10.48550/arxiv.1706.04599",
    "publisher": "arXiv",
    "title": "On Calibration of Modern Neural Networks",
    "URL": "https://doi.org/g95xv6",
    "version": "2",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.1706.04599"
  },
  {
    "type": "article",
    "id": "qfuiP7Hi",
    "categories": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Ruder",
        "given": "Sebastian"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2017
        ]
      ]
    },
    "abstract": "Multi-task learning (MTL) has led to successes in many applications of machine learning, from natural language processing and speech recognition to computer vision and drug discovery. This article aims to give a general overview of MTL, particularly in deep neural networks. It introduces the two most common methods for MTL in Deep Learning, gives an overview of the literature, and discusses recent advances. In particular, it seeks to help ML practitioners apply MTL by shedding light on how MTL works and providing guidelines for choosing appropriate auxiliary tasks.",
    "DOI": "10.48550/arxiv.1706.05098",
    "publisher": "arXiv",
    "title": "An Overview of Multi-Task Learning in Deep Neural Networks",
    "URL": "https://doi.org/g958zh",
    "version": "1",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.1706.05098"
  },
  {
    "type": "article-journal",
    "id": "1Cd8cgDM6",
    "categories": [
      "Machine Learning (stat.ML)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Kim",
        "given": "Been"
      },
      {
        "family": "Wattenberg",
        "given": "Martin"
      },
      {
        "family": "Gilmer",
        "given": "Justin"
      },
      {
        "family": "Cai",
        "given": "Carrie"
      },
      {
        "family": "Wexler",
        "given": "James"
      },
      {
        "family": "Viegas",
        "given": "Fernanda"
      },
      {
        "family": "Sayres",
        "given": "Rory"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2017
        ]
      ]
    },
    "abstract": "The interpretation of deep learning models is a challenge due to their size, complexity, and often opaque internal state. In addition, many systems, such as image classifiers, operate on low-level features rather than high-level concepts. To address these challenges, we introduce Concept Activation Vectors (CAVs), which provide an interpretation of a neural net's internal state in terms of human-friendly concepts. The key idea is to view the high-dimensional internal state of a neural net as an aid, not an obstacle. We show how to use CAVs as part of a technique, Testing with CAVs (TCAV), that uses directional derivatives to quantify the degree to which a user-defined concept is important to a classification result--for example, how sensitive a prediction of \"zebra\" is to the presence of stripes. Using the domain of image classification as a testing ground, we describe how CAVs may be used to explore hypotheses and generate insights for a standard image classification network as well as a medical application.",
    "container-title": "arXiv",
    "DOI": "10.48550/arxiv.1711.11279",
    "publisher": "arXiv",
    "title": "Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV)",
    "URL": "https://doi.org/khk9",
    "version": "5",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.1711.11279"
  },
  {
    "type": "article",
    "id": "NGhi4Vf3",
    "categories": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Cremer",
        "given": "Chris"
      },
      {
        "family": "Li",
        "given": "Xuechen"
      },
      {
        "family": "Duvenaud",
        "given": "David"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2018
        ]
      ]
    },
    "abstract": "Amortized inference allows latent-variable models trained via variational learning to scale to large datasets. The quality of approximate inference is determined by two factors: a) the capacity of the variational distribution to match the true posterior and b) the ability of the recognition network to produce good variational parameters for each datapoint. We examine approximate inference in variational autoencoders in terms of these factors. We find that divergence from the true posterior is often due to imperfect recognition networks, rather than the limited complexity of the approximating distribution. We show that this is due partly to the generator learning to accommodate the choice of approximation. Furthermore, we show that the parameters used to increase the expressiveness of the approximation play a role in generalizing inference rather than simply improving the complexity of the approximation.",
    "DOI": "10.48550/arxiv.1801.03558",
    "publisher": "arXiv",
    "title": "Inference Suboptimality in Variational Autoencoders",
    "URL": "https://doi.org/g958zj",
    "version": "3",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.1801.03558"
  },
  {
    "type": "article",
    "id": "10ULlHdY2",
    "categories": [
      "Machine Learning (cs.LG)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Hoi",
        "given": "Steven C. H."
      },
      {
        "family": "Sahoo",
        "given": "Doyen"
      },
      {
        "family": "Lu",
        "given": "Jing"
      },
      {
        "family": "Zhao",
        "given": "Peilin"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2018
        ]
      ]
    },
    "abstract": "Online learning represents an important family of machine learning algorithms, in which a learner attempts to resolve an online prediction (or any type of decision-making) task by learning a model/hypothesis from a sequence of data instances one at a time. The goal of online learning is to ensure that the online learner would make a sequence of accurate predictions (or correct decisions) given the knowledge of correct answers to previous prediction or learning tasks and possibly additional information. This is in contrast to many traditional batch learning or offline machine learning algorithms that are often designed to train a model in batch from a given collection of training data instances. This survey aims to provide a comprehensive survey of the online machine learning literatures through a systematic review of basic ideas and key principles and a proper categorization of different algorithms and techniques. Generally speaking, according to the learning type and the forms of feedback information, the existing online learning works can be classified into three major categories: (i) supervised online learning where full feedback information is always available, (ii) online learning with limited feedback, and (iii) unsupervised online learning where there is no feedback available. Due to space limitation, the survey will be mainly focused on the first category, but also briefly cover some basics of the other two categories. Finally, we also discuss some open issues and attempt to shed light on potential future research directions in this field.",
    "DOI": "10.48550/arxiv.1802.02871",
    "publisher": "arXiv",
    "title": "Online Learning: A Comprehensive Survey",
    "URL": "https://doi.org/g9643t",
    "version": "2",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.1802.02871"
  },
  {
    "type": "article",
    "id": "1ARNqdn4y",
    "categories": [
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences",
      "I.2"
    ],
    "author": [
      {
        "family": "Poursabzi-Sangdeh",
        "given": "Forough"
      },
      {
        "family": "Goldstein",
        "given": "Daniel G."
      },
      {
        "family": "Hofman",
        "given": "Jake M."
      },
      {
        "family": "Vaughan",
        "given": "Jennifer Wortman"
      },
      {
        "family": "Wallach",
        "given": "Hanna"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2018
        ]
      ]
    },
    "abstract": "With machine learning models being increasingly used to aid decision making even in high-stakes domains, there has been a growing interest in developing interpretable models. Although many supposedly interpretable models have been proposed, there have been relatively few experimental studies investigating whether these models achieve their intended effects, such as making people more closely follow a model's predictions when it is beneficial for them to do so or enabling them to detect when a model has made a mistake. We present a sequence of pre-registered experiments (N=3,800) in which we showed participants functionally identical models that varied only in two factors commonly thought to make machine learning models more or less interpretable: the number of features and the transparency of the model (i.e., whether the model internals are clear or black box). Predictably, participants who saw a clear model with few features could better simulate the model's predictions. However, we did not find that participants more closely followed its predictions. Furthermore, showing participants a clear model meant that they were less able to detect and correct for the model's sizable mistakes, seemingly due to information overload. These counterintuitive findings emphasize the importance of testing over intuition when developing interpretable models.",
    "DOI": "10.48550/arxiv.1802.07810",
    "publisher": "arXiv",
    "title": "Manipulating and Measuring Model Interpretability",
    "URL": "https://doi.org/g958zk",
    "version": "5",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.1802.07810"
  },
  {
    "type": "article",
    "id": "KvoFCtdS",
    "categories": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Papernot",
        "given": "Nicolas"
      },
      {
        "family": "McDaniel",
        "given": "Patrick"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2018
        ]
      ]
    },
    "abstract": "Deep neural networks (DNNs) enable innovative applications of machine learning like image recognition, machine translation, or malware detection. However, deep learning is often criticized for its lack of robustness in adversarial settings (e.g., vulnerability to adversarial inputs) and general inability to rationalize its predictions. In this work, we exploit the structure of deep learning to enable new learning-based inference and decision strategies that achieve desirable properties such as robustness and interpretability. We take a first step in this direction and introduce the Deep k-Nearest Neighbors (DkNN). This hybrid classifier combines the k-nearest neighbors algorithm with representations of the data learned by each layer of the DNN: a test input is compared to its neighboring training points according to the distance that separates them in the representations. We show the labels of these neighboring points afford confidence estimates for inputs outside the model's training manifold, including on malicious inputs like adversarial examples--and therein provides protections against inputs that are outside the models understanding. This is because the nearest neighbors can be used to estimate the nonconformity of, i.e., the lack of support for, a prediction in the training data. The neighbors also constitute human-interpretable explanations of predictions. We evaluate the DkNN algorithm on several datasets, and show the confidence estimates accurately identify inputs outside the model, and that the explanations provided by nearest neighbors are intuitive and useful in understanding model failures.",
    "DOI": "10.48550/arxiv.1803.04765",
    "publisher": "arXiv",
    "title": "Deep k-Nearest Neighbors: Towards Confident, Interpretable and Robust Deep Learning",
    "URL": "https://doi.org/g958zm",
    "version": "1",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.1803.04765"
  },
  {
    "type": "article",
    "id": "jPxFquZY",
    "categories": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Cao",
        "given": "Wei"
      },
      {
        "family": "Wang",
        "given": "Dong"
      },
      {
        "family": "Li",
        "given": "Jian"
      },
      {
        "family": "Zhou",
        "given": "Hao"
      },
      {
        "family": "Li",
        "given": "Lei"
      },
      {
        "family": "Li",
        "given": "Yitan"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2018
        ]
      ]
    },
    "abstract": "Time series are widely used as signals in many classification/regression tasks. It is ubiquitous that time series contains many missing values. Given multiple correlated time series data, how to fill in missing values and to predict their class labels? Existing imputation methods often impose strong assumptions of the underlying data generating process, such as linear dynamics in the state space. In this paper, we propose BRITS, a novel method based on recurrent neural networks for missing value imputation in time series data. Our proposed method directly learns the missing values in a bidirectional recurrent dynamical system, without any specific assumption. The imputed values are treated as variables of RNN graph and can be effectively updated during the backpropagation.BRITS has three advantages: (a) it can handle multiple correlated missing values in time series; (b) it generalizes to time series with nonlinear dynamics underlying; (c) it provides a data-driven imputation procedure and applies to general settings with missing data.We evaluate our model on three real-world datasets, including an air quality dataset, a health-care data, and a localization data for human activity. Experiments show that our model outperforms the state-of-the-art methods in both imputation and classification/regression accuracies.",
    "DOI": "10.48550/arxiv.1805.10572",
    "publisher": "arXiv",
    "title": "BRITS: Bidirectional Recurrent Imputation for Time Series",
    "URL": "https://doi.org/g958zn",
    "version": "1",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.1805.10572"
  },
  {
    "type": "article",
    "id": "p6OGob17",
    "categories": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Ivanov",
        "given": "Oleg"
      },
      {
        "family": "Figurnov",
        "given": "Michael"
      },
      {
        "family": "Vetrov",
        "given": "Dmitry"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2018
        ]
      ]
    },
    "abstract": "We propose a single neural probabilistic model based on variational autoencoder that can be conditioned on an arbitrary subset of observed features and then sample the remaining features in \"one shot\". The features may be both real-valued and categorical. Training of the model is performed by stochastic variational Bayes. The experimental evaluation on synthetic data, as well as feature imputation and image inpainting problems, shows the effectiveness of the proposed approach and diversity of the generated samples.",
    "DOI": "10.48550/arxiv.1806.02382",
    "publisher": "arXiv",
    "title": "Variational Autoencoder with Arbitrary Conditioning",
    "URL": "https://doi.org/g958zp",
    "version": "3",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.1806.02382"
  },
  {
    "type": "article",
    "id": "twzF9qcj",
    "categories": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Yoon",
        "given": "Jinsung"
      },
      {
        "family": "Jordon",
        "given": "James"
      },
      {
        "family": "van der Schaar",
        "given": "Mihaela"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2018
        ]
      ]
    },
    "abstract": "We propose a novel method for imputing missing data by adapting the well-known Generative Adversarial Nets (GAN) framework. Accordingly, we call our method Generative Adversarial Imputation Nets (GAIN). The generator (G) observes some components of a real data vector, imputes the missing components conditioned on what is actually observed, and outputs a completed vector. The discriminator (D) then takes a completed vector and attempts to determine which components were actually observed and which were imputed. To ensure that D forces G to learn the desired distribution, we provide D with some additional information in the form of a hint vector. The hint reveals to D partial information about the missingness of the original sample, which is used by D to focus its attention on the imputation quality of particular components. This hint ensures that G does in fact learn to generate according to the true data distribution. We tested our method on various datasets and found that GAIN significantly outperforms state-of-the-art imputation methods.",
    "DOI": "10.48550/arxiv.1806.02920",
    "publisher": "arXiv",
    "title": "GAIN: Missing Data Imputation using Generative Adversarial Nets",
    "URL": "https://doi.org/g958zq",
    "version": "1",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.1806.02920"
  },
  {
    "type": "article-journal",
    "id": "jO5B8YvG",
    "categories": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (stat.ML)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Chen",
        "given": "Chaofan"
      },
      {
        "family": "Li",
        "given": "Oscar"
      },
      {
        "family": "Tao",
        "given": "Chaofan"
      },
      {
        "family": "Barnett",
        "given": "Alina Jade"
      },
      {
        "family": "Su",
        "given": "Jonathan"
      },
      {
        "family": "Rudin",
        "given": "Cynthia"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2018
        ]
      ]
    },
    "abstract": "When we are faced with challenging image classification tasks, we often explain our reasoning by dissecting the image, and pointing out prototypical aspects of one class or another. The mounting evidence for each of the classes helps us make our final decision. In this work, we introduce a deep network architecture -- prototypical part network (ProtoPNet), that reasons in a similar way: the network dissects the image by finding prototypical parts, and combines evidence from the prototypes to make a final classification. The model thus reasons in a way that is qualitatively similar to the way ornithologists, physicians, and others would explain to people on how to solve challenging image classification tasks. The network uses only image-level labels for training without any annotations for parts of images. We demonstrate our method on the CUB-200-2011 dataset and the Stanford Cars dataset. Our experiments show that ProtoPNet can achieve comparable accuracy with its analogous non-interpretable counterpart, and when several ProtoPNets are combined into a larger network, it can achieve an accuracy that is on par with some of the best-performing deep models. Moreover, ProtoPNet provides a level of interpretability that is absent in other interpretable deep models.",
    "container-title": "arXiv",
    "DOI": "10.48550/arxiv.1806.10574",
    "publisher": "arXiv",
    "title": "This Looks Like That: Deep Learning for Interpretable Image Recognition",
    "URL": "https://doi.org/kg6p",
    "version": "5",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.1806.10574"
  },
  {
    "type": "article",
    "id": "J9eXUymQ",
    "categories": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Ahn",
        "given": "Ho Seok"
      },
      {
        "family": "Dayoub",
        "given": "Feras"
      },
      {
        "family": "Popovic",
        "given": "Marija"
      },
      {
        "family": "MacDonald",
        "given": "Bruce"
      },
      {
        "family": "Siegwart",
        "given": "Roland"
      },
      {
        "family": "Sa",
        "given": "Inkyu"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2018
        ]
      ]
    },
    "abstract": "Horticultural enterprises are becoming more sophisticated as the range of the crops they target expands. Requirements for enhanced efficiency and productivity have driven the demand for automating on-field operations. However, various problems remain yet to be solved for their reliable, safe deployment in real-world scenarios. This paper examines major research trends and current challenges in horticultural robotics. Specifically, our work focuses on sensing and perception in the three main horticultural procedures: pollination, yield estimation, and harvesting. For each task, we expose major issues arising from the unstructured, cluttered, and rugged nature of field environments, including variable lighting conditions and difficulties in fruit-specific detection, and highlight promising contemporary studies.",
    "DOI": "10.48550/arxiv.1807.03124",
    "publisher": "arXiv",
    "title": "An Overview of Perception Methods for Horticultural Robots: From Pollination to Harvest",
    "URL": "https://doi.org/g93rnk",
    "version": "1",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.1807.03124"
  },
  {
    "type": "article",
    "id": "18KQ6Vlb2",
    "categories": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Balseiro",
        "given": "Santiago"
      },
      {
        "family": "Golrezaei",
        "given": "Negin"
      },
      {
        "family": "Mahdian",
        "given": "Mohammad"
      },
      {
        "family": "Mirrokni",
        "given": "Vahab"
      },
      {
        "family": "Schneider",
        "given": "Jon"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2018
        ]
      ]
    },
    "abstract": "In the classical contextual bandits problem, in each round $t$, a learner observes some context $c$, chooses some action $i$ to perform, and receives some reward $r_{i,t}(c)$. We consider the variant of this problem where in addition to receiving the reward $r_{i,t}(c)$, the learner also learns the values of $r_{i,t}(c')$ for some other contexts $c'$ in set $\\mathcal{O}_i(c)$; i.e., the rewards that would have been achieved by performing that action under different contexts $c'\\in \\mathcal{O}_i(c)$. This variant arises in several strategic settings, such as learning how to bid in non-truthful repeated auctions, which has gained a lot of attention lately as many platforms have switched to running first-price auctions. We call this problem the contextual bandits problem with cross-learning. The best algorithms for the classical contextual bandits problem achieve $\\tilde{O}(\\sqrt{CKT})$ regret against all stationary policies, where $C$ is the number of contexts, $K$ the number of actions, and $T$ the number of rounds. We design and analyze new algorithms for the contextual bandits problem with cross-learning and show that their regret has better dependence on the number of contexts. Under complete cross-learning where the rewards for all contexts are learned when choosing an action, i.e., set $\\mathcal{O}_i(c)$ contains all contexts, we show that our algorithms achieve regret $\\tilde{O}(\\sqrt{KT})$, removing the dependence on $C$. For any other cases, i.e., under partial cross-learning where $|\\mathcal{O}_i(c)|&lt; C$ for some context-action pair of $(i,c)$, the regret bounds depend on how the sets $\\mathcal O_i(c)$ impact the degree to which cross-learning between contexts is possible. We simulate our algorithms on real auction data from an ad exchange running first-price auctions and show that they outperform traditional contextual bandit algorithms.",
    "DOI": "10.48550/arxiv.1809.09582",
    "publisher": "arXiv",
    "title": "Contextual Bandits with Cross-learning",
    "URL": "https://doi.org/g958zr",
    "version": "3",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.1809.09582"
  },
  {
    "type": "article",
    "id": "TQNdTc18",
    "categories": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (stat.ML)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Hsu",
        "given": "Kyle"
      },
      {
        "family": "Levine",
        "given": "Sergey"
      },
      {
        "family": "Finn",
        "given": "Chelsea"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2018
        ]
      ]
    },
    "abstract": "A central goal of unsupervised learning is to acquire representations from unlabeled data or experience that can be used for more effective learning of downstream tasks from modest amounts of labeled data. Many prior unsupervised learning works aim to do so by developing proxy objectives based on reconstruction, disentanglement, prediction, and other metrics. Instead, we develop an unsupervised meta-learning method that explicitly optimizes for the ability to learn a variety of tasks from small amounts of data. To do so, we construct tasks from unlabeled data in an automatic way and run meta-learning over the constructed tasks. Surprisingly, we find that, when integrated with meta-learning, relatively simple task construction mechanisms, such as clustering embeddings, lead to good performance on a variety of downstream, human-specified tasks. Our experiments across four image datasets indicate that our unsupervised meta-learning approach acquires a learning algorithm without any labeled data that is applicable to a wide range of downstream classification tasks, improving upon the embedding learned by four prior unsupervised learning methods.",
    "DOI": "10.48550/arxiv.1810.02334",
    "publisher": "arXiv",
    "title": "Unsupervised Learning via Meta-Learning",
    "URL": "https://doi.org/g958zs",
    "version": "6",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.1810.02334"
  },
  {
    "type": "article",
    "id": "urJgpE6q",
    "categories": [
      "Computation and Language (cs.CL)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Devlin",
        "given": "Jacob"
      },
      {
        "family": "Chang",
        "given": "Ming-Wei"
      },
      {
        "family": "Lee",
        "given": "Kenton"
      },
      {
        "family": "Toutanova",
        "given": "Kristina"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2018
        ]
      ]
    },
    "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
    "DOI": "10.48550/arxiv.1810.04805",
    "publisher": "arXiv",
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "URL": "https://doi.org/hm65",
    "version": "2",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.1810.04805"
  },
  {
    "type": "article-journal",
    "id": "URCTSFCA",
    "categories": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Locatello",
        "given": "Francesco"
      },
      {
        "family": "Bauer",
        "given": "Stefan"
      },
      {
        "family": "Lucic",
        "given": "Mario"
      },
      {
        "family": "Rätsch",
        "given": "Gunnar"
      },
      {
        "family": "Gelly",
        "given": "Sylvain"
      },
      {
        "family": "Schölkopf",
        "given": "Bernhard"
      },
      {
        "family": "Bachem",
        "given": "Olivier"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2018
        ]
      ]
    },
    "abstract": "The key idea behind the unsupervised learning of disentangled representations is that real-world data is generated by a few explanatory factors of variation which can be recovered by unsupervised learning algorithms. In this paper, we provide a sober look at recent progress in the field and challenge some common assumptions. We first theoretically show that the unsupervised learning of disentangled representations is fundamentally impossible without inductive biases on both the models and the data. Then, we train more than 12000 models covering most prominent methods and evaluation metrics in a reproducible large-scale experimental study on seven different data sets. We observe that while the different methods successfully enforce properties ``encouraged'' by the corresponding losses, well-disentangled models seemingly cannot be identified without supervision. Furthermore, increased disentanglement does not seem to lead to a decreased sample complexity of learning for downstream tasks. Our results suggest that future work on disentanglement learning should be explicit about the role of inductive biases and (implicit) supervision, investigate concrete benefits of enforcing disentanglement of the learned representations, and consider a reproducible experimental setup covering several data sets.",
    "container-title": "arXiv",
    "DOI": "10.48550/arxiv.1811.12359",
    "publisher": "arXiv",
    "title": "Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations",
    "URL": "https://doi.org/grx79c",
    "version": "4",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.1811.12359"
  },
  {
    "type": "article",
    "id": "K4VkXXUf",
    "categories": [
      "Machine Learning (stat.ML)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Ghorbani",
        "given": "Amirata"
      },
      {
        "family": "Wexler",
        "given": "James"
      },
      {
        "family": "Zou",
        "given": "James"
      },
      {
        "family": "Kim",
        "given": "Been"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2019
        ]
      ]
    },
    "abstract": "Interpretability has become an important topic of research as more machine learning (ML) models are deployed and widely used to make important decisions. Most of the current explanation methods provide explanations through feature importance scores, which identify features that are important for each individual input. However, how to systematically summarize and interpret such per sample feature importance scores itself is challenging. In this work, we propose principles and desiderata for \\emph{concept} based explanation, which goes beyond per-sample features to identify higher-level human-understandable concepts that apply across the entire dataset. We develop a new algorithm, ACE, to automatically extract visual concepts. Our systematic experiments demonstrate that \\alg discovers concepts that are human-meaningful, coherent and important for the neural network's predictions.",
    "DOI": "10.48550/arxiv.1902.03129",
    "publisher": "arXiv",
    "title": "Towards Automatic Concept-based Explanations",
    "URL": "https://doi.org/kf7w",
    "version": "3",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.1902.03129"
  },
  {
    "type": "article",
    "id": "LJ5esEGZ",
    "categories": [
      "Methodology (stat.ME)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Zhou",
        "given": "Yichen"
      },
      {
        "family": "Hooker",
        "given": "Giles"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2019
        ]
      ]
    },
    "abstract": "This paper investigates the integration of gradient boosted decision trees and varying coefficient models. We introduce the tree boosted varying coefficient framework which justifies the implementation of decision tree boosting as the nonparametric effect modifiers in varying coefficient models. This framework requires no structural assumptions in the space containing the varying coefficient covariates, is easy to implement, and keeps a balance between model complexity and interpretability. To provide statistical guarantees, we prove the asymptotic consistency of the proposed method under the regression settings with $L^2$ loss. We further conduct a thorough empirical study to show that the proposed method is capable of providing accurate predictions as well as intelligible visual explanations.",
    "DOI": "10.48550/arxiv.1904.01058",
    "publisher": "arXiv",
    "title": "Tree Boosted Varying Coefficient Models",
    "URL": "https://doi.org/g958zt",
    "version": "1",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.1904.01058"
  },
  {
    "type": "article",
    "id": "11dRZlKVC",
    "categories": [
      "Computation and Language (cs.CL)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Tenney",
        "given": "Ian"
      },
      {
        "family": "Xia",
        "given": "Patrick"
      },
      {
        "family": "Chen",
        "given": "Berlin"
      },
      {
        "family": "Wang",
        "given": "Alex"
      },
      {
        "family": "Poliak",
        "given": "Adam"
      },
      {
        "family": "McCoy",
        "given": "R Thomas"
      },
      {
        "family": "Kim",
        "given": "Najoung"
      },
      {
        "family": "Van Durme",
        "given": "Benjamin"
      },
      {
        "family": "Bowman",
        "given": "Samuel R."
      },
      {
        "family": "Das",
        "given": "Dipanjan"
      },
      {
        "family": "Pavlick",
        "given": "Ellie"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2019
        ]
      ]
    },
    "abstract": "Contextualized representation models such as ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2018) have recently achieved state-of-the-art results on a diverse array of downstream NLP tasks. Building on recent token-level probing work, we introduce a novel edge probing task design and construct a broad suite of sub-sentence tasks derived from the traditional structured NLP pipeline. We probe word-level contextual representations from four recent models and investigate how they encode sentence structure across a range of syntactic, semantic, local, and long-range phenomena. We find that existing models trained on language modeling and translation produce strong representations for syntactic phenomena, but only offer comparably small improvements on semantic tasks over a non-contextual baseline.",
    "DOI": "10.48550/arxiv.1905.06316",
    "publisher": "arXiv",
    "title": "What do you learn from context? Probing for sentence structure in contextualized word representations",
    "URL": "https://doi.org/g958zv",
    "version": "1",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.1905.06316"
  },
  {
    "type": "article",
    "id": "1EfmL7jzn",
    "categories": [
      "Computation and Language (cs.CL)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Petroni",
        "given": "Fabio"
      },
      {
        "family": "Rocktäschel",
        "given": "Tim"
      },
      {
        "family": "Lewis",
        "given": "Patrick"
      },
      {
        "family": "Bakhtin",
        "given": "Anton"
      },
      {
        "family": "Wu",
        "given": "Yuxiang"
      },
      {
        "family": "Miller",
        "given": "Alexander H."
      },
      {
        "family": "Riedel",
        "given": "Sebastian"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2019
        ]
      ]
    },
    "abstract": "Recent progress in pretraining language models on large textual corpora led to a surge of improvements for downstream NLP tasks. Whilst learning linguistic knowledge, these models may also be storing relational knowledge present in the training data, and may be able to answer queries structured as \"fill-in-the-blank\" cloze statements. Language models have many advantages over structured knowledge bases: they require no schema engineering, allow practitioners to query about an open class of relations, are easy to extend to more data, and require no human supervision to train. We present an in-depth analysis of the relational knowledge already present (without fine-tuning) in a wide range of state-of-the-art pretrained language models. We find that (i) without fine-tuning, BERT contains relational knowledge competitive with traditional NLP methods that have some access to oracle knowledge, (ii) BERT also does remarkably well on open-domain question answering against a supervised baseline, and (iii) certain types of factual knowledge are learned much more readily than others by standard language model pretraining approaches. The surprisingly strong ability of these models to recall factual knowledge without any fine-tuning demonstrates their potential as unsupervised open-domain QA systems. The code to reproduce our analysis is available at https://github.com/facebookresearch/LAMA.",
    "DOI": "10.48550/arxiv.1909.01066",
    "publisher": "arXiv",
    "title": "Language Models as Knowledge Bases?",
    "URL": "https://doi.org/g958zw",
    "version": "2",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.1909.01066"
  },
  {
    "type": "article",
    "id": "WlwUpYp",
    "categories": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Lengerich",
        "given": "Benjamin"
      },
      {
        "family": "Aragam",
        "given": "Bryon"
      },
      {
        "family": "Xing",
        "given": "Eric P."
      }
    ],
    "issued": {
      "date-parts": [
        [
          2019
        ]
      ]
    },
    "abstract": "Modern applications of machine learning (ML) deal with increasingly heterogeneous datasets comprised of data collected from overlapping latent subpopulations. As a result, traditional models trained over large datasets may fail to recognize highly predictive localized effects in favour of weakly predictive global patterns. This is a problem because localized effects are critical to developing individualized policies and treatment plans in applications ranging from precision medicine to advertising. To address this challenge, we propose to estimate sample-specific models that tailor inference and prediction at the individual level. In contrast to classical ML models that estimate a single, complex model (or only a few complex models), our approach produces a model personalized to each sample. These sample-specific models can be studied to understand subgroup dynamics that go beyond coarse-grained class labels. Crucially, our approach does not assume that relationships between samples (e.g. a similarity network) are known a priori. Instead, we use unmodeled covariates to learn a latent distance metric over the samples. We apply this approach to financial, biomedical, and electoral data as well as simulated data and show that sample-specific models provide fine-grained interpretations of complicated phenomena without sacrificing predictive accuracy compared to state-of-the-art models such as deep neural networks.",
    "DOI": "10.48550/arxiv.1910.06939",
    "publisher": "arXiv",
    "title": "Learning Sample-Specific Models with Low-Rank Personalized Regression",
    "URL": "https://doi.org/gt68jb",
    "version": "1",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.1910.06939"
  },
  {
    "type": "article",
    "id": "11l8svMmM",
    "categories": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Sagawa",
        "given": "Shiori"
      },
      {
        "family": "Koh",
        "given": "Pang Wei"
      },
      {
        "family": "Hashimoto",
        "given": "Tatsunori B."
      },
      {
        "family": "Liang",
        "given": "Percy"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2019
        ]
      ]
    },
    "abstract": "Overparameterized neural networks can be highly accurate on average on an i.i.d. test set yet consistently fail on atypical groups of the data (e.g., by learning spurious correlations that hold on average but not in such groups). Distributionally robust optimization (DRO) allows us to learn models that instead minimize the worst-case training loss over a set of pre-defined groups. However, we find that naively applying group DRO to overparameterized neural networks fails: these models can perfectly fit the training data, and any model with vanishing average training loss also already has vanishing worst-case training loss. Instead, the poor worst-case performance arises from poor generalization on some groups. By coupling group DRO models with increased regularization---a stronger-than-typical L2 penalty or early stopping---we achieve substantially higher worst-group accuracies, with 10-40 percentage point improvements on a natural language inference task and two image tasks, while maintaining high average accuracies. Our results suggest that regularization is important for worst-group generalization in the overparameterized regime, even if it is not needed for average generalization. Finally, we introduce a stochastic optimization algorithm, with convergence guarantees, to efficiently train group DRO models.",
    "DOI": "10.48550/arxiv.1911.08731",
    "publisher": "arXiv",
    "title": "Distributionally Robust Neural Networks for Group Shifts: On the Importance of Regularization for Worst-Case Generalization",
    "URL": "https://doi.org/g93rnm",
    "version": "2",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.1911.08731"
  },
  {
    "type": "article",
    "id": "6W1y3ZrT",
    "categories": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Kaplan",
        "given": "Jared"
      },
      {
        "family": "McCandlish",
        "given": "Sam"
      },
      {
        "family": "Henighan",
        "given": "Tom"
      },
      {
        "family": "Brown",
        "given": "Tom B."
      },
      {
        "family": "Chess",
        "given": "Benjamin"
      },
      {
        "family": "Child",
        "given": "Rewon"
      },
      {
        "family": "Gray",
        "given": "Scott"
      },
      {
        "family": "Radford",
        "given": "Alec"
      },
      {
        "family": "Wu",
        "given": "Jeffrey"
      },
      {
        "family": "Amodei",
        "given": "Dario"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2020
        ]
      ]
    },
    "abstract": "We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.",
    "DOI": "10.48550/arxiv.2001.08361",
    "publisher": "arXiv",
    "title": "Scaling Laws for Neural Language Models",
    "URL": "https://doi.org/gtb96w",
    "version": "1",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2001.08361"
  },
  {
    "type": "article",
    "id": "GYqEQJ4V",
    "categories": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Hospedales",
        "given": "Timothy"
      },
      {
        "family": "Antoniou",
        "given": "Antreas"
      },
      {
        "family": "Micaelli",
        "given": "Paul"
      },
      {
        "family": "Storkey",
        "given": "Amos"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2020
        ]
      ]
    },
    "abstract": "The field of meta-learning, or learning-to-learn, has seen a dramatic rise in interest in recent years. Contrary to conventional approaches to AI where tasks are solved from scratch using a fixed learning algorithm, meta-learning aims to improve the learning algorithm itself, given the experience of multiple learning episodes. This paradigm provides an opportunity to tackle many conventional challenges of deep learning, including data and computation bottlenecks, as well as generalization. This survey describes the contemporary meta-learning landscape. We first discuss definitions of meta-learning and position it with respect to related fields, such as transfer learning and hyperparameter optimization. We then propose a new taxonomy that provides a more comprehensive breakdown of the space of meta-learning methods today. We survey promising applications and successes of meta-learning such as few-shot learning and reinforcement learning. Finally, we discuss outstanding challenges and promising areas for future research.",
    "DOI": "10.48550/arxiv.2004.05439",
    "publisher": "arXiv",
    "title": "Meta-Learning in Neural Networks: A Survey",
    "URL": "https://doi.org/g958zx",
    "version": "2",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2004.05439"
  },
  {
    "type": "article-journal",
    "id": "4fnOdPts",
    "categories": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Neurons and Cognition (q-bio.NC)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences",
      "FOS: Biological sciences",
      "FOS: Biological sciences"
    ],
    "author": [
      {
        "family": "Geirhos",
        "given": "Robert"
      },
      {
        "family": "Jacobsen",
        "given": "Jörn-Henrik"
      },
      {
        "family": "Michaelis",
        "given": "Claudio"
      },
      {
        "family": "Zemel",
        "given": "Richard"
      },
      {
        "family": "Brendel",
        "given": "Wieland"
      },
      {
        "family": "Bethge",
        "given": "Matthias"
      },
      {
        "family": "Wichmann",
        "given": "Felix A."
      }
    ],
    "issued": {
      "date-parts": [
        [
          2020
        ]
      ]
    },
    "abstract": "Deep learning has triggered the current rise of artificial intelligence and is the workhorse of today's machine intelligence. Numerous success stories have rapidly spread all over science, industry and society, but its limitations have only recently come into focus. In this perspective we seek to distill how many of deep learning's problems can be seen as different symptoms of the same underlying problem: shortcut learning. Shortcuts are decision rules that perform well on standard benchmarks but fail to transfer to more challenging testing conditions, such as real-world scenarios. Related issues are known in Comparative Psychology, Education and Linguistics, suggesting that shortcut learning may be a common characteristic of learning systems, biological and artificial alike. Based on these observations, we develop a set of recommendations for model interpretation and benchmarking, highlighting recent advances in machine learning to improve robustness and transferability from the lab to real-world applications.",
    "container-title": "arXiv",
    "DOI": "10.48550/arxiv.2004.07780",
    "publisher": "arXiv",
    "title": "Shortcut Learning in Deep Neural Networks",
    "URL": "https://doi.org/g93rnn",
    "version": "5",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2004.07780"
  },
  {
    "type": "article-journal",
    "id": "dO6oWLlh",
    "categories": [
      "Computation and Language (cs.CL)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Pfeiffer",
        "given": "Jonas"
      },
      {
        "family": "Kamath",
        "given": "Aishwarya"
      },
      {
        "family": "Rücklé",
        "given": "Andreas"
      },
      {
        "family": "Cho",
        "given": "Kyunghyun"
      },
      {
        "family": "Gurevych",
        "given": "Iryna"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2020
        ]
      ]
    },
    "abstract": "Sequential fine-tuning and multi-task learning are methods aiming to incorporate knowledge from multiple tasks; however, they suffer from catastrophic forgetting and difficulties in dataset balancing. To address these shortcomings, we propose AdapterFusion, a new two stage learning algorithm that leverages knowledge from multiple tasks. First, in the knowledge extraction stage we learn task specific parameters called adapters, that encapsulate the task-specific information. We then combine the adapters in a separate knowledge composition step. We show that by separating the two stages, i.e., knowledge extraction and knowledge composition, the classifier can effectively exploit the representations learned from multiple tasks in a non-destructive manner. We empirically evaluate AdapterFusion on 16 diverse NLU tasks, and find that it effectively combines various types of knowledge at different layers of the model. We show that our approach outperforms traditional strategies such as full fine-tuning as well as multi-task learning. Our code and adapters are available at AdapterHub.ml.",
    "container-title": "arXiv",
    "DOI": "10.48550/arxiv.2005.00247",
    "publisher": "arXiv",
    "title": "AdapterFusion: Non-Destructive Task Composition for Transfer Learning",
    "URL": "https://doi.org/g95xv7",
    "version": "3",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2005.00247"
  },
  {
    "type": "article",
    "id": "rtNEROOT",
    "categories": [
      "Computation and Language (cs.CL)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Brown",
        "given": "Tom B."
      },
      {
        "family": "Mann",
        "given": "Benjamin"
      },
      {
        "family": "Ryder",
        "given": "Nick"
      },
      {
        "family": "Subbiah",
        "given": "Melanie"
      },
      {
        "family": "Kaplan",
        "given": "Jared"
      },
      {
        "family": "Dhariwal",
        "given": "Prafulla"
      },
      {
        "family": "Neelakantan",
        "given": "Arvind"
      },
      {
        "family": "Shyam",
        "given": "Pranav"
      },
      {
        "family": "Sastry",
        "given": "Girish"
      },
      {
        "family": "Askell",
        "given": "Amanda"
      },
      {
        "family": "Agarwal",
        "given": "Sandhini"
      },
      {
        "family": "Herbert-Voss",
        "given": "Ariel"
      },
      {
        "family": "Krueger",
        "given": "Gretchen"
      },
      {
        "family": "Henighan",
        "given": "Tom"
      },
      {
        "family": "Child",
        "given": "Rewon"
      },
      {
        "family": "Ramesh",
        "given": "Aditya"
      },
      {
        "family": "Ziegler",
        "given": "Daniel M."
      },
      {
        "family": "Wu",
        "given": "Jeffrey"
      },
      {
        "family": "Winter",
        "given": "Clemens"
      },
      {
        "family": "Hesse",
        "given": "Christopher"
      },
      {
        "family": "Chen",
        "given": "Mark"
      },
      {
        "family": "Sigler",
        "given": "Eric"
      },
      {
        "family": "Litwin",
        "given": "Mateusz"
      },
      {
        "family": "Gray",
        "given": "Scott"
      },
      {
        "family": "Chess",
        "given": "Benjamin"
      },
      {
        "family": "Clark",
        "given": "Jack"
      },
      {
        "family": "Berner",
        "given": "Christopher"
      },
      {
        "family": "McCandlish",
        "given": "Sam"
      },
      {
        "family": "Radford",
        "given": "Alec"
      },
      {
        "family": "Sutskever",
        "given": "Ilya"
      },
      {
        "family": "Amodei",
        "given": "Dario"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2020
        ]
      ]
    },
    "abstract": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.",
    "DOI": "10.48550/arxiv.2005.14165",
    "publisher": "arXiv",
    "title": "Language Models are Few-Shot Learners",
    "URL": "https://doi.org/gpmv43",
    "version": "4",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2005.14165"
  },
  {
    "type": "article",
    "id": "b0a9ckui",
    "categories": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Koh",
        "given": "Pang Wei"
      },
      {
        "family": "Nguyen",
        "given": "Thao"
      },
      {
        "family": "Tang",
        "given": "Yew Siang"
      },
      {
        "family": "Mussmann",
        "given": "Stephen"
      },
      {
        "family": "Pierson",
        "given": "Emma"
      },
      {
        "family": "Kim",
        "given": "Been"
      },
      {
        "family": "Liang",
        "given": "Percy"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2020
        ]
      ]
    },
    "abstract": "We seek to learn models that we can interact with using high-level concepts: if the model did not think there was a bone spur in the x-ray, would it still predict severe arthritis? State-of-the-art models today do not typically support the manipulation of concepts like \"the existence of bone spurs\", as they are trained end-to-end to go directly from raw input (e.g., pixels) to output (e.g., arthritis severity). We revisit the classic idea of first predicting concepts that are provided at training time, and then using these concepts to predict the label. By construction, we can intervene on these concept bottleneck models by editing their predicted concept values and propagating these changes to the final prediction. On x-ray grading and bird identification, concept bottleneck models achieve competitive accuracy with standard end-to-end models, while enabling interpretation in terms of high-level clinical concepts (\"bone spurs\") or bird attributes (\"wing color\"). These models also allow for richer human-model interaction: accuracy improves significantly if we can correct model mistakes on concepts at test time.",
    "DOI": "10.48550/arxiv.2007.04612",
    "publisher": "arXiv",
    "title": "Concept Bottleneck Models",
    "URL": "https://doi.org/khz6",
    "version": "3",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2007.04612"
  },
  {
    "type": "article-journal",
    "id": "ZhjKyxtu",
    "categories": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Huisman",
        "given": "Mike"
      },
      {
        "family": "van Rijn",
        "given": "Jan N."
      },
      {
        "family": "Plaat",
        "given": "Aske"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2020
        ]
      ]
    },
    "abstract": "Deep neural networks can achieve great successes when presented with large data sets and sufficient computational resources. However, their ability to learn new concepts quickly is limited. Meta-learning is one approach to address this issue, by enabling the network to learn how to learn. The field of Deep Meta-Learning advances at great speed, but lacks a unified, in-depth overview of current techniques. With this work, we aim to bridge this gap. After providing the reader with a theoretical foundation, we investigate and summarize key methods, which are categorized into i)~metric-, ii)~model-, and iii)~optimization-based techniques. In addition, we identify the main open challenges, such as performance evaluations on heterogeneous benchmarks, and reduction of the computational costs of meta-learning.",
    "container-title": "arXiv",
    "DOI": "10.48550/arxiv.2010.03522",
    "publisher": "arXiv",
    "title": "A Survey of Deep Meta-Learning",
    "URL": "https://doi.org/g96dmh",
    "version": "2",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2010.03522"
  },
  {
    "type": "article",
    "id": "PKjSQOD",
    "categories": [
      "Machine Learning (cs.LG)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Koh",
        "given": "Pang Wei"
      },
      {
        "family": "Sagawa",
        "given": "Shiori"
      },
      {
        "family": "Marklund",
        "given": "Henrik"
      },
      {
        "family": "Xie",
        "given": "Sang Michael"
      },
      {
        "family": "Zhang",
        "given": "Marvin"
      },
      {
        "family": "Balsubramani",
        "given": "Akshay"
      },
      {
        "family": "Hu",
        "given": "Weihua"
      },
      {
        "family": "Yasunaga",
        "given": "Michihiro"
      },
      {
        "family": "Phillips",
        "given": "Richard Lanas"
      },
      {
        "family": "Gao",
        "given": "Irena"
      },
      {
        "family": "Lee",
        "given": "Tony"
      },
      {
        "family": "David",
        "given": "Etienne"
      },
      {
        "family": "Stavness",
        "given": "Ian"
      },
      {
        "family": "Guo",
        "given": "Wei"
      },
      {
        "family": "Earnshaw",
        "given": "Berton A."
      },
      {
        "family": "Haque",
        "given": "Imran S."
      },
      {
        "family": "Beery",
        "given": "Sara"
      },
      {
        "family": "Leskovec",
        "given": "Jure"
      },
      {
        "family": "Kundaje",
        "given": "Anshul"
      },
      {
        "family": "Pierson",
        "given": "Emma"
      },
      {
        "family": "Levine",
        "given": "Sergey"
      },
      {
        "family": "Finn",
        "given": "Chelsea"
      },
      {
        "family": "Liang",
        "given": "Percy"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2020
        ]
      ]
    },
    "abstract": "Distribution shifts -- where the training distribution differs from the test distribution -- can substantially degrade the accuracy of machine learning (ML) systems deployed in the wild. Despite their ubiquity in the real-world deployments, these distribution shifts are under-represented in the datasets widely used in the ML community today. To address this gap, we present WILDS, a curated benchmark of 10 datasets reflecting a diverse range of distribution shifts that naturally arise in real-world applications, such as shifts across hospitals for tumor identification; across camera traps for wildlife monitoring; and across time and location in satellite imaging and poverty mapping. On each dataset, we show that standard training yields substantially lower out-of-distribution than in-distribution performance. This gap remains even with models trained by existing methods for tackling distribution shifts, underscoring the need for new methods for training models that are more robust to the types of distribution shifts that arise in practice. To facilitate method development, we provide an open-source package that automates dataset loading, contains default model architectures and hyperparameters, and standardizes evaluations. Code and leaderboards are available at https://wilds.stanford.edu.",
    "DOI": "10.48550/arxiv.2012.07421",
    "publisher": "arXiv",
    "title": "WILDS: A Benchmark of in-the-Wild Distribution Shifts",
    "URL": "https://doi.org/g93rnp",
    "version": "3",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2012.07421"
  },
  {
    "type": "article",
    "id": "17tnf46zM",
    "categories": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Radford",
        "given": "Alec"
      },
      {
        "family": "Kim",
        "given": "Jong Wook"
      },
      {
        "family": "Hallacy",
        "given": "Chris"
      },
      {
        "family": "Ramesh",
        "given": "Aditya"
      },
      {
        "family": "Goh",
        "given": "Gabriel"
      },
      {
        "family": "Agarwal",
        "given": "Sandhini"
      },
      {
        "family": "Sastry",
        "given": "Girish"
      },
      {
        "family": "Askell",
        "given": "Amanda"
      },
      {
        "family": "Mishkin",
        "given": "Pamela"
      },
      {
        "family": "Clark",
        "given": "Jack"
      },
      {
        "family": "Krueger",
        "given": "Gretchen"
      },
      {
        "family": "Sutskever",
        "given": "Ilya"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2021
        ]
      ]
    },
    "abstract": "State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.",
    "DOI": "10.48550/arxiv.2103.00020",
    "publisher": "arXiv",
    "title": "Learning Transferable Visual Models From Natural Language Supervision",
    "URL": "https://doi.org/hs7z",
    "version": "1",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2103.00020"
  },
  {
    "type": "article",
    "id": "mvLZYXJQ",
    "categories": [
      "Computation and Language (cs.CL)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Dai",
        "given": "Damai"
      },
      {
        "family": "Dong",
        "given": "Li"
      },
      {
        "family": "Hao",
        "given": "Yaru"
      },
      {
        "family": "Sui",
        "given": "Zhifang"
      },
      {
        "family": "Chang",
        "given": "Baobao"
      },
      {
        "family": "Wei",
        "given": "Furu"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2021
        ]
      ]
    },
    "abstract": "Large-scale pretrained language models are surprisingly good at recalling factual knowledge presented in the training corpus. In this paper, we present preliminary studies on how factual knowledge is stored in pretrained Transformers by introducing the concept of knowledge neurons. Specifically, we examine the fill-in-the-blank cloze task for BERT. Given a relational fact, we propose a knowledge attribution method to identify the neurons that express the fact. We find that the activation of such knowledge neurons is positively correlated to the expression of their corresponding facts. In our case studies, we attempt to leverage knowledge neurons to edit (such as update, and erase) specific factual knowledge without fine-tuning. Our results shed light on understanding the storage of knowledge within pretrained Transformers. The code is available at https://github.com/Hunter-DDM/knowledge-neurons.",
    "DOI": "10.48550/arxiv.2104.08696",
    "publisher": "arXiv",
    "title": "Knowledge Neurons in Pretrained Transformers",
    "URL": "https://doi.org/gs8cqf",
    "version": "2",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2104.08696"
  },
  {
    "type": "article",
    "id": "gOxiOg9S",
    "categories": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Carmichael",
        "given": "Zachariah"
      },
      {
        "family": "Scheirer",
        "given": "Walter J."
      }
    ],
    "issued": {
      "date-parts": [
        [
          2021
        ]
      ]
    },
    "abstract": "Many applications of data-driven models demand transparency of decisions, especially in health care, criminal justice, and other high-stakes environments. Modern trends in machine learning research have led to algorithms that are increasingly intricate to the degree that they are considered to be black boxes. In an effort to reduce the opacity of decisions, methods have been proposed to construe the inner workings of such models in a human-comprehensible manner. These post hoc techniques are described as being universal explainers - capable of faithfully augmenting decisions with algorithmic insight. Unfortunately, there is little agreement about what constitutes a \"good\" explanation. Moreover, current methods of explanation evaluation are derived from either subjective or proxy means. In this work, we propose a framework for the evaluation of post hoc explainers on ground truth that is directly derived from the additive structure of a model. We demonstrate the efficacy of the framework in understanding explainers by evaluating popular explainers on thousands of synthetic and several real-world tasks. The framework unveils that explanations may be accurate but misattribute the importance of individual features.",
    "DOI": "10.48550/arxiv.2106.08376",
    "publisher": "arXiv",
    "title": "A Framework for Evaluating Post Hoc Feature-Additive Explainers",
    "URL": "https://doi.org/g958zz",
    "version": "2",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2106.08376"
  },
  {
    "type": "article",
    "id": "Pf2hk1xb",
    "categories": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Hu",
        "given": "Edward J."
      },
      {
        "family": "Shen",
        "given": "Yelong"
      },
      {
        "family": "Wallis",
        "given": "Phillip"
      },
      {
        "family": "Allen-Zhu",
        "given": "Zeyuan"
      },
      {
        "family": "Li",
        "given": "Yuanzhi"
      },
      {
        "family": "Wang",
        "given": "Shean"
      },
      {
        "family": "Wang",
        "given": "Lu"
      },
      {
        "family": "Chen",
        "given": "Weizhu"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2021
        ]
      ]
    },
    "abstract": "An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.",
    "DOI": "10.48550/arxiv.2106.09685",
    "publisher": "arXiv",
    "title": "LoRA: Low-Rank Adaptation of Large Language Models",
    "URL": "https://doi.org/gthszt",
    "version": "2",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2106.09685"
  },
  {
    "type": "article",
    "id": "1GbAsSOZV",
    "categories": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Bommasani",
        "given": "Rishi"
      },
      {
        "family": "Hudson",
        "given": "Drew A."
      },
      {
        "family": "Adeli",
        "given": "Ehsan"
      },
      {
        "family": "Altman",
        "given": "Russ"
      },
      {
        "family": "Arora",
        "given": "Simran"
      },
      {
        "family": "von Arx",
        "given": "Sydney"
      },
      {
        "family": "Bernstein",
        "given": "Michael S."
      },
      {
        "family": "Bohg",
        "given": "Jeannette"
      },
      {
        "family": "Bosselut",
        "given": "Antoine"
      },
      {
        "family": "Brunskill",
        "given": "Emma"
      },
      {
        "family": "Brynjolfsson",
        "given": "Erik"
      },
      {
        "family": "Buch",
        "given": "Shyamal"
      },
      {
        "family": "Card",
        "given": "Dallas"
      },
      {
        "family": "Castellon",
        "given": "Rodrigo"
      },
      {
        "family": "Chatterji",
        "given": "Niladri"
      },
      {
        "family": "Chen",
        "given": "Annie"
      },
      {
        "family": "Creel",
        "given": "Kathleen"
      },
      {
        "family": "Davis",
        "given": "Jared Quincy"
      },
      {
        "family": "Demszky",
        "given": "Dora"
      },
      {
        "family": "Donahue",
        "given": "Chris"
      },
      {
        "family": "Doumbouya",
        "given": "Moussa"
      },
      {
        "family": "Durmus",
        "given": "Esin"
      },
      {
        "family": "Ermon",
        "given": "Stefano"
      },
      {
        "family": "Etchemendy",
        "given": "John"
      },
      {
        "family": "Ethayarajh",
        "given": "Kawin"
      },
      {
        "family": "Fei-Fei",
        "given": "Li"
      },
      {
        "family": "Finn",
        "given": "Chelsea"
      },
      {
        "family": "Gale",
        "given": "Trevor"
      },
      {
        "family": "Gillespie",
        "given": "Lauren"
      },
      {
        "family": "Goel",
        "given": "Karan"
      },
      {
        "family": "Goodman",
        "given": "Noah"
      },
      {
        "family": "Grossman",
        "given": "Shelby"
      },
      {
        "family": "Guha",
        "given": "Neel"
      },
      {
        "family": "Hashimoto",
        "given": "Tatsunori"
      },
      {
        "family": "Henderson",
        "given": "Peter"
      },
      {
        "family": "Hewitt",
        "given": "John"
      },
      {
        "family": "Ho",
        "given": "Daniel E."
      },
      {
        "family": "Hong",
        "given": "Jenny"
      },
      {
        "family": "Hsu",
        "given": "Kyle"
      },
      {
        "family": "Huang",
        "given": "Jing"
      },
      {
        "family": "Icard",
        "given": "Thomas"
      },
      {
        "family": "Jain",
        "given": "Saahil"
      },
      {
        "family": "Jurafsky",
        "given": "Dan"
      },
      {
        "family": "Kalluri",
        "given": "Pratyusha"
      },
      {
        "family": "Karamcheti",
        "given": "Siddharth"
      },
      {
        "family": "Keeling",
        "given": "Geoff"
      },
      {
        "family": "Khani",
        "given": "Fereshte"
      },
      {
        "family": "Khattab",
        "given": "Omar"
      },
      {
        "family": "Koh",
        "given": "Pang Wei"
      },
      {
        "family": "Krass",
        "given": "Mark"
      },
      {
        "family": "Krishna",
        "given": "Ranjay"
      },
      {
        "family": "Kuditipudi",
        "given": "Rohith"
      },
      {
        "family": "Kumar",
        "given": "Ananya"
      },
      {
        "family": "Ladhak",
        "given": "Faisal"
      },
      {
        "family": "Lee",
        "given": "Mina"
      },
      {
        "family": "Lee",
        "given": "Tony"
      },
      {
        "family": "Leskovec",
        "given": "Jure"
      },
      {
        "family": "Levent",
        "given": "Isabelle"
      },
      {
        "family": "Li",
        "given": "Xiang Lisa"
      },
      {
        "family": "Li",
        "given": "Xuechen"
      },
      {
        "family": "Ma",
        "given": "Tengyu"
      },
      {
        "family": "Malik",
        "given": "Ali"
      },
      {
        "family": "Manning",
        "given": "Christopher D."
      },
      {
        "family": "Mirchandani",
        "given": "Suvir"
      },
      {
        "family": "Mitchell",
        "given": "Eric"
      },
      {
        "family": "Munyikwa",
        "given": "Zanele"
      },
      {
        "family": "Nair",
        "given": "Suraj"
      },
      {
        "family": "Narayan",
        "given": "Avanika"
      },
      {
        "family": "Narayanan",
        "given": "Deepak"
      },
      {
        "family": "Newman",
        "given": "Ben"
      },
      {
        "family": "Nie",
        "given": "Allen"
      },
      {
        "family": "Niebles",
        "given": "Juan Carlos"
      },
      {
        "family": "Nilforoshan",
        "given": "Hamed"
      },
      {
        "family": "Nyarko",
        "given": "Julian"
      },
      {
        "family": "Ogut",
        "given": "Giray"
      },
      {
        "family": "Orr",
        "given": "Laurel"
      },
      {
        "family": "Papadimitriou",
        "given": "Isabel"
      },
      {
        "family": "Park",
        "given": "Joon Sung"
      },
      {
        "family": "Piech",
        "given": "Chris"
      },
      {
        "family": "Portelance",
        "given": "Eva"
      },
      {
        "family": "Potts",
        "given": "Christopher"
      },
      {
        "family": "Raghunathan",
        "given": "Aditi"
      },
      {
        "family": "Reich",
        "given": "Rob"
      },
      {
        "family": "Ren",
        "given": "Hongyu"
      },
      {
        "family": "Rong",
        "given": "Frieda"
      },
      {
        "family": "Roohani",
        "given": "Yusuf"
      },
      {
        "family": "Ruiz",
        "given": "Camilo"
      },
      {
        "family": "Ryan",
        "given": "Jack"
      },
      {
        "family": "Ré",
        "given": "Christopher"
      },
      {
        "family": "Sadigh",
        "given": "Dorsa"
      },
      {
        "family": "Sagawa",
        "given": "Shiori"
      },
      {
        "family": "Santhanam",
        "given": "Keshav"
      },
      {
        "family": "Shih",
        "given": "Andy"
      },
      {
        "family": "Srinivasan",
        "given": "Krishnan"
      },
      {
        "family": "Tamkin",
        "given": "Alex"
      },
      {
        "family": "Taori",
        "given": "Rohan"
      },
      {
        "family": "Thomas",
        "given": "Armin W."
      },
      {
        "family": "Tramèr",
        "given": "Florian"
      },
      {
        "family": "Wang",
        "given": "Rose E."
      },
      {
        "family": "Wang",
        "given": "William"
      },
      {
        "family": "Wu",
        "given": "Bohan"
      },
      {
        "family": "Wu",
        "given": "Jiajun"
      },
      {
        "family": "Wu",
        "given": "Yuhuai"
      },
      {
        "family": "Xie",
        "given": "Sang Michael"
      },
      {
        "family": "Yasunaga",
        "given": "Michihiro"
      },
      {
        "family": "You",
        "given": "Jiaxuan"
      },
      {
        "family": "Zaharia",
        "given": "Matei"
      },
      {
        "family": "Zhang",
        "given": "Michael"
      },
      {
        "family": "Zhang",
        "given": "Tianyi"
      },
      {
        "family": "Zhang",
        "given": "Xikun"
      },
      {
        "family": "Zhang",
        "given": "Yuhui"
      },
      {
        "family": "Zheng",
        "given": "Lucia"
      },
      {
        "family": "Zhou",
        "given": "Kaitlyn"
      },
      {
        "family": "Liang",
        "given": "Percy"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2021
        ]
      ]
    },
    "abstract": "AI is undergoing a paradigm shift with the rise of models (e.g., BERT, DALL-E, GPT-3) that are trained on broad data at scale and are adaptable to a wide range of downstream tasks. We call these models foundation models to underscore their critically central yet incomplete character. This report provides a thorough account of the opportunities and risks of foundation models, ranging from their capabilities (e.g., language, vision, robotics, reasoning, human interaction) and technical principles(e.g., model architectures, training procedures, data, systems, security, evaluation, theory) to their applications (e.g., law, healthcare, education) and societal impact (e.g., inequity, misuse, economic and environmental impact, legal and ethical considerations). Though foundation models are based on standard deep learning and transfer learning, their scale results in new emergent capabilities,and their effectiveness across so many tasks incentivizes homogenization. Homogenization provides powerful leverage but demands caution, as the defects of the foundation model are inherited by all the adapted models downstream. Despite the impending widespread deployment of foundation models, we currently lack a clear understanding of how they work, when they fail, and what they are even capable of due to their emergent properties. To tackle these questions, we believe much of the critical research on foundation models will require deep interdisciplinary collaboration commensurate with their fundamentally sociotechnical nature.",
    "DOI": "10.48550/arxiv.2108.07258",
    "publisher": "arXiv",
    "title": "On the Opportunities and Risks of Foundation Models",
    "URL": "https://doi.org/hw3v",
    "version": "3",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2108.07258"
  },
  {
    "type": "article",
    "id": "qvjULhAd",
    "categories": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Min",
        "given": "Sewon"
      },
      {
        "family": "Lewis",
        "given": "Mike"
      },
      {
        "family": "Zettlemoyer",
        "given": "Luke"
      },
      {
        "family": "Hajishirzi",
        "given": "Hannaneh"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2021
        ]
      ]
    },
    "abstract": "We introduce MetaICL (Meta-training for In-Context Learning), a new meta-training framework for few-shot learning where a pretrained language model is tuned to do in-context learning on a large set of training tasks. This meta-training enables the model to more effectively learn a new task in context at test time, by simply conditioning on a few training examples with no parameter updates or task-specific templates. We experiment on a large, diverse collection of tasks consisting of 142 NLP datasets including classification, question answering, natural language inference, paraphrase detection and more, across seven different meta-training/target splits. MetaICL outperforms a range of baselines including in-context learning without meta-training and multi-task learning followed by zero-shot transfer. We find that the gains are particularly significant for target tasks that have domain shifts from the meta-training tasks, and that using a diverse set of the meta-training tasks is key to improvements. We also show that MetaICL approaches (and sometimes beats) the performance of models fully finetuned on the target task, and outperforms much bigger models with nearly 8x parameters. Finally, we show that MetaICL is complementary to human-written instructions, and the best performance can be achieved by combining both approaches.",
    "DOI": "10.48550/arxiv.2110.15943",
    "publisher": "arXiv",
    "title": "MetaICL: Learning to Learn In Context",
    "URL": "https://doi.org/g96dmj",
    "version": "2",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2110.15943"
  },
  {
    "type": "article",
    "id": "grNza1Og",
    "categories": [
      "Machine Learning (stat.ML)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Lengerich",
        "given": "Ben"
      },
      {
        "family": "Ellington",
        "given": "Caleb"
      },
      {
        "family": "Aragam",
        "given": "Bryon"
      },
      {
        "family": "Xing",
        "given": "Eric P."
      },
      {
        "family": "Kellis",
        "given": "Manolis"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2021
        ]
      ]
    },
    "abstract": "Context-specific Bayesian networks (i.e. directed acyclic graphs, DAGs) identify context-dependent relationships between variables, but the non-convexity induced by the acyclicity requirement makes it difficult to share information between context-specific estimators (e.g. with graph generator functions). For this reason, existing methods for inferring context-specific Bayesian networks have favored breaking datasets into subsamples, limiting statistical power and resolution, and preventing the use of multidimensional and latent contexts. To overcome this challenge, we propose NOTEARS-optimized Mixtures of Archetypal DAGs (NOTMAD). NOTMAD models context-specific Bayesian networks as the output of a function which learns to mix archetypal networks according to sample context. The archetypal networks are estimated jointly with the context-specific networks and do not require any prior knowledge. We encode the acyclicity constraint as a smooth regularization loss which is back-propagated to the mixing function; in this way, NOTMAD shares information between context-specific acyclic graphs, enabling the estimation of Bayesian network structures and parameters at even single-sample resolution. We demonstrate the utility of NOTMAD and sample-specific network inference through analysis and experiments, including patient-specific gene expression networks which correspond to morphological variation in cancer.",
    "DOI": "10.48550/arxiv.2111.01104",
    "publisher": "arXiv",
    "title": "NOTMAD: Estimating Bayesian Networks with Sample-Specific Structures and Parameters",
    "URL": "https://doi.org/gt68jc",
    "version": "1",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2111.01104"
  },
  {
    "type": "article",
    "id": "LybHVzmd",
    "categories": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Xie",
        "given": "Sang Michael"
      },
      {
        "family": "Raghunathan",
        "given": "Aditi"
      },
      {
        "family": "Liang",
        "given": "Percy"
      },
      {
        "family": "Ma",
        "given": "Tengyu"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2021
        ]
      ]
    },
    "abstract": "Large language models (LMs) such as GPT-3 have the surprising ability to do in-context learning, where the model learns to do a downstream task simply by conditioning on a prompt consisting of input-output examples. The LM learns from these examples without being explicitly pretrained to learn. Thus, it is unclear what enables in-context learning. In this paper, we study how in-context learning can emerge when pretraining documents have long-range coherence. Here, the LM must infer a latent document-level concept to generate coherent next tokens during pretraining. At test time, in-context learning occurs when the LM also infers a shared latent concept between examples in a prompt. We prove when this occurs despite a distribution mismatch between prompts and pretraining data in a setting where the pretraining distribution is a mixture of HMMs. In contrast to messy large-scale datasets used to train LMs capable of in-context learning, we generate a small-scale synthetic dataset (GINC) where Transformers and LSTMs both exhibit in-context learning. Beyond the theory, experiments on GINC exhibit large-scale real-world phenomena including improved in-context performance with model scaling (despite the same pretraining loss), sensitivity to example order, and instances where zero-shot is better than few-shot in-context learning.",
    "DOI": "10.48550/arxiv.2111.02080",
    "publisher": "arXiv",
    "title": "An Explanation of In-context Learning as Implicit Bayesian Inference",
    "URL": "https://doi.org/gtkkfs",
    "version": "6",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2111.02080"
  },
  {
    "type": "article",
    "id": "1FWgzslSV",
    "categories": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Wei",
        "given": "Jason"
      },
      {
        "family": "Wang",
        "given": "Xuezhi"
      },
      {
        "family": "Schuurmans",
        "given": "Dale"
      },
      {
        "family": "Bosma",
        "given": "Maarten"
      },
      {
        "family": "Ichter",
        "given": "Brian"
      },
      {
        "family": "Xia",
        "given": "Fei"
      },
      {
        "family": "Chi",
        "given": "Ed"
      },
      {
        "family": "Le",
        "given": "Quoc"
      },
      {
        "family": "Zhou",
        "given": "Denny"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2022
        ]
      ]
    },
    "abstract": "We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.",
    "DOI": "10.48550/arxiv.2201.11903",
    "publisher": "arXiv",
    "title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
    "URL": "https://doi.org/gr263w",
    "version": "6",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2201.11903"
  },
  {
    "type": "article",
    "id": "32o7NfNa",
    "categories": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences",
      "I.2.7"
    ],
    "author": [
      {
        "family": "Meng",
        "given": "Kevin"
      },
      {
        "family": "Bau",
        "given": "David"
      },
      {
        "family": "Andonian",
        "given": "Alex"
      },
      {
        "family": "Belinkov",
        "given": "Yonatan"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2022
        ]
      ]
    },
    "abstract": "We analyze the storage and recall of factual associations in autoregressive transformer language models, finding evidence that these associations correspond to localized, directly-editable computations. We first develop a causal intervention for identifying neuron activations that are decisive in a model's factual predictions. This reveals a distinct set of steps in middle-layer feed-forward modules that mediate factual predictions while processing subject tokens. To test our hypothesis that these computations correspond to factual association recall, we modify feed-forward weights to update specific factual associations using Rank-One Model Editing (ROME). We find that ROME is effective on a standard zero-shot relation extraction (zsRE) model-editing task, comparable to existing methods. To perform a more sensitive evaluation, we also evaluate ROME on a new dataset of counterfactual assertions, on which it simultaneously maintains both specificity and generalization, whereas other methods sacrifice one or another. Our results confirm an important role for mid-layer feed-forward modules in storing factual associations and suggest that direct manipulation of computational mechanisms may be a feasible approach for model editing. The code, dataset, visualizations, and an interactive demo notebook are available at https://rome.baulab.info/",
    "DOI": "10.48550/arxiv.2202.05262",
    "publisher": "arXiv",
    "title": "Locating and Editing Factual Associations in GPT",
    "URL": "https://doi.org/g958z2",
    "version": "5",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2202.05262"
  },
  {
    "type": "article",
    "id": "61xQEdRS",
    "categories": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Min",
        "given": "Sewon"
      },
      {
        "family": "Lyu",
        "given": "Xinxi"
      },
      {
        "family": "Holtzman",
        "given": "Ari"
      },
      {
        "family": "Artetxe",
        "given": "Mikel"
      },
      {
        "family": "Lewis",
        "given": "Mike"
      },
      {
        "family": "Hajishirzi",
        "given": "Hannaneh"
      },
      {
        "family": "Zettlemoyer",
        "given": "Luke"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2022
        ]
      ]
    },
    "abstract": "Large language models (LMs) are able to in-context learn -- perform a new task via inference alone by conditioning on a few input-label pairs (demonstrations) and making predictions for new inputs. However, there has been little understanding of how the model learns and which aspects of the demonstrations contribute to end task performance. In this paper, we show that ground truth demonstrations are in fact not required -- randomly replacing labels in the demonstrations barely hurts performance on a range of classification and multi-choce tasks, consistently over 12 different models including GPT-3. Instead, we find that other aspects of the demonstrations are the key drivers of end task performance, including the fact that they provide a few examples of (1) the label space, (2) the distribution of the input text, and (3) the overall format of the sequence. Together, our analysis provides a new way of understanding how and why in-context learning works, while opening up new questions about how much can be learned from large language models through inference alone.",
    "DOI": "10.48550/arxiv.2202.12837",
    "publisher": "arXiv",
    "title": "Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?",
    "URL": "https://doi.org/gtkkf4",
    "version": "2",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2202.12837"
  },
  {
    "type": "article",
    "id": "5q4aFZMi",
    "categories": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Hoffmann",
        "given": "Jordan"
      },
      {
        "family": "Borgeaud",
        "given": "Sebastian"
      },
      {
        "family": "Mensch",
        "given": "Arthur"
      },
      {
        "family": "Buchatskaya",
        "given": "Elena"
      },
      {
        "family": "Cai",
        "given": "Trevor"
      },
      {
        "family": "Rutherford",
        "given": "Eliza"
      },
      {
        "family": "Casas",
        "given": "Diego de Las"
      },
      {
        "family": "Hendricks",
        "given": "Lisa Anne"
      },
      {
        "family": "Welbl",
        "given": "Johannes"
      },
      {
        "family": "Clark",
        "given": "Aidan"
      },
      {
        "family": "Hennigan",
        "given": "Tom"
      },
      {
        "family": "Noland",
        "given": "Eric"
      },
      {
        "family": "Millican",
        "given": "Katie"
      },
      {
        "family": "Driessche",
        "given": "George van den"
      },
      {
        "family": "Damoc",
        "given": "Bogdan"
      },
      {
        "family": "Guy",
        "given": "Aurelia"
      },
      {
        "family": "Osindero",
        "given": "Simon"
      },
      {
        "family": "Simonyan",
        "given": "Karen"
      },
      {
        "family": "Elsen",
        "given": "Erich"
      },
      {
        "family": "Rae",
        "given": "Jack W."
      },
      {
        "family": "Vinyals",
        "given": "Oriol"
      },
      {
        "family": "Sifre",
        "given": "Laurent"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2022
        ]
      ]
    },
    "abstract": "We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over 400 language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. We test this hypothesis by training a predicted compute-optimal model, Chinchilla, that uses the same compute budget as Gopher but with 70B parameters and 4$\\times$ more more data. Chinchilla uniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks. This also means that Chinchilla uses substantially less compute for fine-tuning and inference, greatly facilitating downstream usage. As a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5% on the MMLU benchmark, greater than a 7% improvement over Gopher.",
    "DOI": "10.48550/arxiv.2203.15556",
    "publisher": "arXiv",
    "title": "Training Compute-Optimal Large Language Models",
    "URL": "https://doi.org/gthszs",
    "version": "1",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2203.15556"
  },
  {
    "type": "article",
    "id": "k6r0UwSv",
    "categories": [
      "Machine Learning (stat.ML)",
      "Computers and Society (cs.CY)",
      "Machine Learning (cs.LG)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Suriyakumar",
        "given": "Vinith M."
      },
      {
        "family": "Ghassemi",
        "given": "Marzyeh"
      },
      {
        "family": "Ustun",
        "given": "Berk"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2022
        ]
      ]
    },
    "abstract": "Machine learning models are often personalized with categorical attributes that are protected, sensitive, self-reported, or costly to acquire. In this work, we show models that are personalized with group attributes can reduce performance at a group level. We propose formal conditions to ensure the \"fair use\" of group attributes in prediction tasks by training one additional model -- i.e., collective preference guarantees to ensure that each group who provides personal data will receive a tailored gain in performance in return. We present sufficient conditions to ensure fair use in empirical risk minimization and characterize failure modes that lead to fair use violations due to standard practices in model development and deployment. We present a comprehensive empirical study of fair use in clinical prediction tasks. Our results demonstrate the prevalence of fair use violations in practice and illustrate simple interventions to mitigate their harm.",
    "DOI": "10.48550/arxiv.2206.02058",
    "publisher": "arXiv",
    "title": "When Personalization Harms: Reconsidering the Use of Group Attributes in Prediction",
    "URL": "https://doi.org/gt68jd",
    "version": "3",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2206.02058"
  },
  {
    "type": "article",
    "id": "6Y4uv63y",
    "categories": [
      "Computation and Language (cs.CL)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Wei",
        "given": "Jason"
      },
      {
        "family": "Tay",
        "given": "Yi"
      },
      {
        "family": "Bommasani",
        "given": "Rishi"
      },
      {
        "family": "Raffel",
        "given": "Colin"
      },
      {
        "family": "Zoph",
        "given": "Barret"
      },
      {
        "family": "Borgeaud",
        "given": "Sebastian"
      },
      {
        "family": "Yogatama",
        "given": "Dani"
      },
      {
        "family": "Bosma",
        "given": "Maarten"
      },
      {
        "family": "Zhou",
        "given": "Denny"
      },
      {
        "family": "Metzler",
        "given": "Donald"
      },
      {
        "family": "Chi",
        "given": "Ed H."
      },
      {
        "family": "Hashimoto",
        "given": "Tatsunori"
      },
      {
        "family": "Vinyals",
        "given": "Oriol"
      },
      {
        "family": "Liang",
        "given": "Percy"
      },
      {
        "family": "Dean",
        "given": "Jeff"
      },
      {
        "family": "Fedus",
        "given": "William"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2022
        ]
      ]
    },
    "abstract": "Scaling up language models has been shown to predictably improve performance and sample efficiency on a wide range of downstream tasks. This paper instead discusses an unpredictable phenomenon that we refer to as emergent abilities of large language models. We consider an ability to be emergent if it is not present in smaller models but is present in larger models. Thus, emergent abilities cannot be predicted simply by extrapolating the performance of smaller models. The existence of such emergence implies that additional scaling could further expand the range of capabilities of language models.",
    "DOI": "10.48550/arxiv.2206.07682",
    "publisher": "arXiv",
    "title": "Emergent Abilities of Large Language Models",
    "URL": "https://doi.org/jpr3",
    "version": "2",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2206.07682"
  },
  {
    "type": "article",
    "id": "rYveVDKJ",
    "categories": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Hollmann",
        "given": "Noah"
      },
      {
        "family": "Müller",
        "given": "Samuel"
      },
      {
        "family": "Eggensperger",
        "given": "Katharina"
      },
      {
        "family": "Hutter",
        "given": "Frank"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2022
        ]
      ]
    },
    "abstract": "We present TabPFN, a trained Transformer that can do supervised classification for small tabular datasets in less than a second, needs no hyperparameter tuning and is competitive with state-of-the-art classification methods. TabPFN performs in-context learning (ICL), it learns to make predictions using sequences of labeled examples (x, f(x)) given in the input, without requiring further parameter updates. TabPFN is fully entailed in the weights of our network, which accepts training and test samples as a set-valued input and yields predictions for the entire test set in a single forward pass. TabPFN is a Prior-Data Fitted Network (PFN) and is trained offline once, to approximate Bayesian inference on synthetic datasets drawn from our prior. This prior incorporates ideas from causal reasoning: It entails a large space of structural causal models with a preference for simple structures. On the 18 datasets in the OpenML-CC18 suite that contain up to 1 000 training data points, up to 100 purely numerical features without missing values, and up to 10 classes, we show that our method clearly outperforms boosted trees and performs on par with complex state-of-the-art AutoML systems with up to 230$\\times$ speedup. This increases to a 5 700$\\times$ speedup when using a GPU. We also validate these results on an additional 67 small numerical datasets from OpenML. We provide all our code, the trained TabPFN, an interactive browser demo and a Colab notebook at https://github.com/automl/TabPFN.",
    "DOI": "10.48550/arxiv.2207.01848",
    "publisher": "arXiv",
    "title": "TabPFN: A Transformer That Solves Small Tabular Classification Problems in a Second",
    "URL": "https://doi.org/g9t22b",
    "version": "6",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2207.01848"
  },
  {
    "type": "article",
    "id": "R7y5TKp9",
    "categories": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Garg",
        "given": "Shivam"
      },
      {
        "family": "Tsipras",
        "given": "Dimitris"
      },
      {
        "family": "Liang",
        "given": "Percy"
      },
      {
        "family": "Valiant",
        "given": "Gregory"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2022
        ]
      ]
    },
    "abstract": "In-context learning refers to the ability of a model to condition on a prompt sequence consisting of in-context examples (input-output pairs corresponding to some task) along with a new query input, and generate the corresponding output. Crucially, in-context learning happens only at inference time without any parameter updates to the model. While large language models such as GPT-3 exhibit some ability to perform in-context learning, it is unclear what the relationship is between tasks on which this succeeds and what is present in the training data. To make progress towards understanding in-context learning, we consider the well-defined problem of training a model to in-context learn a function class (e.g., linear functions): that is, given data derived from some functions in the class, can we train a model to in-context learn \"most\" functions from this class? We show empirically that standard Transformers can be trained from scratch to perform in-context learning of linear functions -- that is, the trained model is able to learn unseen linear functions from in-context examples with performance comparable to the optimal least squares estimator. In fact, in-context learning is possible even under two forms of distribution shift: (i) between the training data of the model and inference-time prompts, and (ii) between the in-context examples and the query input during inference. We also show that we can train Transformers to in-context learn more complex function classes -- namely sparse linear functions, two-layer neural networks, and decision trees -- with performance that matches or exceeds task-specific learning algorithms. Our code and models are available at https://github.com/dtsip/in-context-learning .",
    "DOI": "10.48550/arxiv.2208.01066",
    "publisher": "arXiv",
    "title": "What Can Transformers Learn In-Context? A Case Study of Simple Function Classes",
    "URL": "https://doi.org/g9t22c",
    "version": "3",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2208.01066"
  },
  {
    "type": "article",
    "id": "m1iAIGu4",
    "categories": [
      "Machine Learning (cs.LG)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Olsson",
        "given": "Catherine"
      },
      {
        "family": "Elhage",
        "given": "Nelson"
      },
      {
        "family": "Nanda",
        "given": "Neel"
      },
      {
        "family": "Joseph",
        "given": "Nicholas"
      },
      {
        "family": "DasSarma",
        "given": "Nova"
      },
      {
        "family": "Henighan",
        "given": "Tom"
      },
      {
        "family": "Mann",
        "given": "Ben"
      },
      {
        "family": "Askell",
        "given": "Amanda"
      },
      {
        "family": "Bai",
        "given": "Yuntao"
      },
      {
        "family": "Chen",
        "given": "Anna"
      },
      {
        "family": "Conerly",
        "given": "Tom"
      },
      {
        "family": "Drain",
        "given": "Dawn"
      },
      {
        "family": "Ganguli",
        "given": "Deep"
      },
      {
        "family": "Hatfield-Dodds",
        "given": "Zac"
      },
      {
        "family": "Hernandez",
        "given": "Danny"
      },
      {
        "family": "Johnston",
        "given": "Scott"
      },
      {
        "family": "Jones",
        "given": "Andy"
      },
      {
        "family": "Kernion",
        "given": "Jackson"
      },
      {
        "family": "Lovitt",
        "given": "Liane"
      },
      {
        "family": "Ndousse",
        "given": "Kamal"
      },
      {
        "family": "Amodei",
        "given": "Dario"
      },
      {
        "family": "Brown",
        "given": "Tom"
      },
      {
        "family": "Clark",
        "given": "Jack"
      },
      {
        "family": "Kaplan",
        "given": "Jared"
      },
      {
        "family": "McCandlish",
        "given": "Sam"
      },
      {
        "family": "Olah",
        "given": "Chris"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2022
        ]
      ]
    },
    "abstract": "\"Induction heads\" are attention heads that implement a simple algorithm to complete token sequences like [A][B] ... [A] -&gt; [B]. In this work, we present preliminary and indirect evidence for a hypothesis that induction heads might constitute the mechanism for the majority of all \"in-context learning\" in large transformer models (i.e. decreasing loss at increasing token indices). We find that induction heads develop at precisely the same point as a sudden sharp increase in in-context learning ability, visible as a bump in the training loss. We present six complementary lines of evidence, arguing that induction heads may be the mechanistic source of general in-context learning in transformer models of any size. For small attention-only models, we present strong, causal evidence; for larger models with MLPs, we present correlational evidence.",
    "DOI": "10.48550/arxiv.2209.11895",
    "publisher": "arXiv",
    "title": "In-context Learning and Induction Heads",
    "URL": "https://doi.org/g95xv8",
    "version": "1",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2209.11895"
  },
  {
    "type": "article",
    "id": "Xtwwrjzy",
    "categories": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Choi",
        "given": "Kristy"
      },
      {
        "family": "Cundy",
        "given": "Chris"
      },
      {
        "family": "Srivastava",
        "given": "Sanjari"
      },
      {
        "family": "Ermon",
        "given": "Stefano"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2022
        ]
      ]
    },
    "abstract": "Particularly in low-data regimes, an outstanding challenge in machine learning is developing principled techniques for augmenting our models with suitable priors. This is to encourage them to learn in ways that are compatible with our understanding of the world. But in contrast to generic priors such as shrinkage or sparsity, we draw inspiration from the recent successes of large-scale language models (LMs) to construct task-specific priors distilled from the rich knowledge of LMs. Our method, Language Model Priors (LMPriors), incorporates auxiliary natural language metadata about the task -- such as variable names and descriptions -- to encourage downstream model outputs to be consistent with the LM's common-sense reasoning based on the metadata. Empirically, we demonstrate that LMPriors improve model performance in settings where such natural language descriptions are available, and perform well on several tasks that benefit from such prior knowledge, such as feature selection, causal inference, and safe reinforcement learning.",
    "DOI": "10.48550/arxiv.2210.12530",
    "publisher": "arXiv",
    "title": "LMPriors: Pre-Trained Language Models as Task-Specific Priors",
    "URL": "https://doi.org/g9t22d",
    "version": "1",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2210.12530"
  },
  {
    "type": "article",
    "id": "lLSU1HNL",
    "categories": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Zhou",
        "given": "Helen"
      },
      {
        "family": "Balakrishnan",
        "given": "Sivaraman"
      },
      {
        "family": "Lipton",
        "given": "Zachary C."
      }
    ],
    "issued": {
      "date-parts": [
        [
          2022
        ]
      ]
    },
    "abstract": "Rates of missing data often depend on record-keeping policies and thus may change across times and locations, even when the underlying features are comparatively stable. In this paper, we introduce the problem of Domain Adaptation under Missingness Shift (DAMS). Here, (labeled) source data and (unlabeled) target data would be exchangeable but for different missing data mechanisms. We show that if missing data indicators are available, DAMS reduces to covariate shift. Addressing cases where such indicators are absent, we establish the following theoretical results for underreporting completely at random: (i) covariate shift is violated (adaptation is required); (ii) the optimal linear source predictor can perform arbitrarily worse on the target domain than always predicting the mean; (iii) the optimal target predictor can be identified, even when the missingness rates themselves are not; and (iv) for linear models, a simple analytic adjustment yields consistent estimates of the optimal target parameters. In experiments on synthetic and semi-synthetic data, we demonstrate the promise of our methods when assumptions hold. Finally, we discuss a rich family of future extensions.",
    "DOI": "10.48550/arxiv.2211.02093",
    "publisher": "arXiv",
    "title": "Domain Adaptation under Missingness Shift",
    "URL": "https://doi.org/g958z3",
    "version": "3",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2211.02093"
  },
  {
    "type": "article-journal",
    "id": "XxvKtGf4",
    "categories": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Liang",
        "given": "Percy"
      },
      {
        "family": "Bommasani",
        "given": "Rishi"
      },
      {
        "family": "Lee",
        "given": "Tony"
      },
      {
        "family": "Tsipras",
        "given": "Dimitris"
      },
      {
        "family": "Soylu",
        "given": "Dilara"
      },
      {
        "family": "Yasunaga",
        "given": "Michihiro"
      },
      {
        "family": "Zhang",
        "given": "Yian"
      },
      {
        "family": "Narayanan",
        "given": "Deepak"
      },
      {
        "family": "Wu",
        "given": "Yuhuai"
      },
      {
        "family": "Kumar",
        "given": "Ananya"
      },
      {
        "family": "Newman",
        "given": "Benjamin"
      },
      {
        "family": "Yuan",
        "given": "Binhang"
      },
      {
        "family": "Yan",
        "given": "Bobby"
      },
      {
        "family": "Zhang",
        "given": "Ce"
      },
      {
        "family": "Cosgrove",
        "given": "Christian"
      },
      {
        "family": "Manning",
        "given": "Christopher D."
      },
      {
        "family": "Ré",
        "given": "Christopher"
      },
      {
        "family": "Acosta-Navas",
        "given": "Diana"
      },
      {
        "family": "Hudson",
        "given": "Drew A."
      },
      {
        "family": "Zelikman",
        "given": "Eric"
      },
      {
        "family": "Durmus",
        "given": "Esin"
      },
      {
        "family": "Ladhak",
        "given": "Faisal"
      },
      {
        "family": "Rong",
        "given": "Frieda"
      },
      {
        "family": "Ren",
        "given": "Hongyu"
      },
      {
        "family": "Yao",
        "given": "Huaxiu"
      },
      {
        "family": "Wang",
        "given": "Jue"
      },
      {
        "family": "Santhanam",
        "given": "Keshav"
      },
      {
        "family": "Orr",
        "given": "Laurel"
      },
      {
        "family": "Zheng",
        "given": "Lucia"
      },
      {
        "family": "Yuksekgonul",
        "given": "Mert"
      },
      {
        "family": "Suzgun",
        "given": "Mirac"
      },
      {
        "family": "Kim",
        "given": "Nathan"
      },
      {
        "family": "Guha",
        "given": "Neel"
      },
      {
        "family": "Chatterji",
        "given": "Niladri"
      },
      {
        "family": "Khattab",
        "given": "Omar"
      },
      {
        "family": "Henderson",
        "given": "Peter"
      },
      {
        "family": "Huang",
        "given": "Qian"
      },
      {
        "family": "Chi",
        "given": "Ryan"
      },
      {
        "family": "Xie",
        "given": "Sang Michael"
      },
      {
        "family": "Santurkar",
        "given": "Shibani"
      },
      {
        "family": "Ganguli",
        "given": "Surya"
      },
      {
        "family": "Hashimoto",
        "given": "Tatsunori"
      },
      {
        "family": "Icard",
        "given": "Thomas"
      },
      {
        "family": "Zhang",
        "given": "Tianyi"
      },
      {
        "family": "Chaudhary",
        "given": "Vishrav"
      },
      {
        "family": "Wang",
        "given": "William"
      },
      {
        "family": "Li",
        "given": "Xuechen"
      },
      {
        "family": "Mai",
        "given": "Yifan"
      },
      {
        "family": "Zhang",
        "given": "Yuhui"
      },
      {
        "family": "Koreeda",
        "given": "Yuta"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2022
        ]
      ]
    },
    "abstract": "Language models (LMs) are becoming the foundation for almost all major language technologies, but their capabilities, limitations, and risks are not well understood. We present Holistic Evaluation of Language Models (HELM) to improve the transparency of language models. First, we taxonomize the vast space of potential scenarios (i.e. use cases) and metrics (i.e. desiderata) that are of interest for LMs. Then we select a broad subset based on coverage and feasibility, noting what's missing or underrepresented (e.g. question answering for neglected English dialects, metrics for trustworthiness). Second, we adopt a multi-metric approach: We measure 7 metrics (accuracy, calibration, robustness, fairness, bias, toxicity, and efficiency) for each of 16 core scenarios when possible (87.5% of the time). This ensures metrics beyond accuracy don't fall to the wayside, and that trade-offs are clearly exposed. We also perform 7 targeted evaluations, based on 26 targeted scenarios, to analyze specific aspects (e.g. reasoning, disinformation). Third, we conduct a large-scale evaluation of 30 prominent language models (spanning open, limited-access, and closed models) on all 42 scenarios, 21 of which were not previously used in mainstream LM evaluation. Prior to HELM, models on average were evaluated on just 17.9% of the core HELM scenarios, with some prominent models not sharing a single scenario in common. We improve this to 96.0%: now all 30 models have been densely benchmarked on the same core scenarios and metrics under standardized conditions. Our evaluation surfaces 25 top-level findings. For full transparency, we release all raw model prompts and completions publicly for further analysis, as well as a general modular toolkit. We intend for HELM to be a living benchmark for the community, continuously updated with new scenarios, metrics, and models.",
    "container-title": "arXiv",
    "DOI": "10.48550/arxiv.2211.09110",
    "publisher": "arXiv",
    "title": "Holistic Evaluation of Language Models",
    "URL": "https://doi.org/kh33",
    "version": "2",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2211.09110"
  },
  {
    "type": "article-journal",
    "id": "T8dO7ymx",
    "categories": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Van Ness",
        "given": "Mike"
      },
      {
        "family": "Bosschieter",
        "given": "Tomas M."
      },
      {
        "family": "Halpin-Gregorio",
        "given": "Roberto"
      },
      {
        "family": "Udell",
        "given": "Madeleine"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2022
        ]
      ]
    },
    "abstract": "Missing data is common in applied data science, particularly for tabular data sets found in healthcare, social sciences, and natural sciences. Most supervised learning methods only work on complete data, thus requiring preprocessing such as missing value imputation to work on incomplete data sets. However, imputation alone does not encode useful information about the missing values themselves. For data sets with informative missing patterns, the Missing Indicator Method (MIM), which adds indicator variables to indicate the missing pattern, can be used in conjunction with imputation to improve model performance. While commonly used in data science, MIM is surprisingly understudied from an empirical and especially theoretical perspective. In this paper, we show empirically and theoretically that MIM improves performance for informative missing values, and we prove that MIM does not hurt linear models asymptotically for uninformative missing values. Additionally, we find that for high-dimensional data sets with many uninformative indicators, MIM can induce model overfitting and thus test performance. To address this issue, we introduce Selective MIM (SMIM), a novel MIM extension that adds missing indicators only for features that have informative missing patterns. We show empirically that SMIM performs at least as well as MIM in general, and improves MIM for high-dimensional data. Lastly, to demonstrate the utility of MIM on real-world data science tasks, we demonstrate the effectiveness of MIM and SMIM on clinical tasks generated from the MIMIC-III database of electronic health records.",
    "container-title": "arXiv",
    "DOI": "10.48550/arxiv.2211.09259",
    "publisher": "arXiv",
    "title": "The Missing Indicator Method: From Low to High Dimensions",
    "URL": "https://doi.org/g958z4",
    "version": "2",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2211.09259"
  },
  {
    "type": "article",
    "id": "16QjxmbmC",
    "categories": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Akyürek",
        "given": "Ekin"
      },
      {
        "family": "Schuurmans",
        "given": "Dale"
      },
      {
        "family": "Andreas",
        "given": "Jacob"
      },
      {
        "family": "Ma",
        "given": "Tengyu"
      },
      {
        "family": "Zhou",
        "given": "Denny"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2022
        ]
      ]
    },
    "abstract": "Neural sequence models, especially transformers, exhibit a remarkable capacity for in-context learning. They can construct new predictors from sequences of labeled examples $(x, f(x))$ presented in the input without further parameter updates. We investigate the hypothesis that transformer-based in-context learners implement standard learning algorithms implicitly, by encoding smaller models in their activations, and updating these implicit models as new examples appear in the context. Using linear regression as a prototypical problem, we offer three sources of evidence for this hypothesis. First, we prove by construction that transformers can implement learning algorithms for linear models based on gradient descent and closed-form ridge regression. Second, we show that trained in-context learners closely match the predictors computed by gradient descent, ridge regression, and exact least-squares regression, transitioning between different predictors as transformer depth and dataset noise vary, and converging to Bayesian estimators for large widths and depths. Third, we present preliminary evidence that in-context learners share algorithmic features with these predictors: learners' late layers non-linearly encode weight vectors and moment matrices. These results suggest that in-context learning is understandable in algorithmic terms, and that (at least in the linear case) learners may rediscover standard estimation algorithms. Code and reference implementations are released at https://github.com/ekinakyurek/google-research/blob/master/incontext.",
    "DOI": "10.48550/arxiv.2211.15661",
    "publisher": "arXiv",
    "title": "What learning algorithm is in-context learning? Investigations with linear models",
    "URL": "https://doi.org/grq582",
    "version": "3",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2211.15661"
  },
  {
    "type": "article",
    "id": "167NV1YDH",
    "categories": [
      "Econometrics (econ.EM)",
      "Applications (stat.AP)",
      "Methodology (stat.ME)",
      "FOS: Economics and business",
      "FOS: Economics and business",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Martin",
        "given": "Gael M."
      },
      {
        "family": "Frazier",
        "given": "David T."
      },
      {
        "family": "Maneesoonthorn",
        "given": "Worapree"
      },
      {
        "family": "Loaiza-Maya",
        "given": "Ruben"
      },
      {
        "family": "Huber",
        "given": "Florian"
      },
      {
        "family": "Koop",
        "given": "Gary"
      },
      {
        "family": "Maheu",
        "given": "John"
      },
      {
        "family": "Nibbering",
        "given": "Didier"
      },
      {
        "family": "Panagiotelis",
        "given": "Anastasios"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2022
        ]
      ]
    },
    "abstract": "The Bayesian statistical paradigm provides a principled and coherent approach to probabilistic forecasting. Uncertainty about all unknowns that characterize any forecasting problem -- model, parameters, latent states -- is able to be quantified explicitly, and factored into the forecast distribution via the process of integration or averaging. Allied with the elegance of the method, Bayesian forecasting is now underpinned by the burgeoning field of Bayesian computation, which enables Bayesian forecasts to be produced for virtually any problem, no matter how large, or complex. The current state of play in Bayesian forecasting in economics and finance is the subject of this review. The aim is to provide the reader with an overview of modern approaches to the field, set in some historical context; and with sufficient computational detail given to assist the reader with implementation.",
    "DOI": "10.48550/arxiv.2212.03471",
    "publisher": "arXiv",
    "title": "Bayesian Forecasting in Economics and Finance: A Modern Review",
    "URL": "https://doi.org/g96dmk",
    "version": "2",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2212.03471"
  },
  {
    "type": "article",
    "id": "V6cbeqie",
    "categories": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "von Oswald",
        "given": "Johannes"
      },
      {
        "family": "Niklasson",
        "given": "Eyvind"
      },
      {
        "family": "Randazzo",
        "given": "Ettore"
      },
      {
        "family": "Sacramento",
        "given": "João"
      },
      {
        "family": "Mordvintsev",
        "given": "Alexander"
      },
      {
        "family": "Zhmoginov",
        "given": "Andrey"
      },
      {
        "family": "Vladymyrov",
        "given": "Max"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2022
        ]
      ]
    },
    "abstract": "At present, the mechanisms of in-context learning in Transformers are not well understood and remain mostly an intuition. In this paper, we suggest that training Transformers on auto-regressive objectives is closely related to gradient-based meta-learning formulations. We start by providing a simple weight construction that shows the equivalence of data transformations induced by 1) a single linear self-attention layer and by 2) gradient-descent (GD) on a regression loss. Motivated by that construction, we show empirically that when training self-attention-only Transformers on simple regression tasks either the models learned by GD and Transformers show great similarity or, remarkably, the weights found by optimization match the construction. Thus we show how trained Transformers become mesa-optimizers i.e. learn models by gradient descent in their forward pass. This allows us, at least in the domain of regression problems, to mechanistically understand the inner workings of in-context learning in optimized Transformers. Building on this insight, we furthermore identify how Transformers surpass the performance of plain gradient descent by learning an iterative curvature correction and learn linear models on deep data representations to solve non-linear regression tasks. Finally, we discuss intriguing parallels to a mechanism identified to be crucial for in-context learning termed induction-head (Olsson et al., 2022) and show how it could be understood as a specific case of in-context learning by gradient descent learning within Transformers. Code to reproduce the experiments can be found at https://github.com/google-research/self-organising-systems/tree/master/transformers_learn_icl_by_gd .",
    "DOI": "10.48550/arxiv.2212.07677",
    "publisher": "arXiv",
    "title": "Transformers learn in-context by gradient descent",
    "URL": "https://doi.org/gshbsq",
    "version": "2",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2212.07677"
  },
  {
    "type": "article",
    "id": "1AazNaZYl",
    "categories": [
      "Computation and Language (cs.CL)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Su",
        "given": "Hongjin"
      },
      {
        "family": "Shi",
        "given": "Weijia"
      },
      {
        "family": "Kasai",
        "given": "Jungo"
      },
      {
        "family": "Wang",
        "given": "Yizhong"
      },
      {
        "family": "Hu",
        "given": "Yushi"
      },
      {
        "family": "Ostendorf",
        "given": "Mari"
      },
      {
        "family": "Yih",
        "given": "Wen-tau"
      },
      {
        "family": "Smith",
        "given": "Noah A."
      },
      {
        "family": "Zettlemoyer",
        "given": "Luke"
      },
      {
        "family": "Yu",
        "given": "Tao"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2022
        ]
      ]
    },
    "abstract": "We introduce INSTRUCTOR, a new method for computing text embeddings given task instructions: every text input is embedded together with instructions explaining the use case (e.g., task and domain descriptions). Unlike encoders from prior work that are more specialized, INSTRUCTOR is a single embedder that can generate text embeddings tailored to different downstream tasks and domains, without any further training. We first annotate instructions for 330 diverse tasks and train INSTRUCTOR on this multitask mixture with a contrastive loss. We evaluate INSTRUCTOR on 70 embedding evaluation tasks (66 of which are unseen during training), ranging from classification and information retrieval to semantic textual similarity and text generation evaluation. INSTRUCTOR, while having an order of magnitude fewer parameters than the previous best model, achieves state-of-the-art performance, with an average improvement of 3.4% compared to the previous best results on the 70 diverse datasets. Our analysis suggests that INSTRUCTOR is robust to changes in instructions, and that instruction finetuning mitigates the challenge of training a single model on diverse datasets. Our model, code, and data are available at https://instructor-embedding.github.io.",
    "DOI": "10.48550/arxiv.2212.09741",
    "publisher": "arXiv",
    "title": "One Embedder, Any Task: Instruction-Finetuned Text Embeddings",
    "URL": "https://doi.org/g9t22f",
    "version": "3",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2212.09741"
  },
  {
    "type": "article",
    "id": "16Xv40Ngd",
    "categories": [
      "Computation and Language (cs.CL)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Dai",
        "given": "Damai"
      },
      {
        "family": "Sun",
        "given": "Yutao"
      },
      {
        "family": "Dong",
        "given": "Li"
      },
      {
        "family": "Hao",
        "given": "Yaru"
      },
      {
        "family": "Ma",
        "given": "Shuming"
      },
      {
        "family": "Sui",
        "given": "Zhifang"
      },
      {
        "family": "Wei",
        "given": "Furu"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2022
        ]
      ]
    },
    "abstract": "Large pretrained language models have shown surprising in-context learning (ICL) ability. With a few demonstration input-label pairs, they can predict the label for an unseen input without parameter updates. Despite the great success in performance, its working mechanism still remains an open question. In this paper, we explain language models as meta-optimizers and understand in-context learning as implicit finetuning. Theoretically, we figure out that Transformer attention has a dual form of gradient descent. On top of it, we understand ICL as follows: GPT first produces meta-gradients according to the demonstration examples, and then these meta-gradients are applied to the original GPT to build an ICL model. We comprehensively compare the behaviors of in-context learning and explicit finetuning on real tasks to provide empirical evidence that supports our understanding. Experimental results show that in-context learning behaves similarly to explicit finetuning from multiple perspectives. Inspired by the dual form between Transformer attention and gradient descent, we design a momentum-based attention by analogy with gradient descent with momentum. The improved performance over vanilla attention further supports our understanding from another perspective, and more importantly, shows the potential to utilize our understanding for future model design. The code is available at \\url{https://aka.ms/icl}.",
    "DOI": "10.48550/arxiv.2212.10559",
    "publisher": "arXiv",
    "title": "Why Can GPT Learn In-Context? Language Models Implicitly Perform Gradient Descent as Meta-Optimizers",
    "URL": "https://doi.org/gtkkf9",
    "version": "3",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2212.10559"
  },
  {
    "type": "article",
    "id": "1EkVeYD9V",
    "categories": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Dong",
        "given": "Qingxiu"
      },
      {
        "family": "Li",
        "given": "Lei"
      },
      {
        "family": "Dai",
        "given": "Damai"
      },
      {
        "family": "Zheng",
        "given": "Ce"
      },
      {
        "family": "Ma",
        "given": "Jingyuan"
      },
      {
        "family": "Li",
        "given": "Rui"
      },
      {
        "family": "Xia",
        "given": "Heming"
      },
      {
        "family": "Xu",
        "given": "Jingjing"
      },
      {
        "family": "Wu",
        "given": "Zhiyong"
      },
      {
        "family": "Liu",
        "given": "Tianyu"
      },
      {
        "family": "Chang",
        "given": "Baobao"
      },
      {
        "family": "Sun",
        "given": "Xu"
      },
      {
        "family": "Li",
        "given": "Lei"
      },
      {
        "family": "Sui",
        "given": "Zhifang"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2023
        ]
      ]
    },
    "abstract": "With the increasing capabilities of large language models (LLMs), in-context learning (ICL) has emerged as a new paradigm for natural language processing (NLP), where LLMs make predictions based on contexts augmented with a few examples. It has been a significant trend to explore ICL to evaluate and extrapolate the ability of LLMs. In this paper, we aim to survey and summarize the progress and challenges of ICL. We first present a formal definition of ICL and clarify its correlation to related studies. Then, we organize and discuss advanced techniques, including training strategies, prompt designing strategies, and related analysis. Additionally, we explore various ICL application scenarios, such as data engineering and knowledge updating. Finally, we address the challenges of ICL and suggest potential directions for further research. We hope that our work can encourage more research on uncovering how ICL works and improving ICL.",
    "DOI": "10.48550/arxiv.2301.00234",
    "publisher": "arXiv",
    "title": "A Survey on In-context Learning",
    "URL": "https://doi.org/gsv9x4",
    "version": "6",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2301.00234"
  },
  {
    "type": "article",
    "id": "11UHeGMyc",
    "categories": [
      "Methodology (stat.ME)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Muñoz",
        "given": "Johanna"
      },
      {
        "family": "Egger",
        "given": "Matthias"
      },
      {
        "family": "Efthimiou",
        "given": "Orestis"
      },
      {
        "family": "Audigier",
        "given": "Vincent"
      },
      {
        "family": "de Jong",
        "given": "Valentijn M. T."
      },
      {
        "family": "Debray",
        "given": "Thomas. P. A."
      }
    ],
    "issued": {
      "date-parts": [
        [
          2023
        ]
      ]
    },
    "abstract": "Missing data is a common problem in medical research, and is commonly addressed using multiple imputation. Although traditional imputation methods allow for valid statistical inference when data are missing at random (MAR), their implementation is problematic when the presence of missingness depends on unobserved variables, i.e. the data are missing not at random (MNAR). Unfortunately, this MNAR situation is rather common, in observational studies, registries and other sources of real-world data. While several imputation methods have been proposed for addressing individual studies when data are MNAR, their application and validity in large datasets with multilevel structure remains unclear. We therefore explored the consequence of MNAR data in hierarchical data in-depth, and proposed a novel multilevel imputation method for common missing patterns in clustered datasets. This method is based on the principles of Heckman selection models and adopts a two-stage meta-analysis approach to impute binary and continuous variables that may be outcomes or predictors and that are systematically or sporadically missing. After evaluating the proposed imputation model in simulated scenarios, we illustrate it use in a cross-sectional community survey to estimate the prevalence of malaria parasitemia in children aged 2-10 years in five subregions in Uganda.",
    "DOI": "10.48550/arxiv.2301.05043",
    "publisher": "arXiv",
    "title": "Multiple imputation of incomplete multilevel data using Heckman selection models",
    "URL": "https://doi.org/g958z5",
    "version": "1",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2301.05043"
  },
  {
    "type": "article",
    "id": "iJglbhTY",
    "categories": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Zhou",
        "given": "Ce"
      },
      {
        "family": "Li",
        "given": "Qian"
      },
      {
        "family": "Li",
        "given": "Chen"
      },
      {
        "family": "Yu",
        "given": "Jun"
      },
      {
        "family": "Liu",
        "given": "Yixin"
      },
      {
        "family": "Wang",
        "given": "Guangjing"
      },
      {
        "family": "Zhang",
        "given": "Kai"
      },
      {
        "family": "Ji",
        "given": "Cheng"
      },
      {
        "family": "Yan",
        "given": "Qiben"
      },
      {
        "family": "He",
        "given": "Lifang"
      },
      {
        "family": "Peng",
        "given": "Hao"
      },
      {
        "family": "Li",
        "given": "Jianxin"
      },
      {
        "family": "Wu",
        "given": "Jia"
      },
      {
        "family": "Liu",
        "given": "Ziwei"
      },
      {
        "family": "Xie",
        "given": "Pengtao"
      },
      {
        "family": "Xiong",
        "given": "Caiming"
      },
      {
        "family": "Pei",
        "given": "Jian"
      },
      {
        "family": "Yu",
        "given": "Philip S."
      },
      {
        "family": "Sun",
        "given": "Lichao"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2023
        ]
      ]
    },
    "abstract": "Pretrained Foundation Models (PFMs) are regarded as the foundation for various downstream tasks with different data modalities. A PFM (e.g., BERT, ChatGPT, and GPT-4) is trained on large-scale data which provides a reasonable parameter initialization for a wide range of downstream applications. BERT learns bidirectional encoder representations from Transformers, which are trained on large datasets as contextual language models. Similarly, the generative pretrained transformer (GPT) method employs Transformers as the feature extractor and is trained using an autoregressive paradigm on large datasets. Recently, ChatGPT shows promising success on large language models, which applies an autoregressive language model with zero shot or few shot prompting. The remarkable achievements of PFM have brought significant breakthroughs to various fields of AI. Numerous studies have proposed different methods, raising the demand for an updated survey. This study provides a comprehensive review of recent research advancements, challenges, and opportunities for PFMs in text, image, graph, as well as other data modalities. The review covers the basic components and existing pretraining methods used in natural language processing, computer vision, and graph learning. Additionally, it explores advanced PFMs used for different data modalities and unified PFMs that consider data quality and quantity. The review also discusses research related to the fundamentals of PFMs, such as model efficiency and compression, security, and privacy. Finally, the study provides key implications, future research directions, challenges, and open problems in the field of PFMs. Overall, this survey aims to shed light on the research of the PFMs on scalability, security, logical reasoning ability, cross-domain learning ability, and the user-friendly interactive ability for artificial general intelligence.",
    "DOI": "10.48550/arxiv.2302.09419",
    "publisher": "arXiv",
    "title": "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT",
    "URL": "https://doi.org/g8vjrk",
    "version": "3",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2302.09419"
  },
  {
    "type": "article",
    "id": "11RvF4F7q",
    "categories": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "McInerney",
        "given": "Denis Jered"
      },
      {
        "family": "Young",
        "given": "Geoffrey"
      },
      {
        "family": "van de Meent",
        "given": "Jan-Willem"
      },
      {
        "family": "Wallace",
        "given": "Byron C."
      }
    ],
    "issued": {
      "date-parts": [
        [
          2023
        ]
      ]
    },
    "abstract": "We propose CHiLL (Crafting High-Level Latents), an approach for natural-language specification of features for linear models. CHiLL prompts LLMs with expert-crafted queries to generate interpretable features from health records. The resulting noisy labels are then used to train a simple linear classifier. Generating features based on queries to an LLM can empower physicians to use their domain expertise to craft features that are clinically meaningful for a downstream task of interest, without having to manually extract these from raw EHR. We are motivated by a real-world risk prediction task, but as a reproducible proxy, we use MIMIC-III and MIMIC-CXR data and standard predictive tasks (e.g., 30-day readmission) to evaluate this approach. We find that linear models using automatically extracted features are comparably performant to models using reference features, and provide greater interpretability than linear models using \"Bag-of-Words\" features. We verify that learned feature weights align well with clinical expectations.",
    "DOI": "10.48550/arxiv.2302.12343",
    "publisher": "arXiv",
    "title": "CHiLL: Zero-shot Custom Interpretable Feature Extraction from Clinical Notes with Large Language Models",
    "URL": "https://doi.org/g9t22g",
    "version": "2",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2302.12343"
  },
  {
    "type": "article",
    "id": "17lpGtuH5",
    "categories": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "OpenAI"
      },
      {
        "family": "Achiam",
        "given": "Josh"
      },
      {
        "family": "Adler",
        "given": "Steven"
      },
      {
        "family": "Agarwal",
        "given": "Sandhini"
      },
      {
        "family": "Ahmad",
        "given": "Lama"
      },
      {
        "family": "Akkaya",
        "given": "Ilge"
      },
      {
        "family": "Aleman",
        "given": "Florencia Leoni"
      },
      {
        "family": "Almeida",
        "given": "Diogo"
      },
      {
        "family": "Altenschmidt",
        "given": "Janko"
      },
      {
        "family": "Altman",
        "given": "Sam"
      },
      {
        "family": "Anadkat",
        "given": "Shyamal"
      },
      {
        "family": "Avila",
        "given": "Red"
      },
      {
        "family": "Babuschkin",
        "given": "Igor"
      },
      {
        "family": "Balaji",
        "given": "Suchir"
      },
      {
        "family": "Balcom",
        "given": "Valerie"
      },
      {
        "family": "Baltescu",
        "given": "Paul"
      },
      {
        "family": "Bao",
        "given": "Haiming"
      },
      {
        "family": "Bavarian",
        "given": "Mohammad"
      },
      {
        "family": "Belgum",
        "given": "Jeff"
      },
      {
        "family": "Bello",
        "given": "Irwan"
      },
      {
        "family": "Berdine",
        "given": "Jake"
      },
      {
        "family": "Bernadett-Shapiro",
        "given": "Gabriel"
      },
      {
        "family": "Berner",
        "given": "Christopher"
      },
      {
        "family": "Bogdonoff",
        "given": "Lenny"
      },
      {
        "family": "Boiko",
        "given": "Oleg"
      },
      {
        "family": "Boyd",
        "given": "Madelaine"
      },
      {
        "family": "Brakman",
        "given": "Anna-Luisa"
      },
      {
        "family": "Brockman",
        "given": "Greg"
      },
      {
        "family": "Brooks",
        "given": "Tim"
      },
      {
        "family": "Brundage",
        "given": "Miles"
      },
      {
        "family": "Button",
        "given": "Kevin"
      },
      {
        "family": "Cai",
        "given": "Trevor"
      },
      {
        "family": "Campbell",
        "given": "Rosie"
      },
      {
        "family": "Cann",
        "given": "Andrew"
      },
      {
        "family": "Carey",
        "given": "Brittany"
      },
      {
        "family": "Carlson",
        "given": "Chelsea"
      },
      {
        "family": "Carmichael",
        "given": "Rory"
      },
      {
        "family": "Chan",
        "given": "Brooke"
      },
      {
        "family": "Chang",
        "given": "Che"
      },
      {
        "family": "Chantzis",
        "given": "Fotis"
      },
      {
        "family": "Chen",
        "given": "Derek"
      },
      {
        "family": "Chen",
        "given": "Sully"
      },
      {
        "family": "Chen",
        "given": "Ruby"
      },
      {
        "family": "Chen",
        "given": "Jason"
      },
      {
        "family": "Chen",
        "given": "Mark"
      },
      {
        "family": "Chess",
        "given": "Ben"
      },
      {
        "family": "Cho",
        "given": "Chester"
      },
      {
        "family": "Chu",
        "given": "Casey"
      },
      {
        "family": "Chung",
        "given": "Hyung Won"
      },
      {
        "family": "Cummings",
        "given": "Dave"
      },
      {
        "family": "Currier",
        "given": "Jeremiah"
      },
      {
        "family": "Dai",
        "given": "Yunxing"
      },
      {
        "family": "Decareaux",
        "given": "Cory"
      },
      {
        "family": "Degry",
        "given": "Thomas"
      },
      {
        "family": "Deutsch",
        "given": "Noah"
      },
      {
        "family": "Deville",
        "given": "Damien"
      },
      {
        "family": "Dhar",
        "given": "Arka"
      },
      {
        "family": "Dohan",
        "given": "David"
      },
      {
        "family": "Dowling",
        "given": "Steve"
      },
      {
        "family": "Dunning",
        "given": "Sheila"
      },
      {
        "family": "Ecoffet",
        "given": "Adrien"
      },
      {
        "family": "Eleti",
        "given": "Atty"
      },
      {
        "family": "Eloundou",
        "given": "Tyna"
      },
      {
        "family": "Farhi",
        "given": "David"
      },
      {
        "family": "Fedus",
        "given": "Liam"
      },
      {
        "family": "Felix",
        "given": "Niko"
      },
      {
        "family": "Fishman",
        "given": "Simón Posada"
      },
      {
        "family": "Forte",
        "given": "Juston"
      },
      {
        "family": "Fulford",
        "given": "Isabella"
      },
      {
        "family": "Gao",
        "given": "Leo"
      },
      {
        "family": "Georges",
        "given": "Elie"
      },
      {
        "family": "Gibson",
        "given": "Christian"
      },
      {
        "family": "Goel",
        "given": "Vik"
      },
      {
        "family": "Gogineni",
        "given": "Tarun"
      },
      {
        "family": "Goh",
        "given": "Gabriel"
      },
      {
        "family": "Gontijo-Lopes",
        "given": "Rapha"
      },
      {
        "family": "Gordon",
        "given": "Jonathan"
      },
      {
        "family": "Grafstein",
        "given": "Morgan"
      },
      {
        "family": "Gray",
        "given": "Scott"
      },
      {
        "family": "Greene",
        "given": "Ryan"
      },
      {
        "family": "Gross",
        "given": "Joshua"
      },
      {
        "family": "Gu",
        "given": "Shixiang Shane"
      },
      {
        "family": "Guo",
        "given": "Yufei"
      },
      {
        "family": "Hallacy",
        "given": "Chris"
      },
      {
        "family": "Han",
        "given": "Jesse"
      },
      {
        "family": "Harris",
        "given": "Jeff"
      },
      {
        "family": "He",
        "given": "Yuchen"
      },
      {
        "family": "Heaton",
        "given": "Mike"
      },
      {
        "family": "Heidecke",
        "given": "Johannes"
      },
      {
        "family": "Hesse",
        "given": "Chris"
      },
      {
        "family": "Hickey",
        "given": "Alan"
      },
      {
        "family": "Hickey",
        "given": "Wade"
      },
      {
        "family": "Hoeschele",
        "given": "Peter"
      },
      {
        "family": "Houghton",
        "given": "Brandon"
      },
      {
        "family": "Hsu",
        "given": "Kenny"
      },
      {
        "family": "Hu",
        "given": "Shengli"
      },
      {
        "family": "Hu",
        "given": "Xin"
      },
      {
        "family": "Huizinga",
        "given": "Joost"
      },
      {
        "family": "Jain",
        "given": "Shantanu"
      },
      {
        "family": "Jain",
        "given": "Shawn"
      },
      {
        "family": "Jang",
        "given": "Joanne"
      },
      {
        "family": "Jiang",
        "given": "Angela"
      },
      {
        "family": "Jiang",
        "given": "Roger"
      },
      {
        "family": "Jin",
        "given": "Haozhun"
      },
      {
        "family": "Jin",
        "given": "Denny"
      },
      {
        "family": "Jomoto",
        "given": "Shino"
      },
      {
        "family": "Jonn",
        "given": "Billie"
      },
      {
        "family": "Jun",
        "given": "Heewoo"
      },
      {
        "family": "Kaftan",
        "given": "Tomer"
      },
      {
        "family": "Kaiser",
        "given": "Łukasz"
      },
      {
        "family": "Kamali",
        "given": "Ali"
      },
      {
        "family": "Kanitscheider",
        "given": "Ingmar"
      },
      {
        "family": "Keskar",
        "given": "Nitish Shirish"
      },
      {
        "family": "Khan",
        "given": "Tabarak"
      },
      {
        "family": "Kilpatrick",
        "given": "Logan"
      },
      {
        "family": "Kim",
        "given": "Jong Wook"
      },
      {
        "family": "Kim",
        "given": "Christina"
      },
      {
        "family": "Kim",
        "given": "Yongjik"
      },
      {
        "family": "Kirchner",
        "given": "Jan Hendrik"
      },
      {
        "family": "Kiros",
        "given": "Jamie"
      },
      {
        "family": "Knight",
        "given": "Matt"
      },
      {
        "family": "Kokotajlo",
        "given": "Daniel"
      },
      {
        "family": "Kondraciuk",
        "given": "Łukasz"
      },
      {
        "family": "Kondrich",
        "given": "Andrew"
      },
      {
        "family": "Konstantinidis",
        "given": "Aris"
      },
      {
        "family": "Kosic",
        "given": "Kyle"
      },
      {
        "family": "Krueger",
        "given": "Gretchen"
      },
      {
        "family": "Kuo",
        "given": "Vishal"
      },
      {
        "family": "Lampe",
        "given": "Michael"
      },
      {
        "family": "Lan",
        "given": "Ikai"
      },
      {
        "family": "Lee",
        "given": "Teddy"
      },
      {
        "family": "Leike",
        "given": "Jan"
      },
      {
        "family": "Leung",
        "given": "Jade"
      },
      {
        "family": "Levy",
        "given": "Daniel"
      },
      {
        "family": "Li",
        "given": "Chak Ming"
      },
      {
        "family": "Lim",
        "given": "Rachel"
      },
      {
        "family": "Lin",
        "given": "Molly"
      },
      {
        "family": "Lin",
        "given": "Stephanie"
      },
      {
        "family": "Litwin",
        "given": "Mateusz"
      },
      {
        "family": "Lopez",
        "given": "Theresa"
      },
      {
        "family": "Lowe",
        "given": "Ryan"
      },
      {
        "family": "Lue",
        "given": "Patricia"
      },
      {
        "family": "Makanju",
        "given": "Anna"
      },
      {
        "family": "Malfacini",
        "given": "Kim"
      },
      {
        "family": "Manning",
        "given": "Sam"
      },
      {
        "family": "Markov",
        "given": "Todor"
      },
      {
        "family": "Markovski",
        "given": "Yaniv"
      },
      {
        "family": "Martin",
        "given": "Bianca"
      },
      {
        "family": "Mayer",
        "given": "Katie"
      },
      {
        "family": "Mayne",
        "given": "Andrew"
      },
      {
        "family": "McGrew",
        "given": "Bob"
      },
      {
        "family": "McKinney",
        "given": "Scott Mayer"
      },
      {
        "family": "McLeavey",
        "given": "Christine"
      },
      {
        "family": "McMillan",
        "given": "Paul"
      },
      {
        "family": "McNeil",
        "given": "Jake"
      },
      {
        "family": "Medina",
        "given": "David"
      },
      {
        "family": "Mehta",
        "given": "Aalok"
      },
      {
        "family": "Menick",
        "given": "Jacob"
      },
      {
        "family": "Metz",
        "given": "Luke"
      },
      {
        "family": "Mishchenko",
        "given": "Andrey"
      },
      {
        "family": "Mishkin",
        "given": "Pamela"
      },
      {
        "family": "Monaco",
        "given": "Vinnie"
      },
      {
        "family": "Morikawa",
        "given": "Evan"
      },
      {
        "family": "Mossing",
        "given": "Daniel"
      },
      {
        "family": "Mu",
        "given": "Tong"
      },
      {
        "family": "Murati",
        "given": "Mira"
      },
      {
        "family": "Murk",
        "given": "Oleg"
      },
      {
        "family": "Mély",
        "given": "David"
      },
      {
        "family": "Nair",
        "given": "Ashvin"
      },
      {
        "family": "Nakano",
        "given": "Reiichiro"
      },
      {
        "family": "Nayak",
        "given": "Rajeev"
      },
      {
        "family": "Neelakantan",
        "given": "Arvind"
      },
      {
        "family": "Ngo",
        "given": "Richard"
      },
      {
        "family": "Noh",
        "given": "Hyeonwoo"
      },
      {
        "family": "Ouyang",
        "given": "Long"
      },
      {
        "family": "O'Keefe",
        "given": "Cullen"
      },
      {
        "family": "Pachocki",
        "given": "Jakub"
      },
      {
        "family": "Paino",
        "given": "Alex"
      },
      {
        "family": "Palermo",
        "given": "Joe"
      },
      {
        "family": "Pantuliano",
        "given": "Ashley"
      },
      {
        "family": "Parascandolo",
        "given": "Giambattista"
      },
      {
        "family": "Parish",
        "given": "Joel"
      },
      {
        "family": "Parparita",
        "given": "Emy"
      },
      {
        "family": "Passos",
        "given": "Alex"
      },
      {
        "family": "Pavlov",
        "given": "Mikhail"
      },
      {
        "family": "Peng",
        "given": "Andrew"
      },
      {
        "family": "Perelman",
        "given": "Adam"
      },
      {
        "family": "Peres",
        "given": "Filipe de Avila Belbute"
      },
      {
        "family": "Petrov",
        "given": "Michael"
      },
      {
        "family": "Pinto",
        "given": "Henrique Ponde de Oliveira"
      },
      {
        "family": "Michael"
      },
      {
        "literal": "Pokorny"
      },
      {
        "family": "Pokrass",
        "given": "Michelle"
      },
      {
        "family": "Pong",
        "given": "Vitchyr H."
      },
      {
        "family": "Powell",
        "given": "Tolly"
      },
      {
        "family": "Power",
        "given": "Alethea"
      },
      {
        "family": "Power",
        "given": "Boris"
      },
      {
        "family": "Proehl",
        "given": "Elizabeth"
      },
      {
        "family": "Puri",
        "given": "Raul"
      },
      {
        "family": "Radford",
        "given": "Alec"
      },
      {
        "family": "Rae",
        "given": "Jack"
      },
      {
        "family": "Ramesh",
        "given": "Aditya"
      },
      {
        "family": "Raymond",
        "given": "Cameron"
      },
      {
        "family": "Real",
        "given": "Francis"
      },
      {
        "family": "Rimbach",
        "given": "Kendra"
      },
      {
        "family": "Ross",
        "given": "Carl"
      },
      {
        "family": "Rotsted",
        "given": "Bob"
      },
      {
        "family": "Roussez",
        "given": "Henri"
      },
      {
        "family": "Ryder",
        "given": "Nick"
      },
      {
        "family": "Saltarelli",
        "given": "Mario"
      },
      {
        "family": "Sanders",
        "given": "Ted"
      },
      {
        "family": "Santurkar",
        "given": "Shibani"
      },
      {
        "family": "Sastry",
        "given": "Girish"
      },
      {
        "family": "Schmidt",
        "given": "Heather"
      },
      {
        "family": "Schnurr",
        "given": "David"
      },
      {
        "family": "Schulman",
        "given": "John"
      },
      {
        "family": "Selsam",
        "given": "Daniel"
      },
      {
        "family": "Sheppard",
        "given": "Kyla"
      },
      {
        "family": "Sherbakov",
        "given": "Toki"
      },
      {
        "family": "Shieh",
        "given": "Jessica"
      },
      {
        "family": "Shoker",
        "given": "Sarah"
      },
      {
        "family": "Shyam",
        "given": "Pranav"
      },
      {
        "family": "Sidor",
        "given": "Szymon"
      },
      {
        "family": "Sigler",
        "given": "Eric"
      },
      {
        "family": "Simens",
        "given": "Maddie"
      },
      {
        "family": "Sitkin",
        "given": "Jordan"
      },
      {
        "family": "Slama",
        "given": "Katarina"
      },
      {
        "family": "Sohl",
        "given": "Ian"
      },
      {
        "family": "Sokolowsky",
        "given": "Benjamin"
      },
      {
        "family": "Song",
        "given": "Yang"
      },
      {
        "family": "Staudacher",
        "given": "Natalie"
      },
      {
        "family": "Such",
        "given": "Felipe Petroski"
      },
      {
        "family": "Summers",
        "given": "Natalie"
      },
      {
        "family": "Sutskever",
        "given": "Ilya"
      },
      {
        "family": "Tang",
        "given": "Jie"
      },
      {
        "family": "Tezak",
        "given": "Nikolas"
      },
      {
        "family": "Thompson",
        "given": "Madeleine B."
      },
      {
        "family": "Tillet",
        "given": "Phil"
      },
      {
        "family": "Tootoonchian",
        "given": "Amin"
      },
      {
        "family": "Tseng",
        "given": "Elizabeth"
      },
      {
        "family": "Tuggle",
        "given": "Preston"
      },
      {
        "family": "Turley",
        "given": "Nick"
      },
      {
        "family": "Tworek",
        "given": "Jerry"
      },
      {
        "family": "Uribe",
        "given": "Juan Felipe Cerón"
      },
      {
        "family": "Vallone",
        "given": "Andrea"
      },
      {
        "family": "Vijayvergiya",
        "given": "Arun"
      },
      {
        "family": "Voss",
        "given": "Chelsea"
      },
      {
        "family": "Wainwright",
        "given": "Carroll"
      },
      {
        "family": "Wang",
        "given": "Justin Jay"
      },
      {
        "family": "Wang",
        "given": "Alvin"
      },
      {
        "family": "Wang",
        "given": "Ben"
      },
      {
        "family": "Ward",
        "given": "Jonathan"
      },
      {
        "family": "Wei",
        "given": "Jason"
      },
      {
        "family": "Weinmann",
        "given": "CJ"
      },
      {
        "family": "Welihinda",
        "given": "Akila"
      },
      {
        "family": "Welinder",
        "given": "Peter"
      },
      {
        "family": "Weng",
        "given": "Jiayi"
      },
      {
        "family": "Weng",
        "given": "Lilian"
      },
      {
        "family": "Wiethoff",
        "given": "Matt"
      },
      {
        "family": "Willner",
        "given": "Dave"
      },
      {
        "family": "Winter",
        "given": "Clemens"
      },
      {
        "family": "Wolrich",
        "given": "Samuel"
      },
      {
        "family": "Wong",
        "given": "Hannah"
      },
      {
        "family": "Workman",
        "given": "Lauren"
      },
      {
        "family": "Wu",
        "given": "Sherwin"
      },
      {
        "family": "Wu",
        "given": "Jeff"
      },
      {
        "family": "Wu",
        "given": "Michael"
      },
      {
        "family": "Xiao",
        "given": "Kai"
      },
      {
        "family": "Xu",
        "given": "Tao"
      },
      {
        "family": "Yoo",
        "given": "Sarah"
      },
      {
        "family": "Yu",
        "given": "Kevin"
      },
      {
        "family": "Yuan",
        "given": "Qiming"
      },
      {
        "family": "Zaremba",
        "given": "Wojciech"
      },
      {
        "family": "Zellers",
        "given": "Rowan"
      },
      {
        "family": "Zhang",
        "given": "Chong"
      },
      {
        "family": "Zhang",
        "given": "Marvin"
      },
      {
        "family": "Zhao",
        "given": "Shengjia"
      },
      {
        "family": "Zheng",
        "given": "Tianhao"
      },
      {
        "family": "Zhuang",
        "given": "Juntang"
      },
      {
        "family": "Zhuk",
        "given": "William"
      },
      {
        "family": "Zoph",
        "given": "Barret"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2023
        ]
      ]
    },
    "abstract": "We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.",
    "DOI": "10.48550/arxiv.2303.08774",
    "publisher": "arXiv",
    "title": "GPT-4 Technical Report",
    "URL": "https://doi.org/grx4cb",
    "version": "6",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2303.08774"
  },
  {
    "type": "article-journal",
    "id": "6ALrEKtt",
    "categories": [
      "Differential Geometry (math.DG)",
      "Analysis of PDEs (math.AP)",
      "FOS: Mathematics",
      "FOS: Mathematics",
      "53E10 53A04"
    ],
    "author": [
      {
        "family": "Khan",
        "given": "Gabriel"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2023
        ]
      ]
    },
    "abstract": "We study curve-shortening flow for twisted curves in $\\mathbb{R}^3$ (i.e., curves with nowhere vanishing curvature $κ$ and torsion $τ$) and define a notion of torsion-curvature entropy. Using this functional, we show that either the curve develops an inflection point or the eventual singularity is highly irregular (and likely impossible). In particular, it must be a Type II singularity which admits sequences along which $\\fracτ{κ^2} \\to \\infty$. This contrasts strongly with Altschuler's planarity theorem [J. Differential Geom. (1991)], which shows that along any essential blow-up sequence, $\\fracτκ \\to 0$.",
    "container-title": "arXiv",
    "DOI": "10.48550/arxiv.2305.07171",
    "publisher": "arXiv",
    "title": "Curvature-Torsion Entropy for Twisted Curves under Curve Shortening Flow",
    "URL": "https://doi.org/g95xv9",
    "version": "1",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2305.07171"
  },
  {
    "type": "article",
    "id": "10hcHcmAG",
    "categories": [
      "Computation and Language (cs.CL)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Patel",
        "given": "Ajay"
      },
      {
        "family": "Rao",
        "given": "Delip"
      },
      {
        "family": "Kothary",
        "given": "Ansh"
      },
      {
        "family": "McKeown",
        "given": "Kathleen"
      },
      {
        "family": "Callison-Burch",
        "given": "Chris"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2023
        ]
      ]
    },
    "abstract": "Style representation learning builds content-independent representations of author style in text. Stylometry, the analysis of style in text, is often performed by expert forensic linguists and no large dataset of stylometric annotations exists for training. Current style representation learning uses neural methods to disentangle style from content to create style vectors, however, these approaches result in uninterpretable representations, complicating their usage in downstream applications like authorship attribution where auditing and explainability is critical. In this work, we use prompting to perform stylometry on a large number of texts to create a synthetic dataset and train human-interpretable style representations we call LISA embeddings. We release our synthetic stylometry dataset and our interpretable style models as resources.",
    "DOI": "10.48550/arxiv.2305.12696",
    "publisher": "arXiv",
    "title": "Learning Interpretable Style Embeddings via Prompting LLMs",
    "URL": "https://doi.org/g9t22h",
    "version": "2",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2305.12696"
  },
  {
    "type": "article",
    "id": "zWwbz3cX",
    "categories": [
      "Computation and Language (cs.CL)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Shen",
        "given": "Sheng"
      },
      {
        "family": "Hou",
        "given": "Le"
      },
      {
        "family": "Zhou",
        "given": "Yanqi"
      },
      {
        "family": "Du",
        "given": "Nan"
      },
      {
        "family": "Longpre",
        "given": "Shayne"
      },
      {
        "family": "Wei",
        "given": "Jason"
      },
      {
        "family": "Chung",
        "given": "Hyung Won"
      },
      {
        "family": "Zoph",
        "given": "Barret"
      },
      {
        "family": "Fedus",
        "given": "William"
      },
      {
        "family": "Chen",
        "given": "Xinyun"
      },
      {
        "family": "Vu",
        "given": "Tu"
      },
      {
        "family": "Wu",
        "given": "Yuexin"
      },
      {
        "family": "Chen",
        "given": "Wuyang"
      },
      {
        "family": "Webson",
        "given": "Albert"
      },
      {
        "family": "Li",
        "given": "Yunxuan"
      },
      {
        "family": "Zhao",
        "given": "Vincent"
      },
      {
        "family": "Yu",
        "given": "Hongkun"
      },
      {
        "family": "Keutzer",
        "given": "Kurt"
      },
      {
        "family": "Darrell",
        "given": "Trevor"
      },
      {
        "family": "Zhou",
        "given": "Denny"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2023
        ]
      ]
    },
    "abstract": "Sparse Mixture-of-Experts (MoE) is a neural architecture design that can be utilized to add learnable parameters to Large Language Models (LLMs) without increasing inference cost. Instruction tuning is a technique for training LLMs to follow instructions. We advocate combining these two approaches, as we find that MoE models benefit more from instruction tuning than dense models. In particular, we conduct empirical studies across three experimental setups: (i) Direct finetuning on individual downstream tasks devoid of instruction tuning; (ii) Instructiontuning followed by in-context few-shot or zero-shot generalization on downstream tasks; and (iii) Instruction tuning supplemented by further finetuning on individual downstream tasks. In the first scenario, MoE models overall underperform dense models of identical computational capacity. This narrative, however, dramatically changes with the introduction of instruction tuning (second and third scenario), used independently or in conjunction with task-specific finetuning. Our most powerful model, FLAN-MOE-32B, surpasses the performance of FLAN-PALM-62B on four benchmark tasks, while using only a third of the FLOPs. The advancements embodied byFLAN-MOE inspire a reevaluation of the design principles of large-scale, high-performance language models in the framework of task-agnostic learning.",
    "DOI": "10.48550/arxiv.2305.14705",
    "publisher": "arXiv",
    "title": "Mixture-of-Experts Meets Instruction Tuning:A Winning Combination for Large Language Models",
    "URL": "https://doi.org/g9t22j",
    "version": "2",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2305.14705"
  },
  {
    "type": "article",
    "id": "iZG9n7mW",
    "categories": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Panwar",
        "given": "Madhur"
      },
      {
        "family": "Ahuja",
        "given": "Kabir"
      },
      {
        "family": "Goyal",
        "given": "Navin"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2023
        ]
      ]
    },
    "abstract": "In-context learning (ICL) is one of the surprising and useful features of large language models and subject of intense research. Recently, stylized meta-learning-like ICL setups have been devised that train transformers on sequences of input-output pairs $(x, f(x))$. The function $f$ comes from a function class and generalization is checked by evaluating on sequences generated from unseen functions from the same class. One of the main discoveries in this line of research has been that for several function classes, such as linear regression, transformers successfully generalize to new functions in the class. However, the inductive biases of these models resulting in this behavior are not clearly understood. A model with unlimited training data and compute is a Bayesian predictor: it learns the pretraining distribution. In this paper we empirically examine how far this Bayesian perspective can help us understand ICL. To this end, we generalize the previous meta-ICL setup to hierarchical meta-ICL setup which involve unions of multiple task families. We instantiate this setup on a diverse range of linear and nonlinear function families and find that transformers can do ICL in this setting as well. Where Bayesian inference is tractable, we find evidence that high-capacity transformers mimic the Bayesian predictor. The Bayesian perspective provides insights into the inductive bias of ICL and how transformers perform a particular task when they are trained on multiple tasks. We also find that transformers can learn to generalize to new function classes that were not seen during pretraining. This involves deviation from the Bayesian predictor. We examine these deviations in more depth offering new insights and hypotheses.",
    "DOI": "10.48550/arxiv.2306.04891",
    "publisher": "arXiv",
    "title": "In-Context Learning through the Bayesian Prism",
    "URL": "https://doi.org/g95xwb",
    "version": "2",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2306.04891"
  },
  {
    "type": "article",
    "id": "7UR8Jpoi",
    "categories": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Awais",
        "given": "Muhammad"
      },
      {
        "family": "Naseer",
        "given": "Muzammal"
      },
      {
        "family": "Khan",
        "given": "Salman"
      },
      {
        "family": "Anwer",
        "given": "Rao Muhammad"
      },
      {
        "family": "Cholakkal",
        "given": "Hisham"
      },
      {
        "family": "Shah",
        "given": "Mubarak"
      },
      {
        "family": "Yang",
        "given": "Ming-Hsuan"
      },
      {
        "family": "Khan",
        "given": "Fahad Shahbaz"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2023
        ]
      ]
    },
    "abstract": "Vision systems to see and reason about the compositional nature of visual scenes are fundamental to understanding our world. The complex relations between objects and their locations, ambiguities, and variations in the real-world environment can be better described in human language, naturally governed by grammatical rules and other modalities such as audio and depth. The models learned to bridge the gap between such modalities coupled with large-scale training data facilitate contextual reasoning, generalization, and prompt capabilities at test time. These models are referred to as foundational models. The output of such models can be modified through human-provided prompts without retraining, e.g., segmenting a particular object by providing a bounding box, having interactive dialogues by asking questions about an image or video scene or manipulating the robot's behavior through language instructions. In this survey, we provide a comprehensive review of such emerging foundational models, including typical architecture designs to combine different modalities (vision, text, audio, etc), training objectives (contrastive, generative), pre-training datasets, fine-tuning mechanisms, and the common prompting patterns; textual, visual, and heterogeneous. We discuss the open challenges and research directions for foundational models in computer vision, including difficulties in their evaluations and benchmarking, gaps in their real-world understanding, limitations of their contextual understanding, biases, vulnerability to adversarial attacks, and interpretability issues. We review recent developments in this field, covering a wide range of applications of foundation models systematically and comprehensively. A comprehensive list of foundational models studied in this work is available at \\url{https://github.com/awaisrauf/Awesome-CV-Foundational-Models}.",
    "DOI": "10.48550/arxiv.2307.13721",
    "publisher": "arXiv",
    "title": "Foundational Models Defining a New Era in Vision: A Survey and Outlook",
    "URL": "https://doi.org/g95zd3",
    "version": "1",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2307.13721"
  },
  {
    "type": "article",
    "id": "gy3ao6P6",
    "categories": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Optimization and Control (math.OC)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences",
      "FOS: Mathematics",
      "FOS: Mathematics"
    ],
    "author": [
      {
        "family": "Tarzanagh",
        "given": "Davoud Ataee"
      },
      {
        "family": "Li",
        "given": "Yingcong"
      },
      {
        "family": "Thrampoulidis",
        "given": "Christos"
      },
      {
        "family": "Oymak",
        "given": "Samet"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2023
        ]
      ]
    },
    "abstract": "Since its inception in \"Attention Is All You Need\", transformer architecture has led to revolutionary advancements in NLP. The attention layer within the transformer admits a sequence of input tokens $X$ and makes them interact through pairwise similarities computed as softmax$(XQK^\\top X^\\top)$, where $(K,Q)$ are the trainable key-query parameters. In this work, we establish a formal equivalence between the optimization geometry of self-attention and a hard-margin SVM problem that separates optimal input tokens from non-optimal tokens using linear constraints on the outer-products of token pairs. This formalism allows us to characterize the implicit bias of 1-layer transformers optimized with gradient descent: (1) Optimizing the attention layer with vanishing regularization, parameterized by $(K,Q)$, converges in direction to an SVM solution minimizing the nuclear norm of the combined parameter $W=KQ^\\top$. Instead, directly parameterizing by $W$ minimizes a Frobenius norm objective. We characterize this convergence, highlighting that it can occur toward locally-optimal directions rather than global ones. (2) Complementing this, we prove the local/global directional convergence of gradient descent under suitable geometric conditions. Importantly, we show that over-parameterization catalyzes global convergence by ensuring the feasibility of the SVM problem and by guaranteeing a benign optimization landscape devoid of stationary points. (3) While our theory applies primarily to linear prediction heads, we propose a more general SVM equivalence that predicts the implicit bias with nonlinear heads. Our findings are applicable to arbitrary datasets and their validity is verified via experiments. We also introduce several open problems and research directions. We believe these findings inspire the interpretation of transformers as a hierarchy of SVMs that separates and selects optimal tokens.",
    "DOI": "10.48550/arxiv.2308.16898",
    "publisher": "arXiv",
    "title": "Transformers as Support Vector Machines",
    "URL": "https://doi.org/g958z6",
    "version": "3",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2308.16898"
  },
  {
    "type": "article",
    "id": "D2dv1dGG",
    "categories": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Kim",
        "given": "Eungyeup"
      },
      {
        "family": "Sun",
        "given": "Mingjie"
      },
      {
        "family": "Baek",
        "given": "Christina"
      },
      {
        "family": "Raghunathan",
        "given": "Aditi"
      },
      {
        "family": "Kolter",
        "given": "J. Zico"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2023
        ]
      ]
    },
    "abstract": "Recently, Miller et al. (2021) and Baek et al. (2022) empirically demonstrated strong linear correlations between in-distribution (ID) versus out-of-distribution (OOD) accuracy and agreement. These trends, coined accuracy-on-the-line (ACL) and agreement-on-the-line (AGL), enable OOD model selection and performance estimation without labeled data. However, these phenomena also break for certain shifts, such as CIFAR10-C Gaussian Noise, posing a critical bottleneck. In this paper, we make a key finding that recent test-time adaptation (TTA) methods not only improve OOD performance, but drastically strengthen the ACL and AGL trends in models, even in shifts where models showed very weak correlations before. To analyze this, we revisit the theoretical conditions from Miller et al. (2021) that outline the types of distribution shifts needed for perfect ACL in linear models. Surprisingly, these conditions are satisfied after applying TTA to deep models in the penultimate feature embedding space. In particular, TTA causes the data distribution to collapse complex shifts into those can be expressed by a singular scaling variable in the feature space. Our results show that by combining TTA with AGL-based estimation methods, we can estimate the OOD performance of models with high precision for a broader set of distribution shifts. This lends us a simple system for selecting the best hyperparameters and adaptation strategy without any OOD labeled data.",
    "DOI": "10.48550/arxiv.2310.04941",
    "publisher": "arXiv",
    "title": "Test-Time Adaptation Induces Stronger Accuracy and Agreement-on-the-Line",
    "URL": "https://doi.org/g958z7",
    "version": "2",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2310.04941"
  },
  {
    "type": "article",
    "id": "RvAOKYai",
    "categories": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Kroeger",
        "given": "Nicholas"
      },
      {
        "family": "Ley",
        "given": "Dan"
      },
      {
        "family": "Krishna",
        "given": "Satyapriya"
      },
      {
        "family": "Agarwal",
        "given": "Chirag"
      },
      {
        "family": "Lakkaraju",
        "given": "Himabindu"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2023
        ]
      ]
    },
    "abstract": "Recent advancements in Large Language Models (LLMs) have demonstrated exceptional capabilities in complex tasks like machine translation, commonsense reasoning, and language understanding. One of the primary reasons for the adaptability of LLMs in such diverse tasks is their in-context learning (ICL) capability, which allows them to perform well on new tasks by simply using a few task samples in the prompt. Despite their effectiveness in enhancing the performance of LLMs on diverse language and tabular tasks, these methods have not been thoroughly explored for their potential to generate post hoc explanations. In this work, we carry out one of the first explorations to analyze the effectiveness of LLMs in explaining other complex predictive models using ICL. To this end, we propose a novel framework, In-Context Explainers, comprising of three novel approaches that exploit the ICL capabilities of LLMs to explain the predictions made by other predictive models. We conduct extensive analysis with these approaches on real-world tabular and text datasets and demonstrate that LLMs are capable of explaining other predictive models similar to state-of-the-art post hoc explainers, opening up promising avenues for future research into LLM-based post hoc explanations of complex predictive models.",
    "DOI": "10.48550/arxiv.2310.05797",
    "publisher": "arXiv",
    "title": "In-Context Explainers: Harnessing LLMs for Explaining Black Box Models",
    "URL": "https://doi.org/g95xwd",
    "version": "4",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2310.05797"
  },
  {
    "type": "article",
    "id": "nYipTPML",
    "categories": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Deuschel",
        "given": "Jannik"
      },
      {
        "family": "Ellington",
        "given": "Caleb N."
      },
      {
        "family": "Luo",
        "given": "Yingtao"
      },
      {
        "family": "Lengerich",
        "given": "Benjamin J."
      },
      {
        "family": "Friederich",
        "given": "Pascal"
      },
      {
        "family": "Xing",
        "given": "Eric P."
      }
    ],
    "issued": {
      "date-parts": [
        [
          2023
        ]
      ]
    },
    "abstract": "Interpretable policy learning seeks to estimate intelligible decision policies from observed actions; however, existing models force a tradeoff between accuracy and interpretability, limiting data-driven interpretations of human decision-making processes. Fundamentally, existing approaches are burdened by this tradeoff because they represent the underlying decision process as a universal policy, when in fact human decisions are dynamic and can change drastically under different contexts. Thus, we develop Contextualized Policy Recovery (CPR), which re-frames the problem of modeling complex decision processes as a multi-task learning problem, where each context poses a unique task and complex decision policies can be constructed piece-wise from many simple context-specific policies. CPR models each context-specific policy as a linear map, and generates new policy models $\\textit{on-demand}$ as contexts are updated with new observations. We provide two flavors of the CPR framework: one focusing on exact local interpretability, and one retaining full global interpretability. We assess CPR through studies on simulated and real data, achieving state-of-the-art performance on predicting antibiotic prescription in intensive care units ($+22\\%$ AUROC vs. previous SOTA) and predicting MRI prescription for Alzheimer's patients ($+7.7\\%$ AUROC vs. previous SOTA). With this improvement, CPR closes the accuracy gap between interpretable and black-box methods, allowing high-resolution exploration and analysis of context-specific decision models.",
    "DOI": "10.48550/arxiv.2310.07918",
    "publisher": "arXiv",
    "title": "Contextualized Policy Recovery: Modeling and Interpreting Medical Decisions with Adaptive Imitation Learning",
    "URL": "https://doi.org/gt68jf",
    "version": "4",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2310.07918"
  },
  {
    "type": "article",
    "id": "HYsEq2UQ",
    "categories": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Lengerich",
        "given": "Benjamin"
      },
      {
        "family": "Ellington",
        "given": "Caleb N."
      },
      {
        "family": "Rubbi",
        "given": "Andrea"
      },
      {
        "family": "Kellis",
        "given": "Manolis"
      },
      {
        "family": "Xing",
        "given": "Eric P."
      }
    ],
    "issued": {
      "date-parts": [
        [
          2023
        ]
      ]
    },
    "abstract": "We examine Contextualized Machine Learning (ML), a paradigm for learning heterogeneous and context-dependent effects. Contextualized ML estimates heterogeneous functions by applying deep learning to the meta-relationship between contextual information and context-specific parametric models. This is a form of varying-coefficient modeling that unifies existing frameworks including cluster analysis and cohort modeling by introducing two reusable concepts: a context encoder which translates sample context into model parameters, and sample-specific model which operates on sample predictors. We review the process of developing contextualized models, nonparametric inference from contextualized models, and identifiability conditions of contextualized models. Finally, we present the open-source PyTorch package ContextualizedML.",
    "DOI": "10.48550/arxiv.2310.11340",
    "publisher": "arXiv",
    "title": "Contextualized Machine Learning",
    "URL": "https://doi.org/gt68jg",
    "version": "1",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2310.11340"
  },
  {
    "type": "article",
    "id": "R3AtGoca",
    "categories": [
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences",
      "FOS: Electrical engineering, electronic engineering, information engineering",
      "FOS: Electrical engineering, electronic engineering, information engineering"
    ],
    "author": [
      {
        "family": "Sekeran",
        "given": "Maya"
      },
      {
        "family": "Syed",
        "given": "Arslan Ali"
      },
      {
        "family": "Lindner",
        "given": "Johannes"
      },
      {
        "family": "Margreiter",
        "given": "Martin"
      },
      {
        "family": "Bogenberger",
        "given": "Klaus"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2023
        ]
      ]
    },
    "abstract": "Lane-free traffic (LFT) is a new traffic system that relies on connected and automated vehicles (CAV) to increase road capacity and utilization by removing traditional lane markings using coordinated maneuvering of CAVs in LFT strategies. LFT is based on two main principles: upstream nudging and vehicles moving without adhering to any lane markings. By leveraging CAV capabilities to communicate and exchange information, LFT represents a promising future traffic system. While current research uses LFT simulations in two-dimensional space, driving simulators are necessary to investigate human behavior and perceived safety in LFT. This paper proposes a conceptual framework for LFT driving simulations and describes the assumptions, requirements, and recent technological developments that make it possible to investigate the human perspective and acceptance of LFT. Additionally, we propose a scenario matrix that can act as a test guide to building driving simulation scenarios for the LFT.",
    "DOI": "10.48550/arxiv.2311.16142",
    "publisher": "arXiv",
    "title": "Investigating Lane-Free Traffic with a Dynamic Driving Simulator",
    "URL": "https://doi.org/g93rnq",
    "version": "1",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2311.16142"
  },
  {
    "type": "article",
    "id": "9S6tI5yv",
    "categories": [
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Sristi",
        "given": "Ram Dyuthi"
      },
      {
        "family": "Lindenbaum",
        "given": "Ofir"
      },
      {
        "family": "Lifshitz",
        "given": "Shira"
      },
      {
        "family": "Lavzin",
        "given": "Maria"
      },
      {
        "family": "Schiller",
        "given": "Jackie"
      },
      {
        "family": "Mishne",
        "given": "Gal"
      },
      {
        "family": "Benisty",
        "given": "Hadas"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2023
        ]
      ]
    },
    "abstract": "Feature selection is a crucial tool in machine learning and is widely applied across various scientific disciplines. Traditional supervised methods generally identify a universal set of informative features for the entire population. However, feature relevance often varies with context, while the context itself may not directly affect the outcome variable. Here, we propose a novel architecture for contextual feature selection where the subset of selected features is conditioned on the value of context variables. Our new approach, Conditional Stochastic Gates (c-STG), models the importance of features using conditional Bernoulli variables whose parameters are predicted based on contextual variables. We introduce a hypernetwork that maps context variables to feature selection parameters to learn the context-dependent gates along with a prediction model. We further present a theoretical analysis of our model, indicating that it can improve performance and flexibility over population-level methods in complex feature selection settings. Finally, we conduct an extensive benchmark using simulated and real-world datasets across multiple domains demonstrating that c-STG can lead to improved feature selection capabilities while enhancing prediction accuracy and interpretability.",
    "DOI": "10.48550/arxiv.2312.14254",
    "publisher": "arXiv",
    "title": "Contextual Feature Selection with Conditional Stochastic Gates",
    "URL": "https://doi.org/gt68jh",
    "version": "2",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2312.14254"
  },
  {
    "type": "article",
    "id": "121VRWHxY",
    "categories": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Zakrisson",
        "given": "Henning"
      },
      {
        "family": "Lindholm",
        "given": "Mathias"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2024
        ]
      ]
    },
    "abstract": "The paper introduces a tree-based varying coefficient model (VCM) where the varying coefficients are modelled using the cyclic gradient boosting machine (CGBM) from Delong et al. (2023). Modelling the coefficient functions using a CGBM allows for dimension-wise early stopping and feature importance scores. The dimension-wise early stopping not only reduces the risk of dimension-specific overfitting, but also reveals differences in model complexity across dimensions. The use of feature importance scores allows for simple feature selection and easy model interpretation. The model is evaluated on the same simulated and real data examples as those used in Richman and Wüthrich (2023), and the results show that it produces results in terms of out of sample loss that are comparable to those of their neural network-based VCM called LocalGLMnet.",
    "DOI": "10.48550/arxiv.2401.05982",
    "publisher": "arXiv",
    "title": "A tree-based varying coefficient model",
    "URL": "https://doi.org/g958z8",
    "version": "3",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2401.05982"
  },
  {
    "type": "article",
    "id": "8C0DuyuD",
    "categories": [
      "Machine Learning (cs.LG)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Bordt",
        "given": "Sebastian"
      },
      {
        "family": "Raidl",
        "given": "Eric"
      },
      {
        "family": "von Luxburg",
        "given": "Ulrike"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2024
        ]
      ]
    },
    "abstract": "In the rapidly growing literature on explanation algorithms, it often remains unclear what precisely these algorithms are for and how they should be used. In this position paper, we argue for a novel and pragmatic perspective: Explainable machine learning needs to recognize its parallels with applied statistics. Concretely, explanations are statistics of high-dimensional functions, and we should think about them analogously to traditional statistical quantities. Among others, this implies that we must think carefully about the matter of interpretation, or how the explanations relate to intuitive questions that humans have about the world. The fact that this is scarcely being discussed in research papers is one of the main drawbacks of the current literature. Moving forward, the analogy between explainable machine learning and applied statistics suggests fruitful ways for how research practices can be improved.",
    "DOI": "10.48550/arxiv.2402.02870",
    "publisher": "arXiv",
    "title": "Rethinking Explainable Machine Learning as Applied Statistics",
    "URL": "https://doi.org/g958z9",
    "version": "5",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2402.02870"
  },
  {
    "type": "article",
    "id": "1AEMaqZx6",
    "categories": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Asadi",
        "given": "Nader"
      },
      {
        "family": "Beitollahi",
        "given": "Mahdi"
      },
      {
        "family": "Khalil",
        "given": "Yasser"
      },
      {
        "family": "Li",
        "given": "Yinchuan"
      },
      {
        "family": "Zhang",
        "given": "Guojun"
      },
      {
        "family": "Chen",
        "given": "Xi"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2024
        ]
      ]
    },
    "abstract": "Parameter-efficient fine-tuning stands as the standard for efficiently fine-tuning large language and vision models on downstream tasks. Specifically, the efficiency of low-rank adaptation has facilitated the creation and sharing of hundreds of custom LoRA modules, each trained on distinct data from various downstream tasks. In this paper, we explore the composability of LoRA modules, examining if combining these pre-trained modules enhances generalization to unseen downstream tasks. Our investigation involves evaluating two approaches: (a) uniform composition, involving averaging upstream LoRA modules with equal weights, and (b) learned composition, where we learn the weights for each upstream module and perform weighted averaging. Our experimental results on both vision and language models reveal that in few-shot settings, where only a limited number of samples are available for the downstream task, both uniform and learned composition methods result in better transfer accuracy; outperforming full fine-tuning and training a LoRA from scratch. Moreover, in full-shot settings, learned composition performs comparably to regular LoRA training with significantly fewer number of trainable parameters. Our research unveils the potential of uniform composition for enhancing transferability in low-shot settings, without introducing additional learnable parameters.",
    "DOI": "10.48550/arxiv.2402.15414",
    "publisher": "arXiv",
    "title": "Does Combining Parameter-efficient Modules Improve Few-shot Transfer Accuracy?",
    "URL": "https://doi.org/g95xwf",
    "version": "1",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2402.15414"
  },
  {
    "type": "article",
    "id": "1CQ8u4q3f",
    "categories": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Huang",
        "given": "Quzhe"
      },
      {
        "family": "An",
        "given": "Zhenwei"
      },
      {
        "family": "Zhuang",
        "given": "Nan"
      },
      {
        "family": "Tao",
        "given": "Mingxu"
      },
      {
        "family": "Zhang",
        "given": "Chen"
      },
      {
        "family": "Jin",
        "given": "Yang"
      },
      {
        "family": "Xu",
        "given": "Kun"
      },
      {
        "family": "Xu",
        "given": "Kun"
      },
      {
        "family": "Chen",
        "given": "Liwei"
      },
      {
        "family": "Huang",
        "given": "Songfang"
      },
      {
        "family": "Feng",
        "given": "Yansong"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2024
        ]
      ]
    },
    "abstract": "In this paper, we introduce a novel dynamic expert selection framework for Mixture of Experts (MoE) models, aiming to enhance computational efficiency and model performance by adjusting the number of activated experts based on input difficulty. Unlike traditional MoE approaches that rely on fixed Top-K routing, which activates a predetermined number of experts regardless of the input's complexity, our method dynamically selects experts based on the confidence level in expert selection for each input. This allows for a more efficient utilization of computational resources, activating more experts for complex tasks requiring advanced reasoning and fewer for simpler tasks. Through extensive evaluations, our dynamic routing method demonstrates substantial improvements over conventional Top-2 routing across various benchmarks, achieving an average improvement of 0.7% with less than 90% activated parameters. Further analysis shows our model dispatches more experts to tasks requiring complex reasoning skills, like BBH, confirming its ability to dynamically allocate computational resources in alignment with the input's complexity. Our findings also highlight a variation in the number of experts needed across different layers of the transformer model, offering insights into the potential for designing heterogeneous MoE frameworks. The code and models are available at https://github.com/ZhenweiAn/Dynamic_MoE.",
    "DOI": "10.48550/arxiv.2403.07652",
    "publisher": "arXiv",
    "title": "Harder Tasks Need More Experts: Dynamic Routing in MoE Models",
    "URL": "https://doi.org/g9582b",
    "version": "1",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2403.07652"
  },
  {
    "type": "article",
    "id": "UskQdlu3",
    "categories": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)",
      "Multimedia (cs.MM)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Wu",
        "given": "Xun"
      },
      {
        "family": "Huang",
        "given": "Shaohan"
      },
      {
        "family": "Wei",
        "given": "Furu"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2024
        ]
      ]
    },
    "abstract": "LoRA has gained widespread acceptance in the fine-tuning of large pre-trained models to cater to a diverse array of downstream tasks, showcasing notable effectiveness and efficiency, thereby solidifying its position as one of the most prevalent fine-tuning techniques. Due to the modular nature of LoRA's plug-and-play plugins, researchers have delved into the amalgamation of multiple LoRAs to empower models to excel across various downstream tasks. Nonetheless, extant approaches for LoRA fusion grapple with inherent challenges. Direct arithmetic merging may result in the loss of the original pre-trained model's generative capabilities or the distinct identity of LoRAs, thereby yielding suboptimal outcomes. On the other hand, Reference tuning-based fusion exhibits limitations concerning the requisite flexibility for the effective combination of multiple LoRAs. In response to these challenges, this paper introduces the Mixture of LoRA Experts (MoLE) approach, which harnesses hierarchical control and unfettered branch selection. The MoLE approach not only achieves superior LoRA fusion performance in comparison to direct arithmetic merging but also retains the crucial flexibility for combining LoRAs effectively. Extensive experimental evaluations conducted in both the Natural Language Processing (NLP) and Vision &amp; Language (V&amp;L) domains substantiate the efficacy of MoLE.",
    "DOI": "10.48550/arxiv.2404.13628",
    "publisher": "arXiv",
    "title": "Mixture of LoRA Experts",
    "URL": "https://doi.org/g93rnr",
    "version": "1",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2404.13628"
  },
  {
    "type": "article",
    "id": "UxVULYQ3",
    "categories": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Ostapenko",
        "given": "Oleksiy"
      },
      {
        "family": "Su",
        "given": "Zhan"
      },
      {
        "family": "Ponti",
        "given": "Edoardo Maria"
      },
      {
        "family": "Charlin",
        "given": "Laurent"
      },
      {
        "family": "Roux",
        "given": "Nicolas Le"
      },
      {
        "family": "Pereira",
        "given": "Matheus"
      },
      {
        "family": "Caccia",
        "given": "Lucas"
      },
      {
        "family": "Sordoni",
        "given": "Alessandro"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2024
        ]
      ]
    },
    "abstract": "The growing number of parameter-efficient adaptations of a base large language model (LLM) calls for studying whether we can reuse such trained adapters to improve performance for new tasks. We study how to best build a library of adapters given multi-task data and devise techniques for both zero-shot and supervised task generalization through routing in such library. We benchmark existing approaches to build this library and introduce model-based clustering, MBC, a method that groups tasks based on the similarity of their adapter parameters, indirectly optimizing for transfer across the multi-task dataset. To re-use the library, we present a novel zero-shot routing mechanism, Arrow, which enables dynamic selection of the most relevant adapters for new inputs without the need for retraining. We experiment with several LLMs, such as Phi-2 and Mistral, on a wide array of held-out tasks, verifying that MBC-based adapters and Arrow routing lead to superior generalization to new tasks. We make steps towards creating modular, adaptable LLMs that can match or outperform traditional joint training.",
    "DOI": "10.48550/arxiv.2405.11157",
    "publisher": "arXiv",
    "title": "Towards Modular LLMs by Building and Reusing a Library of LoRAs",
    "URL": "https://doi.org/g9582c",
    "version": "1",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2405.11157"
  },
  {
    "type": "article",
    "id": "Fto8UbOH",
    "categories": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Cai",
        "given": "Zekun"
      },
      {
        "family": "Bai",
        "given": "Guangji"
      },
      {
        "family": "Jiang",
        "given": "Renhe"
      },
      {
        "family": "Song",
        "given": "Xuan"
      },
      {
        "family": "Zhao",
        "given": "Liang"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2024
        ]
      ]
    },
    "abstract": "Temporal Domain Generalization (TDG) addresses the challenge of training predictive models under temporally varying data distributions. Traditional TDG approaches typically focus on domain data collected at fixed, discrete time intervals, which limits their capability to capture the inherent dynamics within continuous-evolving and irregularly-observed temporal domains. To overcome this, this work formalizes the concept of Continuous Temporal Domain Generalization (CTDG), where domain data are derived from continuous times and are collected at arbitrary times. CTDG tackles critical challenges including: 1) Characterizing the continuous dynamics of both data and models, 2) Learning complex high-dimensional nonlinear dynamics, and 3) Optimizing and controlling the generalization across continuous temporal domains. To address them, we propose a Koopman operator-driven continuous temporal domain generalization (Koodos) framework. We formulate the problem within a continuous dynamic system and leverage the Koopman theory to learn the underlying dynamics; the framework is further enhanced with a comprehensive optimization strategy equipped with analysis and control driven by prior knowledge of the dynamics patterns. Extensive experiments demonstrate the effectiveness and efficiency of our approach. The code can be found at: https://github.com/Zekun-Cai/Koodos.",
    "DOI": "10.48550/arxiv.2405.16075",
    "publisher": "arXiv",
    "title": "Continuous Temporal Domain Generalization",
    "URL": "https://doi.org/g9582d",
    "version": "2",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2405.16075"
  },
  {
    "type": "article",
    "id": "BLSm3phD",
    "categories": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Lo",
        "given": "Ka Man"
      },
      {
        "family": "Huang",
        "given": "Zeyu"
      },
      {
        "family": "Qiu",
        "given": "Zihan"
      },
      {
        "family": "Wang",
        "given": "Zili"
      },
      {
        "family": "Fu",
        "given": "Jie"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2024
        ]
      ]
    },
    "abstract": "Mixture-of-experts (MoE) is gaining increasing attention due to its unique properties and remarkable performance, especially for language tasks. By sparsely activating a subset of parameters for each token, MoE architecture could increase the model size without sacrificing computational efficiency, achieving a better trade-off between performance and training costs. However, the underlying mechanism of MoE still lacks further exploration, and its modularization degree remains questionable. In this paper, we make an initial attempt to understand the inner workings of MoE-based large language models. Concretely, we comprehensively study the parametric and behavioral features of three popular MoE-based models and reveal some intriguing observations, including 1) Neurons act like fine-grained experts; 2) The router of MoE usually selects experts with larger output norms; 3) The expert diversity increases as the layer increases, while the last layer is an outlier, which is further validated by an initial experiment. Based on the observations, we also provide suggestions for a broad spectrum of MoE practitioners, such as router design and expert allocation. We hope this work could shed light on future research on the MoE framework and other modular architectures. Code is available at https://github.com/kamanphoebe/Look-into-MoEs.",
    "DOI": "10.48550/arxiv.2406.18219",
    "publisher": "arXiv",
    "title": "A Closer Look into Mixture-of-Experts in Large Language Models",
    "URL": "https://doi.org/g9582f",
    "version": "3",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2406.18219"
  },
  {
    "type": "article",
    "id": "yWg7tQr1",
    "categories": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Grattafiori",
        "given": "Aaron"
      },
      {
        "family": "Dubey",
        "given": "Abhimanyu"
      },
      {
        "family": "Jauhri",
        "given": "Abhinav"
      },
      {
        "family": "Pandey",
        "given": "Abhinav"
      },
      {
        "family": "Kadian",
        "given": "Abhishek"
      },
      {
        "family": "Al-Dahle",
        "given": "Ahmad"
      },
      {
        "family": "Letman",
        "given": "Aiesha"
      },
      {
        "family": "Mathur",
        "given": "Akhil"
      },
      {
        "family": "Schelten",
        "given": "Alan"
      },
      {
        "family": "Vaughan",
        "given": "Alex"
      },
      {
        "family": "Yang",
        "given": "Amy"
      },
      {
        "family": "Fan",
        "given": "Angela"
      },
      {
        "family": "Goyal",
        "given": "Anirudh"
      },
      {
        "family": "Hartshorn",
        "given": "Anthony"
      },
      {
        "family": "Yang",
        "given": "Aobo"
      },
      {
        "family": "Mitra",
        "given": "Archi"
      },
      {
        "family": "Sravankumar",
        "given": "Archie"
      },
      {
        "family": "Korenev",
        "given": "Artem"
      },
      {
        "family": "Hinsvark",
        "given": "Arthur"
      },
      {
        "family": "Rao",
        "given": "Arun"
      },
      {
        "family": "Zhang",
        "given": "Aston"
      },
      {
        "family": "Rodriguez",
        "given": "Aurelien"
      },
      {
        "family": "Gregerson",
        "given": "Austen"
      },
      {
        "family": "Spataru",
        "given": "Ava"
      },
      {
        "family": "Roziere",
        "given": "Baptiste"
      },
      {
        "family": "Biron",
        "given": "Bethany"
      },
      {
        "family": "Tang",
        "given": "Binh"
      },
      {
        "family": "Chern",
        "given": "Bobbie"
      },
      {
        "family": "Caucheteux",
        "given": "Charlotte"
      },
      {
        "family": "Nayak",
        "given": "Chaya"
      },
      {
        "family": "Bi",
        "given": "Chloe"
      },
      {
        "family": "Marra",
        "given": "Chris"
      },
      {
        "family": "McConnell",
        "given": "Chris"
      },
      {
        "family": "Keller",
        "given": "Christian"
      },
      {
        "family": "Touret",
        "given": "Christophe"
      },
      {
        "family": "Wu",
        "given": "Chunyang"
      },
      {
        "family": "Wong",
        "given": "Corinne"
      },
      {
        "family": "Ferrer",
        "given": "Cristian Canton"
      },
      {
        "family": "Nikolaidis",
        "given": "Cyrus"
      },
      {
        "family": "Allonsius",
        "given": "Damien"
      },
      {
        "family": "Song",
        "given": "Daniel"
      },
      {
        "family": "Pintz",
        "given": "Danielle"
      },
      {
        "family": "Livshits",
        "given": "Danny"
      },
      {
        "family": "Wyatt",
        "given": "Danny"
      },
      {
        "family": "Esiobu",
        "given": "David"
      },
      {
        "family": "Choudhary",
        "given": "Dhruv"
      },
      {
        "family": "Mahajan",
        "given": "Dhruv"
      },
      {
        "family": "Garcia-Olano",
        "given": "Diego"
      },
      {
        "family": "Perino",
        "given": "Diego"
      },
      {
        "family": "Hupkes",
        "given": "Dieuwke"
      },
      {
        "family": "Lakomkin",
        "given": "Egor"
      },
      {
        "family": "AlBadawy",
        "given": "Ehab"
      },
      {
        "family": "Lobanova",
        "given": "Elina"
      },
      {
        "family": "Dinan",
        "given": "Emily"
      },
      {
        "family": "Smith",
        "given": "Eric Michael"
      },
      {
        "family": "Radenovic",
        "given": "Filip"
      },
      {
        "family": "Guzmán",
        "given": "Francisco"
      },
      {
        "family": "Zhang",
        "given": "Frank"
      },
      {
        "family": "Synnaeve",
        "given": "Gabriel"
      },
      {
        "family": "Lee",
        "given": "Gabrielle"
      },
      {
        "family": "Anderson",
        "given": "Georgia Lewis"
      },
      {
        "family": "Thattai",
        "given": "Govind"
      },
      {
        "family": "Nail",
        "given": "Graeme"
      },
      {
        "family": "Mialon",
        "given": "Gregoire"
      },
      {
        "family": "Pang",
        "given": "Guan"
      },
      {
        "family": "Cucurell",
        "given": "Guillem"
      },
      {
        "family": "Nguyen",
        "given": "Hailey"
      },
      {
        "family": "Korevaar",
        "given": "Hannah"
      },
      {
        "family": "Xu",
        "given": "Hu"
      },
      {
        "family": "Touvron",
        "given": "Hugo"
      },
      {
        "family": "Zarov",
        "given": "Iliyan"
      },
      {
        "family": "Ibarra",
        "given": "Imanol Arrieta"
      },
      {
        "family": "Kloumann",
        "given": "Isabel"
      },
      {
        "family": "Misra",
        "given": "Ishan"
      },
      {
        "family": "Evtimov",
        "given": "Ivan"
      },
      {
        "family": "Zhang",
        "given": "Jack"
      },
      {
        "family": "Copet",
        "given": "Jade"
      },
      {
        "family": "Lee",
        "given": "Jaewon"
      },
      {
        "family": "Geffert",
        "given": "Jan"
      },
      {
        "family": "Vranes",
        "given": "Jana"
      },
      {
        "family": "Park",
        "given": "Jason"
      },
      {
        "family": "Mahadeokar",
        "given": "Jay"
      },
      {
        "family": "Shah",
        "given": "Jeet"
      },
      {
        "family": "van der Linde",
        "given": "Jelmer"
      },
      {
        "family": "Billock",
        "given": "Jennifer"
      },
      {
        "family": "Hong",
        "given": "Jenny"
      },
      {
        "family": "Lee",
        "given": "Jenya"
      },
      {
        "family": "Fu",
        "given": "Jeremy"
      },
      {
        "family": "Chi",
        "given": "Jianfeng"
      },
      {
        "family": "Huang",
        "given": "Jianyu"
      },
      {
        "family": "Liu",
        "given": "Jiawen"
      },
      {
        "family": "Wang",
        "given": "Jie"
      },
      {
        "family": "Yu",
        "given": "Jiecao"
      },
      {
        "family": "Bitton",
        "given": "Joanna"
      },
      {
        "family": "Spisak",
        "given": "Joe"
      },
      {
        "family": "Park",
        "given": "Jongsoo"
      },
      {
        "family": "Rocca",
        "given": "Joseph"
      },
      {
        "family": "Johnstun",
        "given": "Joshua"
      },
      {
        "family": "Saxe",
        "given": "Joshua"
      },
      {
        "family": "Jia",
        "given": "Junteng"
      },
      {
        "family": "Alwala",
        "given": "Kalyan Vasuden"
      },
      {
        "family": "Prasad",
        "given": "Karthik"
      },
      {
        "family": "Upasani",
        "given": "Kartikeya"
      },
      {
        "family": "Plawiak",
        "given": "Kate"
      },
      {
        "family": "Li",
        "given": "Ke"
      },
      {
        "family": "Heafield",
        "given": "Kenneth"
      },
      {
        "family": "Stone",
        "given": "Kevin"
      },
      {
        "family": "El-Arini",
        "given": "Khalid"
      },
      {
        "family": "Iyer",
        "given": "Krithika"
      },
      {
        "family": "Malik",
        "given": "Kshitiz"
      },
      {
        "family": "Chiu",
        "given": "Kuenley"
      },
      {
        "family": "Bhalla",
        "given": "Kunal"
      },
      {
        "family": "Lakhotia",
        "given": "Kushal"
      },
      {
        "family": "Rantala-Yeary",
        "given": "Lauren"
      },
      {
        "family": "van der Maaten",
        "given": "Laurens"
      },
      {
        "family": "Chen",
        "given": "Lawrence"
      },
      {
        "family": "Tan",
        "given": "Liang"
      },
      {
        "family": "Jenkins",
        "given": "Liz"
      },
      {
        "family": "Martin",
        "given": "Louis"
      },
      {
        "family": "Madaan",
        "given": "Lovish"
      },
      {
        "family": "Malo",
        "given": "Lubo"
      },
      {
        "family": "Blecher",
        "given": "Lukas"
      },
      {
        "family": "Landzaat",
        "given": "Lukas"
      },
      {
        "family": "de Oliveira",
        "given": "Luke"
      },
      {
        "family": "Muzzi",
        "given": "Madeline"
      },
      {
        "family": "Pasupuleti",
        "given": "Mahesh"
      },
      {
        "family": "Singh",
        "given": "Mannat"
      },
      {
        "family": "Paluri",
        "given": "Manohar"
      },
      {
        "family": "Kardas",
        "given": "Marcin"
      },
      {
        "family": "Tsimpoukelli",
        "given": "Maria"
      },
      {
        "family": "Oldham",
        "given": "Mathew"
      },
      {
        "family": "Rita",
        "given": "Mathieu"
      },
      {
        "family": "Pavlova",
        "given": "Maya"
      },
      {
        "family": "Kambadur",
        "given": "Melanie"
      },
      {
        "family": "Lewis",
        "given": "Mike"
      },
      {
        "family": "Si",
        "given": "Min"
      },
      {
        "family": "Singh",
        "given": "Mitesh Kumar"
      },
      {
        "family": "Hassan",
        "given": "Mona"
      },
      {
        "family": "Goyal",
        "given": "Naman"
      },
      {
        "family": "Torabi",
        "given": "Narjes"
      },
      {
        "family": "Bashlykov",
        "given": "Nikolay"
      },
      {
        "family": "Bogoychev",
        "given": "Nikolay"
      },
      {
        "family": "Chatterji",
        "given": "Niladri"
      },
      {
        "family": "Zhang",
        "given": "Ning"
      },
      {
        "family": "Duchenne",
        "given": "Olivier"
      },
      {
        "family": "Çelebi",
        "given": "Onur"
      },
      {
        "family": "Alrassy",
        "given": "Patrick"
      },
      {
        "family": "Zhang",
        "given": "Pengchuan"
      },
      {
        "family": "Li",
        "given": "Pengwei"
      },
      {
        "family": "Vasic",
        "given": "Petar"
      },
      {
        "family": "Weng",
        "given": "Peter"
      },
      {
        "family": "Bhargava",
        "given": "Prajjwal"
      },
      {
        "family": "Dubal",
        "given": "Pratik"
      },
      {
        "family": "Krishnan",
        "given": "Praveen"
      },
      {
        "family": "Koura",
        "given": "Punit Singh"
      },
      {
        "family": "Xu",
        "given": "Puxin"
      },
      {
        "family": "He",
        "given": "Qing"
      },
      {
        "family": "Dong",
        "given": "Qingxiao"
      },
      {
        "family": "Srinivasan",
        "given": "Ragavan"
      },
      {
        "family": "Ganapathy",
        "given": "Raj"
      },
      {
        "family": "Calderer",
        "given": "Ramon"
      },
      {
        "family": "Cabral",
        "given": "Ricardo Silveira"
      },
      {
        "family": "Stojnic",
        "given": "Robert"
      },
      {
        "family": "Raileanu",
        "given": "Roberta"
      },
      {
        "family": "Maheswari",
        "given": "Rohan"
      },
      {
        "family": "Girdhar",
        "given": "Rohit"
      },
      {
        "family": "Patel",
        "given": "Rohit"
      },
      {
        "family": "Sauvestre",
        "given": "Romain"
      },
      {
        "family": "Polidoro",
        "given": "Ronnie"
      },
      {
        "family": "Sumbaly",
        "given": "Roshan"
      },
      {
        "family": "Taylor",
        "given": "Ross"
      },
      {
        "family": "Silva",
        "given": "Ruan"
      },
      {
        "family": "Hou",
        "given": "Rui"
      },
      {
        "family": "Wang",
        "given": "Rui"
      },
      {
        "family": "Hosseini",
        "given": "Saghar"
      },
      {
        "family": "Chennabasappa",
        "given": "Sahana"
      },
      {
        "family": "Singh",
        "given": "Sanjay"
      },
      {
        "family": "Bell",
        "given": "Sean"
      },
      {
        "family": "Kim",
        "given": "Seohyun Sonia"
      },
      {
        "family": "Edunov",
        "given": "Sergey"
      },
      {
        "family": "Nie",
        "given": "Shaoliang"
      },
      {
        "family": "Narang",
        "given": "Sharan"
      },
      {
        "family": "Raparthy",
        "given": "Sharath"
      },
      {
        "family": "Shen",
        "given": "Sheng"
      },
      {
        "family": "Wan",
        "given": "Shengye"
      },
      {
        "family": "Bhosale",
        "given": "Shruti"
      },
      {
        "family": "Zhang",
        "given": "Shun"
      },
      {
        "family": "Vandenhende",
        "given": "Simon"
      },
      {
        "family": "Batra",
        "given": "Soumya"
      },
      {
        "family": "Whitman",
        "given": "Spencer"
      },
      {
        "family": "Sootla",
        "given": "Sten"
      },
      {
        "family": "Collot",
        "given": "Stephane"
      },
      {
        "family": "Gururangan",
        "given": "Suchin"
      },
      {
        "family": "Borodinsky",
        "given": "Sydney"
      },
      {
        "family": "Herman",
        "given": "Tamar"
      },
      {
        "family": "Fowler",
        "given": "Tara"
      },
      {
        "family": "Sheasha",
        "given": "Tarek"
      },
      {
        "family": "Georgiou",
        "given": "Thomas"
      },
      {
        "family": "Scialom",
        "given": "Thomas"
      },
      {
        "family": "Speckbacher",
        "given": "Tobias"
      },
      {
        "family": "Mihaylov",
        "given": "Todor"
      },
      {
        "family": "Xiao",
        "given": "Tong"
      },
      {
        "family": "Karn",
        "given": "Ujjwal"
      },
      {
        "family": "Goswami",
        "given": "Vedanuj"
      },
      {
        "family": "Gupta",
        "given": "Vibhor"
      },
      {
        "family": "Ramanathan",
        "given": "Vignesh"
      },
      {
        "family": "Kerkez",
        "given": "Viktor"
      },
      {
        "family": "Gonguet",
        "given": "Vincent"
      },
      {
        "family": "Do",
        "given": "Virginie"
      },
      {
        "family": "Vogeti",
        "given": "Vish"
      },
      {
        "family": "Albiero",
        "given": "Vítor"
      },
      {
        "family": "Petrovic",
        "given": "Vladan"
      },
      {
        "family": "Chu",
        "given": "Weiwei"
      },
      {
        "family": "Xiong",
        "given": "Wenhan"
      },
      {
        "family": "Fu",
        "given": "Wenyin"
      },
      {
        "family": "Meers",
        "given": "Whitney"
      },
      {
        "family": "Martinet",
        "given": "Xavier"
      },
      {
        "family": "Wang",
        "given": "Xiaodong"
      },
      {
        "family": "Wang",
        "given": "Xiaofang"
      },
      {
        "family": "Tan",
        "given": "Xiaoqing Ellen"
      },
      {
        "family": "Xia",
        "given": "Xide"
      },
      {
        "family": "Xie",
        "given": "Xinfeng"
      },
      {
        "family": "Jia",
        "given": "Xuchao"
      },
      {
        "family": "Wang",
        "given": "Xuewei"
      },
      {
        "family": "Goldschlag",
        "given": "Yaelle"
      },
      {
        "family": "Gaur",
        "given": "Yashesh"
      },
      {
        "family": "Babaei",
        "given": "Yasmine"
      },
      {
        "family": "Wen",
        "given": "Yi"
      },
      {
        "family": "Song",
        "given": "Yiwen"
      },
      {
        "family": "Zhang",
        "given": "Yuchen"
      },
      {
        "family": "Li",
        "given": "Yue"
      },
      {
        "family": "Mao",
        "given": "Yuning"
      },
      {
        "family": "Coudert",
        "given": "Zacharie Delpierre"
      },
      {
        "family": "Yan",
        "given": "Zheng"
      },
      {
        "family": "Chen",
        "given": "Zhengxing"
      },
      {
        "family": "Papakipos",
        "given": "Zoe"
      },
      {
        "family": "Singh",
        "given": "Aaditya"
      },
      {
        "family": "Srivastava",
        "given": "Aayushi"
      },
      {
        "family": "Jain",
        "given": "Abha"
      },
      {
        "family": "Kelsey",
        "given": "Adam"
      },
      {
        "family": "Shajnfeld",
        "given": "Adam"
      },
      {
        "family": "Gangidi",
        "given": "Adithya"
      },
      {
        "family": "Victoria",
        "given": "Adolfo"
      },
      {
        "family": "Goldstand",
        "given": "Ahuva"
      },
      {
        "family": "Menon",
        "given": "Ajay"
      },
      {
        "family": "Sharma",
        "given": "Ajay"
      },
      {
        "family": "Boesenberg",
        "given": "Alex"
      },
      {
        "family": "Baevski",
        "given": "Alexei"
      },
      {
        "family": "Feinstein",
        "given": "Allie"
      },
      {
        "family": "Kallet",
        "given": "Amanda"
      },
      {
        "family": "Sangani",
        "given": "Amit"
      },
      {
        "family": "Teo",
        "given": "Amos"
      },
      {
        "family": "Yunus",
        "given": "Anam"
      },
      {
        "family": "Lupu",
        "given": "Andrei"
      },
      {
        "family": "Alvarado",
        "given": "Andres"
      },
      {
        "family": "Caples",
        "given": "Andrew"
      },
      {
        "family": "Gu",
        "given": "Andrew"
      },
      {
        "family": "Ho",
        "given": "Andrew"
      },
      {
        "family": "Poulton",
        "given": "Andrew"
      },
      {
        "family": "Ryan",
        "given": "Andrew"
      },
      {
        "family": "Ramchandani",
        "given": "Ankit"
      },
      {
        "family": "Dong",
        "given": "Annie"
      },
      {
        "family": "Franco",
        "given": "Annie"
      },
      {
        "family": "Goyal",
        "given": "Anuj"
      },
      {
        "family": "Saraf",
        "given": "Aparajita"
      },
      {
        "family": "Chowdhury",
        "given": "Arkabandhu"
      },
      {
        "family": "Gabriel",
        "given": "Ashley"
      },
      {
        "family": "Bharambe",
        "given": "Ashwin"
      },
      {
        "family": "Eisenman",
        "given": "Assaf"
      },
      {
        "family": "Yazdan",
        "given": "Azadeh"
      },
      {
        "family": "James",
        "given": "Beau"
      },
      {
        "family": "Maurer",
        "given": "Ben"
      },
      {
        "family": "Leonhardi",
        "given": "Benjamin"
      },
      {
        "family": "Huang",
        "given": "Bernie"
      },
      {
        "family": "Loyd",
        "given": "Beth"
      },
      {
        "family": "De Paola",
        "given": "Beto"
      },
      {
        "family": "Paranjape",
        "given": "Bhargavi"
      },
      {
        "family": "Liu",
        "given": "Bing"
      },
      {
        "family": "Wu",
        "given": "Bo"
      },
      {
        "family": "Ni",
        "given": "Boyu"
      },
      {
        "family": "Hancock",
        "given": "Braden"
      },
      {
        "family": "Wasti",
        "given": "Bram"
      },
      {
        "family": "Spence",
        "given": "Brandon"
      },
      {
        "family": "Stojkovic",
        "given": "Brani"
      },
      {
        "family": "Gamido",
        "given": "Brian"
      },
      {
        "family": "Montalvo",
        "given": "Britt"
      },
      {
        "family": "Parker",
        "given": "Carl"
      },
      {
        "family": "Burton",
        "given": "Carly"
      },
      {
        "family": "Mejia",
        "given": "Catalina"
      },
      {
        "family": "Liu",
        "given": "Ce"
      },
      {
        "family": "Wang",
        "given": "Changhan"
      },
      {
        "family": "Kim",
        "given": "Changkyu"
      },
      {
        "family": "Zhou",
        "given": "Chao"
      },
      {
        "family": "Hu",
        "given": "Chester"
      },
      {
        "family": "Chu",
        "given": "Ching-Hsiang"
      },
      {
        "family": "Cai",
        "given": "Chris"
      },
      {
        "family": "Tindal",
        "given": "Chris"
      },
      {
        "family": "Feichtenhofer",
        "given": "Christoph"
      },
      {
        "family": "Gao",
        "given": "Cynthia"
      },
      {
        "family": "Civin",
        "given": "Damon"
      },
      {
        "family": "Beaty",
        "given": "Dana"
      },
      {
        "family": "Kreymer",
        "given": "Daniel"
      },
      {
        "family": "Li",
        "given": "Daniel"
      },
      {
        "family": "Adkins",
        "given": "David"
      },
      {
        "family": "Xu",
        "given": "David"
      },
      {
        "family": "Testuggine",
        "given": "Davide"
      },
      {
        "family": "David",
        "given": "Delia"
      },
      {
        "family": "Parikh",
        "given": "Devi"
      },
      {
        "family": "Liskovich",
        "given": "Diana"
      },
      {
        "family": "Foss",
        "given": "Didem"
      },
      {
        "family": "Wang",
        "given": "Dingkang"
      },
      {
        "family": "Le",
        "given": "Duc"
      },
      {
        "family": "Holland",
        "given": "Dustin"
      },
      {
        "family": "Dowling",
        "given": "Edward"
      },
      {
        "family": "Jamil",
        "given": "Eissa"
      },
      {
        "family": "Montgomery",
        "given": "Elaine"
      },
      {
        "family": "Presani",
        "given": "Eleonora"
      },
      {
        "family": "Hahn",
        "given": "Emily"
      },
      {
        "family": "Wood",
        "given": "Emily"
      },
      {
        "family": "Le",
        "given": "Eric-Tuan"
      },
      {
        "family": "Brinkman",
        "given": "Erik"
      },
      {
        "family": "Arcaute",
        "given": "Esteban"
      },
      {
        "family": "Dunbar",
        "given": "Evan"
      },
      {
        "family": "Smothers",
        "given": "Evan"
      },
      {
        "family": "Sun",
        "given": "Fei"
      },
      {
        "family": "Kreuk",
        "given": "Felix"
      },
      {
        "family": "Tian",
        "given": "Feng"
      },
      {
        "family": "Kokkinos",
        "given": "Filippos"
      },
      {
        "family": "Ozgenel",
        "given": "Firat"
      },
      {
        "family": "Caggioni",
        "given": "Francesco"
      },
      {
        "family": "Kanayet",
        "given": "Frank"
      },
      {
        "family": "Seide",
        "given": "Frank"
      },
      {
        "family": "Florez",
        "given": "Gabriela Medina"
      },
      {
        "family": "Schwarz",
        "given": "Gabriella"
      },
      {
        "family": "Badeer",
        "given": "Gada"
      },
      {
        "family": "Swee",
        "given": "Georgia"
      },
      {
        "family": "Halpern",
        "given": "Gil"
      },
      {
        "family": "Herman",
        "given": "Grant"
      },
      {
        "family": "Sizov",
        "given": "Grigory"
      },
      {
        "family": "Guangyi"
      },
      {
        "family": "Zhang"
      },
      {
        "family": "Lakshminarayanan",
        "given": "Guna"
      },
      {
        "family": "Inan",
        "given": "Hakan"
      },
      {
        "family": "Shojanazeri",
        "given": "Hamid"
      },
      {
        "family": "Zou",
        "given": "Han"
      },
      {
        "family": "Wang",
        "given": "Hannah"
      },
      {
        "family": "Zha",
        "given": "Hanwen"
      },
      {
        "family": "Habeeb",
        "given": "Haroun"
      },
      {
        "family": "Rudolph",
        "given": "Harrison"
      },
      {
        "family": "Suk",
        "given": "Helen"
      },
      {
        "family": "Aspegren",
        "given": "Henry"
      },
      {
        "family": "Goldman",
        "given": "Hunter"
      },
      {
        "family": "Zhan",
        "given": "Hongyuan"
      },
      {
        "family": "Damlaj",
        "given": "Ibrahim"
      },
      {
        "family": "Molybog",
        "given": "Igor"
      },
      {
        "family": "Tufanov",
        "given": "Igor"
      },
      {
        "family": "Leontiadis",
        "given": "Ilias"
      },
      {
        "family": "Veliche",
        "given": "Irina-Elena"
      },
      {
        "family": "Gat",
        "given": "Itai"
      },
      {
        "family": "Weissman",
        "given": "Jake"
      },
      {
        "family": "Geboski",
        "given": "James"
      },
      {
        "family": "Kohli",
        "given": "James"
      },
      {
        "family": "Lam",
        "given": "Janice"
      },
      {
        "family": "Asher",
        "given": "Japhet"
      },
      {
        "family": "Gaya",
        "given": "Jean-Baptiste"
      },
      {
        "family": "Marcus",
        "given": "Jeff"
      },
      {
        "family": "Tang",
        "given": "Jeff"
      },
      {
        "family": "Chan",
        "given": "Jennifer"
      },
      {
        "family": "Zhen",
        "given": "Jenny"
      },
      {
        "family": "Reizenstein",
        "given": "Jeremy"
      },
      {
        "family": "Teboul",
        "given": "Jeremy"
      },
      {
        "family": "Zhong",
        "given": "Jessica"
      },
      {
        "family": "Jin",
        "given": "Jian"
      },
      {
        "family": "Yang",
        "given": "Jingyi"
      },
      {
        "family": "Cummings",
        "given": "Joe"
      },
      {
        "family": "Carvill",
        "given": "Jon"
      },
      {
        "family": "Shepard",
        "given": "Jon"
      },
      {
        "family": "McPhie",
        "given": "Jonathan"
      },
      {
        "family": "Torres",
        "given": "Jonathan"
      },
      {
        "family": "Ginsburg",
        "given": "Josh"
      },
      {
        "family": "Wang",
        "given": "Junjie"
      },
      {
        "family": "Wu",
        "given": "Kai"
      },
      {
        "family": "U",
        "given": "Kam Hou"
      },
      {
        "family": "Saxena",
        "given": "Karan"
      },
      {
        "family": "Khandelwal",
        "given": "Kartikay"
      },
      {
        "family": "Zand",
        "given": "Katayoun"
      },
      {
        "family": "Matosich",
        "given": "Kathy"
      },
      {
        "family": "Veeraraghavan",
        "given": "Kaushik"
      },
      {
        "family": "Michelena",
        "given": "Kelly"
      },
      {
        "family": "Li",
        "given": "Keqian"
      },
      {
        "family": "Jagadeesh",
        "given": "Kiran"
      },
      {
        "family": "Huang",
        "given": "Kun"
      },
      {
        "family": "Chawla",
        "given": "Kunal"
      },
      {
        "family": "Huang",
        "given": "Kyle"
      },
      {
        "family": "Chen",
        "given": "Lailin"
      },
      {
        "family": "Garg",
        "given": "Lakshya"
      },
      {
        "family": "A",
        "given": "Lavender"
      },
      {
        "family": "Silva",
        "given": "Leandro"
      },
      {
        "family": "Bell",
        "given": "Lee"
      },
      {
        "family": "Zhang",
        "given": "Lei"
      },
      {
        "family": "Guo",
        "given": "Liangpeng"
      },
      {
        "family": "Yu",
        "given": "Licheng"
      },
      {
        "family": "Moshkovich",
        "given": "Liron"
      },
      {
        "family": "Wehrstedt",
        "given": "Luca"
      },
      {
        "family": "Khabsa",
        "given": "Madian"
      },
      {
        "family": "Avalani",
        "given": "Manav"
      },
      {
        "family": "Bhatt",
        "given": "Manish"
      },
      {
        "family": "Mankus",
        "given": "Martynas"
      },
      {
        "family": "Hasson",
        "given": "Matan"
      },
      {
        "family": "Lennie",
        "given": "Matthew"
      },
      {
        "family": "Reso",
        "given": "Matthias"
      },
      {
        "family": "Groshev",
        "given": "Maxim"
      },
      {
        "family": "Naumov",
        "given": "Maxim"
      },
      {
        "family": "Lathi",
        "given": "Maya"
      },
      {
        "family": "Keneally",
        "given": "Meghan"
      },
      {
        "family": "Liu",
        "given": "Miao"
      },
      {
        "family": "Seltzer",
        "given": "Michael L."
      },
      {
        "family": "Valko",
        "given": "Michal"
      },
      {
        "family": "Restrepo",
        "given": "Michelle"
      },
      {
        "family": "Patel",
        "given": "Mihir"
      },
      {
        "family": "Vyatskov",
        "given": "Mik"
      },
      {
        "family": "Samvelyan",
        "given": "Mikayel"
      },
      {
        "family": "Clark",
        "given": "Mike"
      },
      {
        "family": "Macey",
        "given": "Mike"
      },
      {
        "family": "Wang",
        "given": "Mike"
      },
      {
        "family": "Hermoso",
        "given": "Miquel Jubert"
      },
      {
        "family": "Metanat",
        "given": "Mo"
      },
      {
        "family": "Rastegari",
        "given": "Mohammad"
      },
      {
        "family": "Bansal",
        "given": "Munish"
      },
      {
        "family": "Santhanam",
        "given": "Nandhini"
      },
      {
        "family": "Parks",
        "given": "Natascha"
      },
      {
        "family": "White",
        "given": "Natasha"
      },
      {
        "family": "Bawa",
        "given": "Navyata"
      },
      {
        "family": "Singhal",
        "given": "Nayan"
      },
      {
        "family": "Egebo",
        "given": "Nick"
      },
      {
        "family": "Usunier",
        "given": "Nicolas"
      },
      {
        "family": "Mehta",
        "given": "Nikhil"
      },
      {
        "family": "Laptev",
        "given": "Nikolay Pavlovich"
      },
      {
        "family": "Dong",
        "given": "Ning"
      },
      {
        "family": "Cheng",
        "given": "Norman"
      },
      {
        "family": "Chernoguz",
        "given": "Oleg"
      },
      {
        "family": "Hart",
        "given": "Olivia"
      },
      {
        "family": "Salpekar",
        "given": "Omkar"
      },
      {
        "family": "Kalinli",
        "given": "Ozlem"
      },
      {
        "family": "Kent",
        "given": "Parkin"
      },
      {
        "family": "Parekh",
        "given": "Parth"
      },
      {
        "family": "Saab",
        "given": "Paul"
      },
      {
        "family": "Balaji",
        "given": "Pavan"
      },
      {
        "family": "Rittner",
        "given": "Pedro"
      },
      {
        "family": "Bontrager",
        "given": "Philip"
      },
      {
        "family": "Roux",
        "given": "Pierre"
      },
      {
        "family": "Dollar",
        "given": "Piotr"
      },
      {
        "family": "Zvyagina",
        "given": "Polina"
      },
      {
        "family": "Ratanchandani",
        "given": "Prashant"
      },
      {
        "family": "Yuvraj",
        "given": "Pritish"
      },
      {
        "family": "Liang",
        "given": "Qian"
      },
      {
        "family": "Alao",
        "given": "Rachad"
      },
      {
        "family": "Rodriguez",
        "given": "Rachel"
      },
      {
        "family": "Ayub",
        "given": "Rafi"
      },
      {
        "family": "Murthy",
        "given": "Raghotham"
      },
      {
        "family": "Nayani",
        "given": "Raghu"
      },
      {
        "family": "Mitra",
        "given": "Rahul"
      },
      {
        "family": "Parthasarathy",
        "given": "Rangaprabhu"
      },
      {
        "family": "Li",
        "given": "Raymond"
      },
      {
        "family": "Hogan",
        "given": "Rebekkah"
      },
      {
        "family": "Battey",
        "given": "Robin"
      },
      {
        "family": "Wang",
        "given": "Rocky"
      },
      {
        "family": "Howes",
        "given": "Russ"
      },
      {
        "family": "Rinott",
        "given": "Ruty"
      },
      {
        "family": "Mehta",
        "given": "Sachin"
      },
      {
        "family": "Siby",
        "given": "Sachin"
      },
      {
        "family": "Bondu",
        "given": "Sai Jayesh"
      },
      {
        "family": "Datta",
        "given": "Samyak"
      },
      {
        "family": "Chugh",
        "given": "Sara"
      },
      {
        "family": "Hunt",
        "given": "Sara"
      },
      {
        "family": "Dhillon",
        "given": "Sargun"
      },
      {
        "family": "Sidorov",
        "given": "Sasha"
      },
      {
        "family": "Pan",
        "given": "Satadru"
      },
      {
        "family": "Mahajan",
        "given": "Saurabh"
      },
      {
        "family": "Verma",
        "given": "Saurabh"
      },
      {
        "family": "Yamamoto",
        "given": "Seiji"
      },
      {
        "family": "Ramaswamy",
        "given": "Sharadh"
      },
      {
        "family": "Lindsay",
        "given": "Shaun"
      },
      {
        "family": "Lindsay",
        "given": "Shaun"
      },
      {
        "family": "Feng",
        "given": "Sheng"
      },
      {
        "family": "Lin",
        "given": "Shenghao"
      },
      {
        "family": "Zha",
        "given": "Shengxin Cindy"
      },
      {
        "family": "Patil",
        "given": "Shishir"
      },
      {
        "family": "Shankar",
        "given": "Shiva"
      },
      {
        "family": "Zhang",
        "given": "Shuqiang"
      },
      {
        "family": "Zhang",
        "given": "Shuqiang"
      },
      {
        "family": "Wang",
        "given": "Sinong"
      },
      {
        "family": "Agarwal",
        "given": "Sneha"
      },
      {
        "family": "Sajuyigbe",
        "given": "Soji"
      },
      {
        "family": "Chintala",
        "given": "Soumith"
      },
      {
        "family": "Max",
        "given": "Stephanie"
      },
      {
        "family": "Chen",
        "given": "Stephen"
      },
      {
        "family": "Kehoe",
        "given": "Steve"
      },
      {
        "family": "Satterfield",
        "given": "Steve"
      },
      {
        "family": "Govindaprasad",
        "given": "Sudarshan"
      },
      {
        "family": "Gupta",
        "given": "Sumit"
      },
      {
        "family": "Deng",
        "given": "Summer"
      },
      {
        "family": "Cho",
        "given": "Sungmin"
      },
      {
        "family": "Virk",
        "given": "Sunny"
      },
      {
        "family": "Subramanian",
        "given": "Suraj"
      },
      {
        "family": "Choudhury",
        "given": "Sy"
      },
      {
        "family": "Goldman",
        "given": "Sydney"
      },
      {
        "family": "Remez",
        "given": "Tal"
      },
      {
        "family": "Glaser",
        "given": "Tamar"
      },
      {
        "family": "Best",
        "given": "Tamara"
      },
      {
        "family": "Koehler",
        "given": "Thilo"
      },
      {
        "family": "Robinson",
        "given": "Thomas"
      },
      {
        "family": "Li",
        "given": "Tianhe"
      },
      {
        "family": "Zhang",
        "given": "Tianjun"
      },
      {
        "family": "Matthews",
        "given": "Tim"
      },
      {
        "family": "Chou",
        "given": "Timothy"
      },
      {
        "family": "Shaked",
        "given": "Tzook"
      },
      {
        "family": "Vontimitta",
        "given": "Varun"
      },
      {
        "family": "Ajayi",
        "given": "Victoria"
      },
      {
        "family": "Montanez",
        "given": "Victoria"
      },
      {
        "family": "Mohan",
        "given": "Vijai"
      },
      {
        "family": "Kumar",
        "given": "Vinay Satish"
      },
      {
        "family": "Mangla",
        "given": "Vishal"
      },
      {
        "family": "Ionescu",
        "given": "Vlad"
      },
      {
        "family": "Poenaru",
        "given": "Vlad"
      },
      {
        "family": "Mihailescu",
        "given": "Vlad Tiberiu"
      },
      {
        "family": "Ivanov",
        "given": "Vladimir"
      },
      {
        "family": "Li",
        "given": "Wei"
      },
      {
        "family": "Wang",
        "given": "Wenchen"
      },
      {
        "family": "Jiang",
        "given": "Wenwen"
      },
      {
        "family": "Bouaziz",
        "given": "Wes"
      },
      {
        "family": "Constable",
        "given": "Will"
      },
      {
        "family": "Tang",
        "given": "Xiaocheng"
      },
      {
        "family": "Wu",
        "given": "Xiaojian"
      },
      {
        "family": "Wang",
        "given": "Xiaolan"
      },
      {
        "family": "Wu",
        "given": "Xilun"
      },
      {
        "family": "Gao",
        "given": "Xinbo"
      },
      {
        "family": "Kleinman",
        "given": "Yaniv"
      },
      {
        "family": "Chen",
        "given": "Yanjun"
      },
      {
        "family": "Hu",
        "given": "Ye"
      },
      {
        "family": "Jia",
        "given": "Ye"
      },
      {
        "family": "Qi",
        "given": "Ye"
      },
      {
        "family": "Li",
        "given": "Yenda"
      },
      {
        "family": "Zhang",
        "given": "Yilin"
      },
      {
        "family": "Zhang",
        "given": "Ying"
      },
      {
        "family": "Adi",
        "given": "Yossi"
      },
      {
        "family": "Nam",
        "given": "Youngjin"
      },
      {
        "family": "Yu"
      },
      {
        "literal": "Wang"
      },
      {
        "family": "Zhao",
        "given": "Yu"
      },
      {
        "family": "Hao",
        "given": "Yuchen"
      },
      {
        "family": "Qian",
        "given": "Yundi"
      },
      {
        "family": "Li",
        "given": "Yunlu"
      },
      {
        "family": "He",
        "given": "Yuzi"
      },
      {
        "family": "Rait",
        "given": "Zach"
      },
      {
        "family": "DeVito",
        "given": "Zachary"
      },
      {
        "family": "Rosnbrick",
        "given": "Zef"
      },
      {
        "family": "Wen",
        "given": "Zhaoduo"
      },
      {
        "family": "Yang",
        "given": "Zhenyu"
      },
      {
        "family": "Zhao",
        "given": "Zhiwei"
      },
      {
        "family": "Ma",
        "given": "Zhiyu"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2024
        ]
      ]
    },
    "abstract": "Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.",
    "DOI": "10.48550/arxiv.2407.21783",
    "publisher": "arXiv",
    "title": "The Llama 3 Herd of Models",
    "URL": "https://doi.org/ndw6",
    "version": "3",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2407.21783"
  },
  {
    "type": "article",
    "id": "Q3DSEqgH",
    "categories": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Wu",
        "given": "Renjie"
      },
      {
        "family": "Wang",
        "given": "Hu"
      },
      {
        "family": "Chen",
        "given": "Hsiang-Ting"
      },
      {
        "family": "Carneiro",
        "given": "Gustavo"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2024
        ]
      ]
    },
    "abstract": "During multimodal model training and testing, certain data modalities may be absent due to sensor limitations, cost constraints, privacy concerns, or data loss, negatively affecting performance. Multimodal learning techniques designed to handle missing modalities can mitigate this by ensuring model robustness even when some modalities are unavailable. This survey reviews recent progress in Multimodal Learning with Missing Modality (MLMM), focusing on deep learning methods. It provides the first comprehensive survey that covers the motivation and distinctions between MLMM and standard multimodal learning setups, followed by a detailed analysis of current methods, applications, and datasets, concluding with challenges and future directions.",
    "DOI": "10.48550/arxiv.2409.07825",
    "publisher": "arXiv",
    "title": "Deep Multimodal Learning with Missing Modality: A Survey",
    "URL": "https://doi.org/g9582g",
    "version": "3",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2409.07825"
  },
  {
    "type": "article",
    "id": "4aDIdh6z",
    "categories": [
      "Econometrics (econ.EM)",
      "FOS: Economics and business",
      "FOS: Economics and business"
    ],
    "author": [
      {
        "family": "Zhang",
        "given": "Wei"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2024
        ]
      ]
    },
    "abstract": "We introduce a class of Bayesian matrix dynamic factor models that accommodates time-varying volatility, outliers, and cross-sectional correlation in the idiosyncratic components. For model comparison, we employ an importance-sampling estimator of the marginal likelihood based on the cross-entropy method to determine: (1) the optimal dimension of the factor matrix; (2) whether a vector- or matrix-valued structure is more suitable; and (3) whether an approximate or exact factor model is favored by the data. Through a series of Monte Carlo experiments, we demonstrate the accuracy of the factor estimates and the effectiveness of the marginal likelihood estimator in correctly identifying the true model. Applications to macroeconomic and financial datasets illustrate the model's ability to capture key features in matrix-valued time series.",
    "DOI": "10.48550/arxiv.2409.08354",
    "publisher": "arXiv",
    "title": "Bayesian Dynamic Factor Models for High-dimensional Matrix-valued Time Series",
    "URL": "https://doi.org/g96dmm",
    "version": "3",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2409.08354"
  },
  {
    "type": "article",
    "id": "XpHq6HEw",
    "categories": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Zhong",
        "given": "Ruiqi"
      },
      {
        "family": "Wang",
        "given": "Heng"
      },
      {
        "family": "Klein",
        "given": "Dan"
      },
      {
        "family": "Steinhardt",
        "given": "Jacob"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2024
        ]
      ]
    },
    "abstract": "To make sense of massive data, we often fit simplified models and then interpret the parameters; for example, we cluster the text embeddings and then interpret the mean parameters of each cluster. However, these parameters are often high-dimensional and hard to interpret. To make model parameters directly interpretable, we introduce a family of statistical models -- including clustering, time series, and classification models -- parameterized by natural language predicates. For example, a cluster of text about COVID could be parameterized by the predicate \"discusses COVID\". To learn these statistical models effectively, we develop a model-agnostic algorithm that optimizes continuous relaxations of predicate parameters with gradient descent and discretizes them by prompting language models (LMs). Finally, we apply our framework to a wide range of problems: taxonomizing user chat dialogues, characterizing how they evolve across time, finding categories where one language model is better than the other, clustering math problems based on subareas, and explaining visual features in memorable images. Our framework is highly versatile, applicable to both textual and visual domains, can be easily steered to focus on specific properties (e.g. subareas), and explains sophisticated concepts that classical methods (e.g. n-gram analysis) struggle to produce.",
    "DOI": "10.48550/arxiv.2409.08466",
    "publisher": "arXiv",
    "title": "Explaining Datasets in Words: Statistical Models with Natural Language Parameters",
    "URL": "https://doi.org/g9t22k",
    "version": "2",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2409.08466"
  },
  {
    "type": "article",
    "id": "FpwH3vaM",
    "categories": [
      "Machine Learning (cs.LG)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Ambekar",
        "given": "Sameer"
      },
      {
        "family": "Schnabel",
        "given": "Julia A."
      },
      {
        "family": "Bercea",
        "given": "Cosmin I."
      }
    ],
    "issued": {
      "date-parts": [
        [
          2024
        ]
      ]
    },
    "abstract": "Deep learning models in medical imaging often encounter challenges when adapting to new clinical settings unseen during training. Test-time adaptation offers a promising approach to optimize models for these unseen domains, yet its application in anomaly detection (AD) remains largely unexplored. AD aims to efficiently identify deviations from normative distributions; however, full adaptation, including pathological shifts, may inadvertently learn the anomalies it intends to detect. We introduce a novel concept of selective test-time adaptation that utilizes the inherent characteristics of deep pre-trained features to adapt selectively in a zero-shot manner to any test image from an unseen domain. This approach employs a model-agnostic, lightweight multi-layer perceptron for neural implicit representations, enabling the adaptation of outputs from any reconstruction-based AD method without altering the source-trained model. Rigorous validation in brain AD demonstrated that our strategy substantially enhances detection accuracy for multiple conditions and different target distributions. Specifically, our method improves the detection rates by up to 78% for enlarged ventricles and 24% for edemas.",
    "DOI": "10.48550/arxiv.2410.03306",
    "publisher": "arXiv",
    "title": "Selective Test-Time Adaptation for Unsupervised Anomaly Detection using Neural Implicit Representations",
    "URL": "https://doi.org/g9582h",
    "version": "2",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2410.03306"
  },
  {
    "type": "article",
    "id": "3EfUtiJg",
    "categories": [
      "Methodology (stat.ME)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Murakami",
        "given": "Daisuke"
      },
      {
        "family": "Shirota",
        "given": "Shinichiro"
      },
      {
        "family": "Kajita",
        "given": "Seiji"
      },
      {
        "family": "Kajita",
        "given": "Mami"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2024
        ]
      ]
    },
    "abstract": "Spatially and temporally varying coefficient (STVC) models are currently attracting attention as a flexible tool to explore the spatio-temporal patterns in regression coefficients. However, these models often struggle with balancing computational efficiency and model flexibility. To address this challenge, this study develops a fast and flexible method for STVC modeling. For enhanced flexibility in modeling, we assume multiple processes in each varying coefficient, including purely spatial, purely temporal, and spatio-temporal interaction processes with or without time cyclicity. While considering multiple processes can be time consuming, we combine a pre-conditioning method with a model selection procedure, inspired by reluctant interaction modeling. This approach allows us to computationally efficiently select and specify the latent space-time structure. Monte Carlo experiments demonstrate that the proposed method outperforms alternatives in terms of coefficient estimation accuracy and computational efficiency. Finally, we apply the proposed method to crime analysis using a sample size of 279,360, confirming that the proposed method provides reasonable estimates of varying coefficients. The STVC model is implemented in an R package spmoran.",
    "DOI": "10.48550/arxiv.2410.07229",
    "publisher": "arXiv",
    "title": "Fast spatio-temporally varying coefficient modeling with reluctant interaction selection",
    "URL": "https://doi.org/g9582j",
    "version": "2",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2410.07229"
  },
  {
    "type": "article",
    "id": "18QzJqDj4",
    "categories": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Schafhalter",
        "given": "Peter"
      },
      {
        "family": "Liao",
        "given": "Shun"
      },
      {
        "family": "Zhou",
        "given": "Yanqi"
      },
      {
        "family": "Yeh",
        "given": "Chih-Kuan"
      },
      {
        "family": "Kandoor",
        "given": "Arun"
      },
      {
        "family": "Laudon",
        "given": "James"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2024
        ]
      ]
    },
    "abstract": "Domain-specific adaptation is critical to maximizing the performance of pre-trained language models (PLMs) on one or multiple targeted tasks, especially under resource-constrained use cases, such as edge devices. However, existing methods often struggle to balance domain-specific performance, retention of general knowledge, and efficiency for training and inference. To address these challenges, we propose Modular Domain Experts (MoDE). MoDE is a mixture-of-experts architecture that augments a general PLMs with modular, domain-specialized experts. These experts are trained independently and composed together via a lightweight training process. In contrast to standard low-rank adaptation methods, each MoDE expert consists of several transformer layers which scale better with more training examples and larger parameter counts. Our evaluation demonstrates that MoDE achieves comparable target performances to full parameter fine-tuning while achieving 1.65% better retention performance. Moreover, MoDE's architecture enables flexible sharding configurations and improves training speeds by up to 38% over state-of-the-art distributed training configurations.",
    "DOI": "10.48550/arxiv.2410.10181",
    "publisher": "arXiv",
    "title": "Scalable Multi-Domain Adaptation of Language Models using Modular Experts",
    "URL": "https://doi.org/g9582k",
    "version": "2",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2410.10181"
  },
  {
    "type": "article",
    "id": "18PGQ3fOX",
    "categories": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Formal Languages and Automata Theory (cs.FL)",
      "Machine Learning (cs.LG)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences",
      "I.2.7"
    ],
    "author": [
      {
        "family": "Arora",
        "given": "Aryaman"
      },
      {
        "family": "Jurafsky",
        "given": "Dan"
      },
      {
        "family": "Potts",
        "given": "Christopher"
      },
      {
        "family": "Goodman",
        "given": "Noah D."
      }
    ],
    "issued": {
      "date-parts": [
        [
          2024
        ]
      ]
    },
    "abstract": "In-context learning (ICL) is a powerful technique for getting language models to perform complex tasks with no training updates. Prior work has established strong correlations between the number of in-context examples provided and the accuracy of the model's predictions. In this paper, we seek to explain this correlation by showing that ICL approximates a Bayesian learner. This perspective gives rise to a novel Bayesian scaling law for ICL. In experiments with \\mbox{GPT-2} models of different sizes, our scaling law matches existing scaling laws in accuracy while also offering interpretable terms for task priors, learning efficiency, and per-example probabilities. To illustrate the analytic power that such interpretable scaling laws provide, we report on controlled synthetic dataset experiments designed to inform real-world studies of safety alignment. In our experimental protocol, we use SFT or DPO to suppress an unwanted existing model capability and then use ICL to try to bring that capability back (many-shot jailbreaking). We then study ICL on real-world instruction-tuned LLMs using capabilities benchmarks as well as a new many-shot jailbreaking dataset. In all cases, Bayesian scaling laws accurately predict the conditions under which ICL will cause suppressed behaviors to reemerge, which sheds light on the ineffectiveness of post-training at increasing LLM safety.",
    "DOI": "10.48550/arxiv.2410.16531",
    "publisher": "arXiv",
    "title": "Bayesian scaling laws for in-context learning",
    "URL": "https://doi.org/g9582m",
    "version": "4",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2410.16531"
  },
  {
    "type": "article",
    "id": "2Asz98yx",
    "categories": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Chen",
        "given": "Liang"
      },
      {
        "family": "Zhang",
        "given": "Yong"
      },
      {
        "family": "Song",
        "given": "Yibing"
      },
      {
        "family": "Shen",
        "given": "Zhiqiang"
      },
      {
        "family": "Liu",
        "given": "Lingqiao"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2024
        ]
      ]
    },
    "abstract": "Domain generalization (DG) methods aim to maintain good performance in an unseen target domain by using training data from multiple source domains. While success on certain occasions are observed, enhancing the baseline across most scenarios remains challenging. This work introduces a simple yet effective framework, dubbed learning from multiple experts (LFME), that aims to make the target model an expert in all source domains to improve DG. Specifically, besides learning the target model used in inference, LFME will also train multiple experts specialized in different domains, whose output probabilities provide professional guidance by simply regularizing the logit of the target model. Delving deep into the framework, we reveal that the introduced logit regularization term implicitly provides effects of enabling the target model to harness more information, and mining hard samples from the experts during training. Extensive experiments on benchmarks from different DG tasks demonstrate that LFME is consistently beneficial to the baseline and can achieve comparable performance to existing arts. Code is available at~\\url{https://github.com/liangchen527/LFME}.",
    "DOI": "10.48550/arxiv.2410.17020",
    "publisher": "arXiv",
    "title": "LFME: A Simple Framework for Learning from Multiple Experts in Domain Generalization",
    "URL": "https://doi.org/g9582n",
    "version": "2",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2410.17020"
  },
  {
    "type": "article",
    "id": "1BOhEiiMt",
    "categories": [
      "Machine Learning (cs.LG)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Reuter",
        "given": "Arik"
      },
      {
        "family": "Rudner",
        "given": "Tim G. J."
      },
      {
        "family": "Fortuin",
        "given": "Vincent"
      },
      {
        "family": "Rügamer",
        "given": "David"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025
        ]
      ]
    },
    "abstract": "Transformers have emerged as the dominant architecture in the field of deep learning, with a broad range of applications and remarkable in-context learning (ICL) capabilities. While not yet fully understood, ICL has already proved to be an intriguing phenomenon, allowing transformers to learn in context -- without requiring further training. In this paper, we further advance the understanding of ICL by demonstrating that transformers can perform full Bayesian inference for commonly used statistical models in context. More specifically, we introduce a general framework that builds on ideas from prior fitted networks and continuous normalizing flows and enables us to infer complex posterior distributions for models such as generalized linear models and latent factor models. Extensive experiments on real-world datasets demonstrate that our ICL approach yields posterior samples that are similar in quality to state-of-the-art MCMC or variational inference methods that do not operate in context. The source code for this paper is available at https://github.com/ArikReuter/ICL_for_Full_Bayesian_Inference.",
    "DOI": "10.48550/arxiv.2501.16825",
    "publisher": "arXiv",
    "title": "Can Transformers Learn Full Bayesian Inference in Context?",
    "URL": "https://doi.org/g9582p",
    "version": "2",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2501.16825"
  },
  {
    "type": "article",
    "id": "1DBdzlHLI",
    "categories": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Qu",
        "given": "Jingang"
      },
      {
        "family": "Holzmüller",
        "given": "David"
      },
      {
        "family": "Varoquaux",
        "given": "Gaël"
      },
      {
        "family": "Morvan",
        "given": "Marine Le"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025
        ]
      ]
    },
    "abstract": "The long-standing dominance of gradient-boosted decision trees on tabular data is currently challenged by tabular foundation models using In-Context Learning (ICL): setting the training data as context for the test data and predicting in a single forward pass without parameter updates. While TabPFNv2 foundation model excels on tables with up to 10K samples, its alternating column- and row-wise attentions make handling large training sets computationally prohibitive. So, can ICL be effectively scaled and deliver a benefit for larger tables? We introduce TabICL, a tabular foundation model for classification, pretrained on synthetic datasets with up to 60K samples and capable of handling 500K samples on affordable resources. This is enabled by a novel two-stage architecture: a column-then-row attention mechanism to build fixed-dimensional embeddings of rows, followed by a transformer for efficient ICL. Across 200 classification datasets from the TALENT benchmark, TabICL is on par with TabPFNv2 while being systematically faster (up to 10 times), and significantly outperforms all other approaches. On 53 datasets with over 10K samples, TabICL surpasses both TabPFNv2 and CatBoost, demonstrating the potential of ICL for large data. Pretraining code, inference code, and pre-trained models are available at https://github.com/soda-inria/tabicl.",
    "DOI": "10.48550/arxiv.2502.05564",
    "publisher": "arXiv",
    "title": "TabICL: A Tabular Foundation Model for In-Context Learning on Large Data",
    "URL": "https://doi.org/g96dmn",
    "version": "2",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2502.05564"
  },
  {
    "type": "article",
    "id": "LUKqKQYa",
    "categories": [
      "Applications (stat.AP)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Englert",
        "given": "Jacob"
      },
      {
        "family": "Chang",
        "given": "Howard"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025
        ]
      ]
    },
    "abstract": "Recent studies of associations between environmental exposures and health outcomes have shifted toward estimating the effect of simultaneous exposure to multiple chemicals. Summary index methods, such as the weighted quantile sum and quantile g-computation, are now commonly used to analyze environmental exposure mixtures in a broad range of applications. These methods provide a simple and interpretable framework for quantifying mixture effects. However, when data arise from a large geographical study region, it may be unreasonable to expect a common mixture effect. In this work, we explore the use of a recently developed spatially varying coefficient model based on Bayesian additive regression trees to estimate spatially heterogeneous mixture effects using quantile g-computation. We conducted simulation studies to evaluate the method's performance. We then applied this model to an analysis of multiple ambient air pollutants and birthweight in Georgia, USA from 2005-2016. We find evidence of county-level spatially varying mixture associations, where for 17 of 159 counties in Georgia, elevated concentrations of a mixture of PM2.5, nitrogen dioxide, sulfur dioxide, ozone, and carbon monoxide were associated with a reduction in birthweight by as much as -16.65 grams (95% credible interval: -33.93, -0.40) per decile increase in all five air pollutants.",
    "DOI": "10.48550/arxiv.2502.14651",
    "publisher": "arXiv",
    "title": "Spatially Varying Coefficient Models for Estimating Heterogeneous Mixture Effects",
    "URL": "https://doi.org/g9582q",
    "version": "1",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2502.14651"
  },
  {
    "type": "article",
    "id": "1BlaGxmbh",
    "categories": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Zhai",
        "given": "Runtian"
      },
      {
        "family": "Yang",
        "given": "Kai"
      },
      {
        "family": "Tsai",
        "given": "Che-Ping"
      },
      {
        "family": "Varici",
        "given": "Burak"
      },
      {
        "family": "Kolter",
        "given": "Zico"
      },
      {
        "family": "Ravikumar",
        "given": "Pradeep"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025
        ]
      ]
    },
    "abstract": "Despite the empirical success of foundation models, we do not have a systematic characterization of the representations that these models learn. In this paper, we establish the contexture theory. It shows that a large class of representation learning methods can be characterized as learning from the association between the input and a context variable. Specifically, we show that many popular methods aim to approximate the top-d singular functions of the expectation operator induced by the context, in which case we say that the representation learns the contexture. We demonstrate the generality of the contexture theory by proving that representation learning within various learning paradigms -- supervised, self-supervised, and manifold learning -- can all be studied from such a perspective. We also prove that the representations that learn the contexture are optimal on those tasks that are compatible with the context. One important implication of the contexture theory is that once the model is large enough to approximate the top singular functions, further scaling up the model size yields diminishing returns. Therefore, scaling is not all we need, and further improvement requires better contexts. To this end, we study how to evaluate the usefulness of a context without knowing the downstream tasks. We propose a metric and show by experiments that it correlates well with the actual performance of the encoder on many real datasets.",
    "DOI": "10.48550/arxiv.2505.01557",
    "publisher": "arXiv",
    "title": "Contextures: Representations from Contexts",
    "URL": "https://doi.org/g977nj",
    "version": "1",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2505.01557"
  },
  {
    "type": "article",
    "id": "Hhj8Woby",
    "categories": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Wurgaft",
        "given": "Daniel"
      },
      {
        "family": "Lubana",
        "given": "Ekdeep Singh"
      },
      {
        "family": "Park",
        "given": "Core Francisco"
      },
      {
        "family": "Tanaka",
        "given": "Hidenori"
      },
      {
        "family": "Reddy",
        "given": "Gautam"
      },
      {
        "family": "Goodman",
        "given": "Noah D."
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025
        ]
      ]
    },
    "abstract": "Recent work analyzing in-context learning (ICL) has identified a broad set of strategies that describe model behavior in different experimental conditions. We aim to unify these findings by asking why a model learns these disparate strategies in the first place. Specifically, we start with the observation that when trained to learn a mixture of tasks, as is popular in the literature, the strategies learned by a model for performing ICL can be captured by a family of Bayesian predictors: a memorizing predictor, which assumes a discrete prior on the set of seen tasks, and a generalizing predictor, where the prior matches the underlying task distribution. Adopting the normative lens of rational analysis, where a learner's behavior is explained as an optimal adaptation to data given computational constraints, we develop a hierarchical Bayesian framework that almost perfectly predicts Transformer next-token predictions throughout training -- without assuming access to its weights. Under this framework, pretraining is viewed as a process of updating the posterior probability of different strategies, and inference-time behavior as a posterior-weighted average over these strategies' predictions. Our framework draws on common assumptions about neural network learning dynamics, which make explicit a tradeoff between loss and complexity among candidate strategies: beyond how well it explains the data, a model's preference towards implementing a strategy is dictated by its complexity. This helps explain well-known ICL phenomena, while offering novel predictions: e.g., we show a superlinear trend in the timescale for transitioning from generalization to memorization as task diversity increases. Overall, our work advances an explanatory and predictive account of ICL grounded in tradeoffs between strategy loss and complexity.",
    "DOI": "10.48550/arxiv.2506.17859",
    "publisher": "arXiv",
    "title": "In-Context Learning Strategies Emerge Rationally",
    "URL": "https://doi.org/g96dmp",
    "version": "2",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2506.17859"
  },
  {
    "type": "article",
    "id": "IncFoA7e",
    "categories": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Dherin",
        "given": "Benoit"
      },
      {
        "family": "Munn",
        "given": "Michael"
      },
      {
        "family": "Mazzawi",
        "given": "Hanna"
      },
      {
        "family": "Wunder",
        "given": "Michael"
      },
      {
        "family": "Gonzalvo",
        "given": "Javier"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025
        ]
      ]
    },
    "abstract": "One of the most striking features of Large Language Models (LLMs) is their ability to learn in-context. Namely at inference time an LLM is able to learn new patterns without any additional weight update when these patterns are presented in the form of examples in the prompt, even if these patterns were not seen during training. The mechanisms through which this can happen are still largely unknown. In this work, we show that the stacking of a self-attention layer with an MLP, allows the transformer block to implicitly modify the weights of the MLP layer according to the context. We argue through theory and experimentation that this simple mechanism may be the reason why LLMs can learn in-context and not only during training. Specifically, we show how a transformer block implicitly transforms a context into a low-rank weight-update of its MLP layer.",
    "DOI": "10.48550/arxiv.2507.16003",
    "publisher": "arXiv",
    "title": "Learning without training: The implicit dynamics of in-context learning",
    "URL": "https://doi.org/g96dmq",
    "version": "3",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2507.16003"
  },
  {
    "type": "article",
    "id": "1Ft0Gvajp",
    "categories": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "FOS: Computer and information sciences",
      "FOS: Computer and information sciences"
    ],
    "author": [
      {
        "family": "Zhang",
        "given": "Qizheng"
      },
      {
        "family": "Hu",
        "given": "Changran"
      },
      {
        "family": "Upasani",
        "given": "Shubhangi"
      },
      {
        "family": "Ma",
        "given": "Boyuan"
      },
      {
        "family": "Hong",
        "given": "Fenglu"
      },
      {
        "family": "Kamanuru",
        "given": "Vamsidhar"
      },
      {
        "family": "Rainton",
        "given": "Jay"
      },
      {
        "family": "Wu",
        "given": "Chen"
      },
      {
        "family": "Ji",
        "given": "Mengmeng"
      },
      {
        "family": "Li",
        "given": "Hanchen"
      },
      {
        "family": "Thakker",
        "given": "Urmish"
      },
      {
        "family": "Zou",
        "given": "James"
      },
      {
        "family": "Olukotun",
        "given": "Kunle"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2025
        ]
      ]
    },
    "abstract": "Large language model (LLM) applications such as agents and domain-specific reasoning increasingly rely on context adaptation -- modifying inputs with instructions, strategies, or evidence, rather than weight updates. Prior approaches improve usability but often suffer from brevity bias, which drops domain insights for concise summaries, and from context collapse, where iterative rewriting erodes details over time. Building on the adaptive memory introduced by Dynamic Cheatsheet, we introduce ACE (Agentic Context Engineering), a framework that treats contexts as evolving playbooks that accumulate, refine, and organize strategies through a modular process of generation, reflection, and curation. ACE prevents collapse with structured, incremental updates that preserve detailed knowledge and scale with long-context models. Across agent and domain-specific benchmarks, ACE optimizes contexts both offline (e.g., system prompts) and online (e.g., agent memory), consistently outperforming strong baselines: +10.6% on agents and +8.6% on finance, while significantly reducing adaptation latency and rollout cost. Notably, ACE could adapt effectively without labeled supervision and instead by leveraging natural execution feedback. On the AppWorld leaderboard, ACE matches the top-ranked production-level agent on the overall average and surpasses it on the harder test-challenge split, despite using a smaller open-source model. These results show that comprehensive, evolving contexts enable scalable, efficient, and self-improving LLM systems with low overhead.",
    "DOI": "10.48550/arxiv.2510.04618",
    "publisher": "arXiv",
    "title": "Agentic Context Engineering: Evolving Contexts for Self-Improving Language Models",
    "URL": "https://doi.org/g96dmr",
    "version": "1",
    "note": "This CSL Item was generated by Manubot v0.6.1 from its persistent identifier (standard_id).\nstandard_id: doi:10.48550/arxiv.2510.04618"
  },
  {
    "id": "12CgohhqK",
    "type": "paper-conference",
    "title": "A Framework for the Quantitative Evaluation of Disentangled Representations",
    "author": [
      {
        "family": "Eastwood",
        "given": "Cian"
      },
      {
        "family": "Williams",
        "given": "Christopher K. I."
      }
    ],
    "container-title": "International Conference on Learning Representations",
    "issued": {
      "date-parts": [
        [
          2018
        ]
      ]
    },
    "URL": "https://openreview.net/forum?id=By-7dz-AZ",
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: eastwood2018a"
  },
  {
    "id": "INMMN2Vz",
    "type": "paper-conference",
    "title": "beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework",
    "author": [
      {
        "family": "Higgins",
        "given": "Irina"
      },
      {
        "family": "Matthey",
        "given": "Loic"
      },
      {
        "family": "Pal",
        "given": "Arka"
      },
      {
        "family": "Burgess",
        "given": "Christopher"
      },
      {
        "family": "Glorot",
        "given": "Xavier"
      },
      {
        "family": "Botvinick",
        "given": "Matthew"
      },
      {
        "family": "Mohamed",
        "given": "Shakir"
      },
      {
        "family": "Lerchner",
        "given": "Alexander"
      }
    ],
    "container-title": "International Conference on Learning Representations",
    "issued": {
      "date-parts": [
        [
          2017
        ]
      ]
    },
    "URL": "https://openreview.net/forum?id=Sy2fzU9gl",
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: higgins2017betavae"
  },
  {
    "id": "15RY4QKp3",
    "type": "paper-conference",
    "author": [
      {
        "family": "Vapnik",
        "given": "V."
      }
    ],
    "title": "Principles of risk minimization for learning theory",
    "issued": {
      "date-parts": [
        [
          1991
        ]
      ]
    },
    "publisher": "Morgan Kaufmann Publishers Inc.",
    "publisher-place": "San Francisco, CA, USA",
    "collection-title": "Proceedings of the 5th International Conference on Neural Information Processing Systems",
    "event-place": "Denver, Colorado",
    "page": "831–838",
    "number-of-pages": "8",
    "ISBN": "1558602224",
    "URL": "https://dl.acm.org/doi/10.5555/2986916.2987018",
    "abstract": "Learning is posed as a problem of function estimation, for which two principles of solution are considered: empirical risk minimization and structural risk minimization. These two principles are applied to two different statements of the function estimation problem: global and local. Systematic improvements in prediction power are illustrated in application to zip-code recognition.",
    "container-title": "Advances in Neural Information Processing Systems 5 (NIPS'91)",
    "note": "Loaded from an external bibliography file by Manubot.\nsource_bibliography: manual-references.json\nstandard_id: vapnik1991principles"
  }
]
