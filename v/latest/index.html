<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US" xml:lang="en-US">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Jingyun Jia" />
  <meta name="author" content="Zhiyuan Li" />
  <meta name="author" content="Ben Lengerich" />
  <meta name="dcterms.date" content="2026-03-01" />
  <meta name="keywords" content="markdown, publishing, manubot" />
  <title>Do Biomedical Tasks Require Biomedical Foundation Models?</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
    }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
  <!--
  Manubot generated metadata rendered from header-includes-template.html.
  Suggest improvements at https://github.com/manubot/manubot/blob/main/manubot/process/header-includes-template.html
  -->
  <meta name="dc.format" content="text/html" />
  <meta property="og:type" content="article" />
  <meta name="dc.title" content="Do Biomedical Tasks Require Biomedical Foundation Models?" />
  <meta name="citation_title" content="Do Biomedical Tasks Require Biomedical Foundation Models?" />
  <meta property="og:title" content="Do Biomedical Tasks Require Biomedical Foundation Models?" />
  <meta property="twitter:title" content="Do Biomedical Tasks Require Biomedical Foundation Models?" />
  <meta name="dc.date" content="2026-03-01" />
  <meta name="citation_publication_date" content="2026-03-01" />
  <meta property="article:published_time" content="2026-03-01" />
  <meta name="dc.modified" content="2026-03-01T06:55:43+00:00" />
  <meta property="article:modified_time" content="2026-03-01T06:55:43+00:00" />
  <meta name="dc.language" content="en-US" />
  <meta name="citation_language" content="en-US" />
  <meta name="dc.relation.ispartof" content="Manubot" />
  <meta name="dc.publisher" content="Manubot" />
  <meta name="citation_journal_title" content="Manubot" />
  <meta name="citation_technical_report_institution" content="Manubot" />
  <meta name="citation_author" content="Jingyun Jia" />
  <meta name="citation_author_institution" content="Department of Statistics, University of Wisconsin-Madison" />
  <meta name="citation_author_orcid" content="0009-0006-3241-3485" />
  <meta name="twitter:creator" content="@None" />
  <meta name="citation_author" content="Zhiyuan Li" />
  <meta name="citation_author_institution" content="Department of Computer Sciences, University of Wisconsin-Madison" />
  <meta name="citation_author_orcid" content="0009-0006-6016-7381" />
  <meta name="twitter:creator" content="@None" />
  <meta name="citation_author" content="Ben Lengerich" />
  <meta name="citation_author_institution" content="Department of Statistics, University of Wisconsin-Madison" />
  <meta name="citation_author_orcid" content="0000-0001-8690-9554" />
  <meta name="twitter:creator" content="@ben_lengerich" />
  <link rel="canonical" href="https://AdaptInfer.github.io/fm-survey/" />
  <meta property="og:url" content="https://AdaptInfer.github.io/fm-survey/" />
  <meta property="twitter:url" content="https://AdaptInfer.github.io/fm-survey/" />
  <meta name="citation_fulltext_html_url" content="https://AdaptInfer.github.io/fm-survey/" />
  <meta name="citation_pdf_url" content="https://AdaptInfer.github.io/fm-survey/manuscript.pdf" />
  <link rel="alternate" type="application/pdf" href="https://AdaptInfer.github.io/fm-survey/manuscript.pdf" />
  <link rel="alternate" type="text/html" href="https://AdaptInfer.github.io/fm-survey/v/d8908432faef168549db10f85f2511f4d9f35b68/" />
  <meta name="manubot_html_url_versioned" content="https://AdaptInfer.github.io/fm-survey/v/d8908432faef168549db10f85f2511f4d9f35b68/" />
  <meta name="manubot_pdf_url_versioned" content="https://AdaptInfer.github.io/fm-survey/v/d8908432faef168549db10f85f2511f4d9f35b68/manuscript.pdf" />
  <meta property="og:type" content="article" />
  <meta property="twitter:card" content="summary_large_image" />
  <link rel="icon" type="image/png" sizes="192x192" href="https://manubot.org/favicon-192x192.png" />
  <link rel="mask-icon" href="https://manubot.org/safari-pinned-tab.svg" color="#ad1457" />
  <meta name="theme-color" content="#ad1457" />
  <!-- end Manubot generated metadata -->
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">Do Biomedical Tasks Require Biomedical Foundation Models?</h1>
</header>
<p><small><em>
This manuscript
(<a href="https://AdaptInfer.github.io/fm-survey/v/d8908432faef168549db10f85f2511f4d9f35b68/">permalink</a>)
was automatically generated
from <a href="https://github.com/AdaptInfer/fm-survey/tree/d8908432faef168549db10f85f2511f4d9f35b68">AdaptInfer/fm-survey@d890843</a>
on March 1, 2026.
</em></small></p>
<h2 id="authors">Authors</h2>
<ul>
<li><p><strong>Jingyun Jia</strong>
<br>
<img src="images/orcid.svg" class="inline_icon" width="16" height="16" alt="ORCID icon" />
<a href="https://orcid.org/0009-0006-3241-3485">0009-0006-3241-3485</a>
· <img src="images/github.svg" class="inline_icon" width="16" height="16" alt="GitHub icon" />
<a href="https://github.com/Clouddelta">Clouddelta</a>
<br>
<small>
Department of Statistics, University of Wisconsin-Madison
</small></p></li>
<li><p><strong>Zhiyuan Li</strong>
<br>
<img src="images/orcid.svg" class="inline_icon" width="16" height="16" alt="ORCID icon" />
<a href="https://orcid.org/0009-0006-6016-7381">0009-0006-6016-7381</a>
· <img src="images/github.svg" class="inline_icon" width="16" height="16" alt="GitHub icon" />
<a href="https://github.com/LZYEIL">LZYEIL</a>
<br>
<small>
Department of Computer Sciences, University of Wisconsin-Madison
</small></p></li>
<li><p><strong>Ben Lengerich</strong>
<sup><a href="#correspondence">✉</a></sup><br>
<img src="images/orcid.svg" class="inline_icon" width="16" height="16" alt="ORCID icon" />
<a href="https://orcid.org/0000-0001-8690-9554">0000-0001-8690-9554</a>
· <img src="images/github.svg" class="inline_icon" width="16" height="16" alt="GitHub icon" />
<a href="https://github.com/blengerich">blengerich</a>
· <img src="images/twitter.svg" class="inline_icon" width="16" height="16" alt="Twitter icon" />
<a href="https://twitter.com/ben_lengerich">ben_lengerich</a>
<br>
<small>
Department of Statistics, University of Wisconsin-Madison
</small></p></li>
</ul>
<div id="correspondence">
<p>✉ — Correspondence possible via <a href="https://github.com/AdaptInfer/fm-survey/issues">GitHub Issues</a>
or email to
Ben Lengerich &lt;lengerich@wisc.edu&gt;.</p>
</div>
<h2 class="page_break_before" id="abstract">Abstract</h2>
<p>Modern predictive systems are expected to adapt their behavior to the specific situation they are facing.
A clinical model should not treat every patient the same; a retrieval-augmented model should change its answer when given different evidence; a mixture-of-experts model should route different inputs to different experts.
We call this capability <strong>context-adaptive inference</strong>: before predicting, the system uses information about the current context to specialize its parameters or computation for that instance.</p>
<p>This article provides a unified view of context-adaptive inference across three traditions that are usually treated separately:
(i) explicit adaptation in statistics (e.g. varying-coefficient models, local regression, hierarchical sharing),
(ii) rapid task-specific adaptation in meta-learning and transfer, and
(iii) implicit adaptation in large foundation models via prompting, retrieval, and expert routing.
We formalize these approaches under a common objective: to map context <span class="math inline">\(c\)</span> to adapted parameters
<span class="math inline">\(\theta(c)\)</span>, then to predict via <span class="math inline">\(f(x; \theta(c))\)</span>.
Under squared loss, linear prediction heads, and fixed features, we prove that explicit parameter adaptation and implicit routing are mathematically equivalent to kernel ridge regression on joint features of inputs and context.
Building on this bridge, we propose practical design principles and evaluation metrics including adaptation-efficiency, routing stability, and context-specific robustness to guide when to specialize, how to constrain that specialization, and how to audit context-adaptive models in deployment.
Finally, we identify open problems in identifiability, robustness under distribution shift, and efficient large-scale adaptation, outlining design principles for methods that are scalable, reliable, and transparent in real-world settings.</p>
<h2 id="introduction">Introduction</h2>
<h2 id="from-population-assumptions-to-context-adaptive-inference">From Population Assumptions to Context-Adaptive Inference</h2>
<p>Most statistical and machine learning models begin with a foundational assumption: that all samples are drawn independently and identically from a shared population distribution. This assumption simplifies estimation and enables generalization from limited data, but it collapses in the presence of meaningful heterogeneity.</p>
<p>In practice, data often reflect differences across individuals, environments, or conditions. These differences may stem from biological variation, temporal drift, site effects, or shifts in measurement context. Treating heterogeneous data as if it were homogeneous can obscure real effects, inflate variance, and lead to brittle predictions. As data grows more complex, the failure of this assumption not only limits accuracy but also obscures causal and contextual relationships underlying modern inference.</p>
<h3 id="failure-modes-of-population-models">Failure Modes of Population Models</h3>
<p>Even when traditional models appear to fit aggregate data well, they may hide systematic failure modes.</p>
<p><strong>Mode Collapse</strong><br />
When one subpopulation is much larger than another, standard models are biased toward the dominant group, underrepresenting the minority group in both fit and predictions.</p>
<p><strong>Outlier Sensitivity</strong><br />
In the parameter-averaging regime, small but extreme groups can disproportionately distort the global model, especially in methods like ordinary least squares.</p>
<p><strong>Phantom Populations</strong><br />
When multiple subpopulations are equally represented, the global model may fit none of them well, instead converging to a solution that represents a non-existent average case.</p>
<div id="fig:population-failures" class="fignos">
<figure>
<img src="images/population_failures.png" style="width:80.0%" alt="Figure 1: Failure Modes of Population Models. Illustrative schematics of common failure types when fitting a single global model to heterogeneous data. (A) Mode Collapse: the dominant group drives the fit, underrepresenting the minority. (B) Outlier Sensitivity: extreme points distort the global line, shifting predictions away from the majority. (C) Phantom Populations: the global fit represents no actual subgroup, but an artificial average. (D) Hidden Confounding / Simpson’s Paradox: aggregate trends reverse subgroup trends, obscuring true relationships." />
<figcaption aria-hidden="true"><span>Figure 1:</span> Failure Modes of Population Models. Illustrative schematics of common failure types when fitting a single global model to heterogeneous data.
(A) <strong>Mode Collapse</strong>: the dominant group drives the fit, underrepresenting the minority.
(B) <strong>Outlier Sensitivity</strong>: extreme points distort the global line, shifting predictions away from the majority.
(C) <strong>Phantom Populations</strong>: the global fit represents no actual subgroup, but an artificial average.
(D) <strong>Hidden Confounding / Simpson’s Paradox</strong>: aggregate trends reverse subgroup trends, obscuring true relationships.</figcaption>
</figure>
</div>
<p>These behaviors reflect a deeper problem: the assumption of identically distributed samples is not just incorrect, but actively harmful in heterogeneous settings.</p>
<h3 id="toward-context-aware-models">Toward Context-Aware Models</h3>
<p>To account for heterogeneity, we must relax the assumption of shared parameters and allow the data-generating process to vary across samples. A general formulation assumes each observation is governed by its own latent parameters:
<span class="math display">\[
x_i \sim P(x; \theta_i),
\]</span></p>
<p>However, estimating <span class="math inline">\(N\)</span> free parameters from <span class="math inline">\(N\)</span> samples is underdetermined.
Context-aware approaches resolve this by introducing structure on how parameters vary, often by assuming that <span class="math inline">\(\theta_i\)</span> depends on an observed context <span class="math inline">\(c_i\)</span>:</p>
<p><span class="math display">\[
\theta_i = f(c_i) \quad \text{or} \quad \theta_i \sim P(\theta \mid c_i).
\]</span></p>
<p>This formulation makes the model estimable, but it raises new challenges.
How should <span class="math inline">\(f\)</span> be chosen? How smooth, flexible, or structured should it be? The remainder of this review explores different answers to this question, and shows how implicit and explicit representations of context can lead to powerful, personalized models.</p>
<p>A classical example of this challenge arises in causal inference.
Following the Neyman–Rubin potential outcomes framework, we let <span class="math inline">\(Y(1)\)</span> and <span class="math inline">\(Y(0)\)</span> denote
the outcomes that would be observed under treatment and control, respectively.
The average treatment effect (ATE) is then <span class="math inline">\(E[Y(1) - Y(0)]\)</span>, or more generally the conditional
average treatment effect (CATE) given covariates.
Standard approaches often condition only on <span class="math inline">\(X\)</span>, while heterogeneous treatment effect (HTE)
models incorporate additional context <span class="math inline">\(C\)</span> to capture systematic variation across subpopulations
(Figure <a href="#fig:hte-context">2</a>).</p>
<div id="fig:hte-context" class="fignos">
<figure>
<img src="images/hte.png" style="width:70.0%" alt="Figure 2: Heterogeneous treatment effects. Left: average treatment effect (ATE) conditional on X, implicitly assuming homogeneity across contexts. Right: conditional average treatment effect (CATE) that allows treatment effects to vary systematically with additional context C." />
<figcaption aria-hidden="true"><span>Figure 2:</span> Heterogeneous treatment effects. Left: average treatment effect (ATE) conditional on <span class="math inline">\(X\)</span>,
implicitly assuming homogeneity across contexts. Right: conditional average treatment effect (CATE)
that allows treatment effects to vary systematically with additional context <span class="math inline">\(C\)</span>.</figcaption>
</figure>
</div>
<p>These models highlight both the promise and the challenges of choosing and estimating <span class="math inline">\(f(c)\)</span>.</p>
<h3 id="classical-remedies-grouped-and-distance-based-models">Classical Remedies: Grouped and Distance-Based Models</h3>
<p>Before diving into flexible estimators of <span class="math inline">\(f(c)\)</span>, we review early modeling strategies that attempt to break away from homogeneity.</p>
<h4 id="conditional-and-clustered-models">Conditional and Clustered Models</h4>
<p>One approach is to group observations into C contexts, either by manually defining conditions (e.g. male vs. female) or using unsupervised clustering. Each group is then assigned a distinct parameter vector:</p>
<p><span class="math display">\[
\{\widehat{\theta}_0, \ldots, \widehat{\theta}_C\} = \arg\max_{\theta_0, \ldots, \theta_C} \sum_{c \in \mathcal{C}} \ell(X_c; \theta_c),
\]</span>
where <span class="math inline">\(\ell(X; \theta)\)</span> is the log-likelihood of <span class="math inline">\(\theta\)</span> on <span class="math inline">\(X\)</span> and <span class="math inline">\(c\)</span> specifies the covariate group that samples are assigned to. This reduces variance but limits granularity. It assumes that all members of a group share the same distribution and fails to capture variation within a group.</p>
<p>These early methods relax global homogeneity yet still rely on discrete partitions, motivating smoother and more flexible formulations explored in the next sections.</p>
<h4 id="distance-regularized-estimation">Distance-Regularized Estimation</h4>
<p>A more flexible alternative assumes that observations with similar contexts should have similar parameters. This is encoded as a regularization penalty that discourages large differences in <span class="math inline">\(\theta_i\)</span> for nearby <span class="math inline">\(c_i\)</span>:</p>
<p><span class="math display">\[
\{\widehat{\theta}_0, \ldots, \widehat{\theta}_N\} = \arg\max_{\theta_0, \ldots, \theta_N} \left( \sum_i \ell(x_i; \theta_i) - \sum_{i,j} \frac{\|\theta_i - \theta_j\|}{D(c_i, c_j)} \right),
\]</span></p>
<p>where <span class="math inline">\(D(c_i, c_j)\)</span> is a distance metric between contexts. This approach allows for smoother parameter variation but requires careful choice of <span class="math inline">\(D\)</span> and regularization strength <span class="math inline">\(\lambda\)</span> to balance bias and variance.<br />
The choice of distance metric D and regularization strength λ controls the bias–variance tradeoff.</p>
<h3 id="parametric-and-semi-parametric-varying-coefficient-models">Parametric and Semi-parametric Varying-Coefficient Models</h3>
<p>Varying-coefficient models (VCMs) provide one of the earliest formal frameworks for explicit adaptivity. Parametric VCMs assume that parameters vary linearly with covariates, a restrictive but interpretable assumption <span class="citation" data-cites="ugXwusl0">[<a href="#ref-ugXwusl0" role="doc-biblioref">1</a>]</span>. The estimation can be written as
<span class="math display">\[
\widehat{A} = \arg\max_A \sum_i \ell(x_i; A c_i).
\]</span>
This formulation can be interpreted as a special case of distance-regularized estimation where the distance metric is Euclidean. Related developments in graphical models extend this idea to structured dependencies <span class="citation" data-cites="TULLRYDp">[<a href="#ref-TULLRYDp" role="doc-biblioref">2</a>]</span>.</p>
<p>Semi-parametric VCMs relax the linearity assumption by requiring only that parameter variation be smooth. This is commonly encoded through kernel weighting, where the relevance of each sample is determined by its similarity in the covariate space <span class="citation" data-cites="l6vMkIsa 1CkxORTSX">[<a href="#ref-l6vMkIsa" role="doc-biblioref">3</a>,<a href="#ref-1CkxORTSX" role="doc-biblioref">4</a>]</span>. These models are more flexible but may fail when the true relationship between covariates and parameters is discontinuous.</p>
<h3 id="contextualized-models">Contextualized Models</h3>
<p>Contextualized models take a fully non-parametric approach, introduced in <span class="citation" data-cites="SfCo6pSp">[<a href="#ref-SfCo6pSp" role="doc-biblioref">5</a>]</span>. They assume that parameters are functions of context, <span class="math inline">\(f(c)\)</span>, but do not restrict the form of <span class="math inline">\(f\)</span>. Instead, <span class="math inline">\(f\)</span> is estimated directly, often with deep neural networks as function approximators:
<span class="math display">\[
\widehat{f} = \arg \max_{f \in \mathcal{F}} \sum_i \ell(x_i; f(c_i)).
\]</span>
This framework has been widely applied, from machine learning toolboxes <span class="citation" data-cites="HYsEq2UQ 4cK1tiec">[<a href="#ref-HYsEq2UQ" role="doc-biblioref">6</a>,<a href="#ref-4cK1tiec" role="doc-biblioref">7</a>]</span> to personalized genomics <span class="citation" data-cites="Rt6voTFN grNza1Og">[<a href="#ref-Rt6voTFN" role="doc-biblioref">8</a>,<a href="#ref-grNza1Og" role="doc-biblioref">9</a>]</span>, biomedical informatics <span class="citation" data-cites="nYipTPML esxxcr9l O1UU4a5P">[<a href="#ref-nYipTPML" role="doc-biblioref">10</a>,<a href="#ref-esxxcr9l" role="doc-biblioref">11</a>,<a href="#ref-O1UU4a5P" role="doc-biblioref">12</a>]</span>, and contextual feature selection <span class="citation" data-cites="9S6tI5yv">[<a href="#ref-9S6tI5yv" role="doc-biblioref">13</a>]</span>. These examples highlight how contextual signals can drive adaptation without assuming a fixed functional form.</p>
<h3 id="partition-and-latent-structure-models">Partition and Latent-Structure Models</h3>
<p>Partition models extend the contextualized framework by assuming that parameters can be divided into homogeneous groups, while leaving group boundaries to be inferred. This design is useful for capturing abrupt changes over covariates such as time. Estimation typically balances the likelihood with a penalty on parameter differences between adjacent samples, often expressed through a Total Variation (TV) penalty <span class="citation" data-cites="lAsTg3IH">[<a href="#ref-lAsTg3IH" role="doc-biblioref">14</a>]</span>:
<span class="math display">\[
\{\widehat{\theta}_0, \dots, \widehat{\theta}_N\} = \arg\max_{\theta_0, \dots, \theta_N} \left( \sum_i \ell(x_i; \theta_i) + \lambda \sum_{i = 2}^N \|\theta_i - \theta_{i-1}\| \right).
\]</span>
By encouraging piecewise-constant structures, partition models get closer to personalized modeling, balancing fit and parsimony, moving closer to personalized inference, trading off flexibility for interpretability.</p>
<h3 id="fine-tuned-models-and-transfer-learning">Fine-tuned Models and Transfer Learning</h3>
<p>Another practical strategy for handling heterogeneity is fine-tuning. A global population model is first estimated, and then a smaller set of parameters is updated for particular subpopulations. This idea underlies transfer learning, where large pre-trained models are adapted to new tasks with limited additional training <span class="citation" data-cites="k6r0UwSv">[<a href="#ref-k6r0UwSv" role="doc-biblioref">15</a>]</span>. Fine-tuning balances the bias–variance tradeoff by borrowing statistical strength from large datasets while preserving flexibility for local adaptation. This notion was already recognized in early VCM literature as a form of semi-parametric estimation <span class="citation" data-cites="l6vMkIsa">[<a href="#ref-l6vMkIsa" role="doc-biblioref">3</a>]</span>.</p>
<h3 id="models-for-explicit-subgroup-separation">Models for Explicit Subgroup Separation</h3>
<p>Most adaptive methods encourage parameters for similar contexts to converge, but recent work explores the opposite: ensuring that models for distinct subgroups remain separated. This prevents minority subgroups from collapsing into majority patterns. Such “negative information sharing” is often implemented by learning representations that disentangle subgroup structure, bridging statistical partitioning with adversarial or contrastive learning objectives <span class="citation" data-cites="WlwUpYp">[<a href="#ref-WlwUpYp" role="doc-biblioref">16</a>]</span>.</p>
<h3 id="a-spectrum-of-context-awareness">A Spectrum of Context-Awareness</h3>
<p>Context-aware models can be organized along a spectrum of assumptions about the relationship between context and parameters:</p>
<ul>
<li><strong>Global models</strong>: <span class="math inline">\(\theta_i = \theta\)</span> for all <span class="math inline">\(i\)</span>.</li>
<li><strong>Grouped models</strong>: <span class="math inline">\(\theta_i = \theta_c\)</span> for some finite set of groups.</li>
<li><strong>Smooth models</strong>: <span class="math inline">\(\theta_i = f(c_i)\)</span>, with <span class="math inline">\(f\)</span> assumed to be continuous or low-complexity.</li>
<li><strong>Latent models</strong>: <span class="math inline">\(\theta_i \sim P(\theta \mid c_i)\)</span>, with <span class="math inline">\(f\)</span> learned implicitly.</li>
</ul>
<div id="fig:spectrum-context" class="fignos">
<figure>
<img src="images/spectrum_context.png" style="width:70.0%" alt="Figure 3: A spectrum of context awareness in modeling, showing global, grouped, smooth, and latent models" />
<figcaption aria-hidden="true"><span>Figure 3:</span> A spectrum of context awareness in modeling, showing global, grouped, smooth, and latent models</figcaption>
</figure>
</div>
<p>Each formulation encodes different beliefs about parameter variation. The next section formalizes these principles and examines general strategies for adaptivity in statistical modeling. For a discussion of how subpopulation shifts influence generalization, see <span class="citation" data-cites="HzzgJQN0">[<a href="#ref-HzzgJQN0" role="doc-biblioref">17</a>]</span>.</p>
<h3 id="independent-and-identically-distributed-samples">Independent and identically distributed samples</h3>
<p>The initial research data mostly came from strictly designed experiments, such as agricultural field trials or psychological experiments. The characteristics of such data are small scale and simple structure. Researchers usually assume that each observation is independent of one another and identically distributed <span class="citation" data-cites="17ryvnoPR">[<a href="#ref-17ryvnoPR" role="doc-biblioref">18</a>]</span>. Under this setting, there is no dependency among the data, and researchers mainly focus on the overall average level or effect.</p>
<p>Linear models emerged as the fundamental approach to conduct statistical analysis for such data. One of the first methods in the development of linear models, the method of least squares, was first published in a work by Legendre <span class="citation" data-cites="8BCLzHII">[<a href="#ref-8BCLzHII" role="doc-biblioref">19</a>]</span> and later independently developed and justified by Gauss <span class="citation" data-cites="vKtb27Ba">[<a href="#ref-vKtb27Ba" role="doc-biblioref">20</a>]</span>. By reducing the squared deviations between predictions and results, this method offered a general framework for fitting a regression line via observed data. The estimator is expressed as follows:</p>
<p><span class="math display">\[
\hat{\beta} = \arg\min_{\beta} \sum_{i=1}^{n} \bigl( y_i - x_i^{\top}\beta \bigr)^{2}
\]</span></p>
<p>which has a closed-form solution,</p>
<p><span class="math display">\[
\hat{\beta} = (X^{\top}X)^{-1}X^{\top}y
\]</span></p>
<p>This offered a systematic procedure for predicting unknown parameters from independently and identically distributed data, which builds the foundation of regression analysis.</p>
<p>The establishment of generalized linear models (GLMs) expanded the concepts of linear regression to non-Gaussian outcomes. Nelder and Wedderburn <span class="citation" data-cites="kXBVO9y8">[<a href="#ref-kXBVO9y8" role="doc-biblioref">21</a>]</span> initiated the central concept of connecting the mean of the response variable to a linear predictor through a link function, which was later formalized into the now-standard compact notation <span class="citation" data-cites="1HWqsAX0j">[<a href="#ref-1HWqsAX0j" role="doc-biblioref">22</a>]</span>:</p>
<p><span class="math display">\[
g(\mu_i) = \eta_i
\]</span></p>
<p>Before this unified general framework, one of the first instances was the use of the logistic function for binary data. Berkson advocated its application to biomedical dose–response studies, illustrating how the probabilities of success or failure in experimental settings could be captured by the link formulation <span class="citation" data-cites="15YaJttEf">[<a href="#ref-15YaJttEf" role="doc-biblioref">23</a>]</span>. Based on this, logistic regression was formalized as a regression model for binary sequences, establishing the logit link <span class="citation" data-cites="fzXXj6RE">[<a href="#ref-fzXXj6RE" role="doc-biblioref">24</a>]</span>:</p>
<p><span class="math display">\[
\log \frac{p_i}{1-p_i} = x_i^\top \beta
\]</span></p>
<p>For count data, Poisson regression was introduced within the GLM framework, employing the log link <span class="citation" data-cites="kXBVO9y8">[<a href="#ref-kXBVO9y8" role="doc-biblioref">21</a>]</span>:</p>
<p><span class="math display">\[
\log(\mu_i) = x_i^\top \beta
\]</span></p>
<p>These developments strengthened GLMs as a unifying framework for a variety of independently and identically distributed data types by extending linear modeling to categorical and count outcomes.</p>
<p>Alongside regression, analysis of variance (ANOVA) was another early milestone of statistical methodology. The development of ANOVA introduced the concept of splitting total variance into components related to within-group and between-group differences <span class="citation" data-cites="18GHyIdqs">[<a href="#ref-18GHyIdqs" role="doc-biblioref">25</a>]</span>. The central idea for ANOVA was the F ratio,</p>
<p><span class="math display">\[
F = \frac{MS_{\text{Between}}}{MS_{\text{Within}}}
\]</span></p>
<p>which provided a consistent framework for determining the significance of differences between group means and established the foundation of data analysis in modern experiments.</p>
<p>Together, these early statistical frameworks provided the foundation for analysis of independently and identically distributed data. However, as studies became more complex and observations were no longer truly independent, new methods were needed, leading to the development of hierarchical models.</p>
<h3 id="hierarchical-data">Hierarchical data</h3>
<p>With the expansion of research fields, a hierarchical structure gradually emerges in the collected empirical data. For example, bull is nested in sire and herd <span class="citation" data-cites="1FmrthAK7">[<a href="#ref-1FmrthAK7" role="doc-biblioref">26</a>]</span>; the respondents are nested within the population of subjects <span class="citation" data-cites="3oGysJy4">[<a href="#ref-3oGysJy4" role="doc-biblioref">27</a>]</span>; and repeated measurements of the same object at different time points also make the data show longitudinal correlation. This type of data is more complex than independent and identically distributed samples. It not only has differences among groups but also needs to take into account the changes of individuals over time simultaneously. The observed values are no longer completely independent of each other, but there exists a distinct hierarchical effect.</p>
<p>Early work on hierarchical dependence started with the introduction of linear models that contain both fixed and random effects to estimate genetic parameters <span class="citation" data-cites="1GKMYvnqB">[<a href="#ref-1GKMYvnqB" role="doc-biblioref">28</a>]</span>. This approach laid the statistical basis for partitioning variability into systematic and random components, which became the basis for how intraclass correlation would later be understood and applied. A general form of the linear models can be written as:</p>
<p><span class="math display">\[
y_{ij} = \mu + u_j + \varepsilon_{ij}, \quad u_j \sim N(0, \sigma_u^2), \quad \varepsilon_{ij} \sim N(0, \sigma^2)
\]</span></p>
<p>Around the same period, it became clear that comparative rate estimation in clinical and epidemiological studies depends on the structure of sampled subgroups, with heterogeneity in source populations affecting inference in widespread use <span class="citation" data-cites="pC7P4enS">[<a href="#ref-pC7P4enS" role="doc-biblioref">29</a>]</span>. Building on this recognition of heterogeneity, applications expanded to repeated measurements and longitudinal structures; restricted maximum likelihood estimation was introduced to advance the framework and improve variance component inference in unbalanced settings <span class="citation" data-cites="xPNSAM4v">[<a href="#ref-xPNSAM4v" role="doc-biblioref">30</a>]</span>. This methodological foundation enabled the formulation of linear mixed-effects models (LMMs) that combined fixed effects for population-level trends with random effects for subject- or group-specific deviations <span class="citation" data-cites="7EbnZ6mY">[<a href="#ref-7EbnZ6mY" role="doc-biblioref">31</a>]</span>. In matrix notation, these models take the form</p>
<p><span class="math display">\[
y = X\beta + Zu + \varepsilon, \quad u \sim N(0, G), \quad \varepsilon \sim N(0, R)
\]</span></p>
<p>and later became widely used in the field of biostatistics and social sciences. To extend mixed-effects modeling beyond Gaussian responses, practical estimation procedures for generalized linear models with random effects were proposed, enabling the application of link functions to clustered binary, count, or categorical outcomes <span class="citation" data-cites="iS2cDavY">[<a href="#ref-iS2cDavY" role="doc-biblioref">32</a>]</span>. Further methodological advances introduced penalized quasi-likelihood and approximate inference techniques that made practical application feasible in many fields, especially medical and biological fields <span class="citation" data-cites="sPnUGQ8l">[<a href="#ref-sPnUGQ8l" role="doc-biblioref">33</a>]</span>. These developments gave rise to Generalized Linear Mixed Models (GLMMs), formulated as</p>
<p><span class="math display">\[
g(\mu_i) = x_i^\top \beta + z_i^\top u
\]</span></p>
<p>combining link functions with both fixed (<span class="math inline">\(\beta\)</span>) and random (<span class="math inline">\(u\)</span>) effects. As mixed-effects approaches developed rapidly, the conceptual foundation of Bayesian hierarchical modeling—in which multilevel structures with prior distributions link parameters across groups—had already been articulated <span class="citation" data-cites="mLMS4pAF">[<a href="#ref-mLMS4pAF" role="doc-biblioref">34</a>]</span>:</p>
<p><span class="math display">\[
y_{ij} \mid \theta_j \sim p(y_{ij} \mid \theta_j), \quad \theta_j \sim p(\theta_j \mid \phi), \quad \phi \sim p(\phi)
\]</span></p>
<p>Nevertheless, it wasn’t until later that the Bayesian hierarchical model had practical influence. In the 1980s, the demonstration of the practical use of Gibbs sampling in image analysis paved the way for adoption in hierarchical Bayesian modeling <span class="citation" data-cites="SXlx1fmn">[<a href="#ref-SXlx1fmn" role="doc-biblioref">35</a>]</span>. Later, advances in Markov chain Monte Carlo methods provided the computational tools necessary to fit Bayesian hierarchical models and made Bayesian hierarchical modeling practical for applied researchers <span class="citation" data-cites="Jk46kTvA">[<a href="#ref-Jk46kTvA" role="doc-biblioref">36</a>]</span>.</p>
<p>Collectively, these developments shifted statistical modeling from independence assumptions to explicitly capturing correlation and hierarchical structure. As methods for nested data improved, new problems arose with functional, continuous, and high-dimensional observations. This led to the development of approaches for curves, trajectories, and large feature spaces, which in turn led to functional data analysis and high-dimensional inference.</p>
<h3 id="functional-types-and-high-dimensional-data">Functional types and high-dimensional data</h3>
<p>As data collection advanced into the 1980s and 1990s, researchers began encountering observations that are entire curves or functions (e.g., time series, spectra, images) rather than fixed-dimensional vectors. Functional Data Analysis (FDA) <span class="citation" data-cites="OXCuJonU">[<a href="#ref-OXCuJonU" role="doc-biblioref">37</a>]</span> treats each observation as a smooth function <span class="math inline">\(x_i(t)\)</span> defined over a continuum (time, wavelength, etc.). Typical FDA methods use basis expansions (e.g., Fourier or spline bases) to represent <span class="math inline">\(x_i(t)\)</span> and perform tasks such as functional principal component analysis. This emphasis on continuous predictors demanded flexible regression tools. Generalized Additive Models (GAMs) <span class="citation" data-cites="12q6JvZEW">[<a href="#ref-12q6JvZEW" role="doc-biblioref">38</a>]</span> were developed to capture nonlinear effects while retaining interpretability. In a GAM, the response is modeled as</p>
<p><span class="math display">\[
y_i = \alpha + \sum_{j=1}^p f_j(x_{ij}) + \varepsilon_i
\]</span></p>
<p>where each <span class="math inline">\(f_j\)</span> is an estimated smooth function (often implemented with splines or kernels). GAMs then generalize linear models by allowing the data to determine the shape of each predictor’s effect. For example, a GAM can automatically reveal a U-shaped or threshold effect in a covariate, which a purely linear model would miss.</p>
<p>As data dimensionality and complexity continued to grow (for instance with genomic or imaging data), there was a shift toward automated feature learning. Representation learning <span class="citation" data-cites="SVoGevDg">[<a href="#ref-SVoGevDg" role="doc-biblioref">39</a>]</span> generalizes classical dimension reduction (like PCA) to nonlinear, data-driven embeddings. Neural autoencoders are a prototypical example: one trains an encoder network <span class="math inline">\(h_\phi(x)\)</span> and a decoder <span class="math inline">\(g_\theta(z)\)</span> by minimizing reconstruction error,</p>
<p><span class="math display">\[
(\theta^{\\ast},\phi^{\\ast}) = \arg\min_{\theta,\phi}\sum_{i=1}^n |x_i - g_\theta(h_\phi(x_i))|^2
\]</span></p>
<p>Here <span class="math inline">\(z_i = h_\phi(x_i)\)</span> is a low-dimensional code capturing the essential structure of <span class="math inline">\(x_i\)</span>. With linear networks this recovers PCA, but in general the learned code can represent highly nonlinear features. Modern representation learning methods (e.g., deep autoencoders, variational autoencoders, deep embedding networks) therefore enable models to adapt their feature extraction to complex, high-dimensional data in a context-dependent way <span class="citation" data-cites="SVoGevDg">[<a href="#ref-SVoGevDg" role="doc-biblioref">39</a>]</span>. By learning bases and transformations from the data itself, these methods built on the ideas of FDA and GAMs to handle the evolving scale and richness of scientific data.</p>
<h3 id="heterogeneous-tasks-and-sparse-data">Heterogeneous tasks and sparse data</h3>
<p>With the proliferation of diverse tasks and domains, statistical learning shifted toward methods that transfer information across problems. Multi-task learning (MTL) <span class="citation" data-cites="9k4OKrXL">[<a href="#ref-9k4OKrXL" role="doc-biblioref">40</a>]</span> arose in response: it jointly models related tasks to improve performance, especially when each task has limited data. Concretely, if we have <span class="math inline">\(T\)</span> tasks with data <span class="math inline">\((X^t,Y^t)\)</span> for <span class="math inline">\(t=1,\dots,T\)</span>, one can learn task-specific models <span class="math inline">\(w^t\)</span> by minimizing a coupled objective, for example</p>
<p><span class="math display">\[
\min_{W}\sum_{t=1}^T \sum_{i=1}^{n_t} \ell(y_i^t, f(x_i^t;w^t)) + \lambda\,\Omega(W)
\]</span></p>
<p>where <span class="math inline">\(W=[w^1,\dots,w^T]\)</span> collects all task parameters and <span class="math inline">\(\Omega(W)\)</span> is a regularizer that enforces shared structure (e.g., penalizing deviations from a common parameter). This formulation lets tasks share a representation or bias: information useful for one task can aid others. Transfer learning extends this idea between domains wherein a model learned on a large source dataset is adapted to a target domain with little labeled data <span class="citation" data-cites="12JtL2o6T">[<a href="#ref-12JtL2o6T" role="doc-biblioref">41</a>]</span>. For instance, a convolutional neural network pretrained on ImageNet can be fine-tuned on a small medical imaging dataset, effectively reusing learned features to improve accuracy with scarce data.</p>
<p>Another fundamental challenge is distribution shift between training and deployment. Covariate shift occurs when the input distribution <span class="math inline">\(p(x)\)</span> changes but the conditional <span class="math inline">\(p(y|x)\)</span> remains the same. In this case one can correct the loss by importance weighting. Denote <span class="math inline">\(p_{\mathrm{train}}(x)\)</span> and <span class="math inline">\(p_{\mathrm{test}}(x)\)</span> the feature distributions; then</p>
<p><span class="math display">\[
\mathbb{E}_{x\sim p_{\mathrm{test}}}[\ell(f(x),y)]
= \mathbb{E}_{x\sim p_{\mathrm{train}}}\Bigl[\tfrac{p_{\mathrm{test}}(x)}{p_{\mathrm{train}}(x)}\,\ell(f(x),y)\Bigr], \quad
w(x) = \frac{p_{\mathrm{test}}(x)}{p_{\mathrm{train}}(x)}
\]</span></p>
<p>More generally, domain adaptation methods address cases where both <span class="math inline">\(p(x)\)</span> and <span class="math inline">\(p(y|x)\)</span> differ across source and target domains. Techniques such as kernel mean matching, adversarial domain-invariant representations, or graph-based alignment were developed to tackle these shifts. These methods emerged as data collection became distributed across different environments (e.g., new sensors, populations, or changing conditions), requiring models that adapt to new contexts beyond the original training distribution.</p>
<p>Finally, the combination of many tasks and very few examples per task has given rise to few-shot learning. In few-shot learning <span class="citation" data-cites="S1Lim9f9">[<a href="#ref-S1Lim9f9" role="doc-biblioref">42</a>]</span>, the goal is to generalize to new classes or tasks from only a handful of labeled examples. Modern approaches typically leverage prior experience across tasks or classes. Metric-based methods, such as Matching Networks and Prototypical Networks <span class="citation" data-cites="3wZYBtMr 1EGW21dqi">[<a href="#ref-3wZYBtMr" role="doc-biblioref">43</a>,<a href="#ref-1EGW21dqi" role="doc-biblioref">44</a>]</span>, learn an embedding <span class="math inline">\(\phi(x)\)</span> so that samples from the same class cluster together. A novel class can then be represented by the mean of its few support examples in this latent space. Alternatively, meta-learning algorithms train on many simulated few-shot tasks so the model learns to adapt quickly. These few-shot and meta-learning frameworks explicitly confront data scarcity by transferring inductive biases across tasks, enabling effective learning in regimes far beyond the traditional large-sample assumptions.</p>
<h3 id="online-and-interactive-data">Online and interactive data</h3>
<p>In the Internet era, the way data is collected has undergone significant changes. User behavior data, sensor data, and platform experimental data exhibit the characteristics of streaming and interactivity <span class="citation" data-cites="On9K3pxW">[<a href="#ref-On9K3pxW" role="doc-biblioref">45</a>]</span>. The generation of dynamic data poses new challenges to model construction and problem-solving. Data is not collected all at once but is generated in real time and is often related to the feedback loop of the system <span class="citation" data-cites="RauXHY7u">[<a href="#ref-RauXHY7u" role="doc-biblioref">46</a>]</span>. This type of data is more dynamic and complex compared to traditional experimental or measurement data, involving both time dependence and continuous influence from the environment and interaction.</p>
<p>A natural starting point to address this challenge is the concept of online learning <span class="citation" data-cites="10ULlHdY2">[<a href="#ref-10ULlHdY2" role="doc-biblioref">47</a>]</span>. A foundational formulation is the Online Convex Optimization (OCO) framework <span class="citation" data-cites="5S8Ulwe8">[<a href="#ref-5S8Ulwe8" role="doc-biblioref">48</a>]</span>. At each round <span class="math inline">\(t = 1, \dots, T\)</span>, the learner selects a decision <span class="math inline">\(x_t \in F\)</span>, where <span class="math inline">\(F \subset \mathbb{R}^n\)</span> is a convex decision set. The environment then reveals a convex loss function <span class="math inline">\(c_t : F \to \mathbb{R}\)</span>, and the learner incurs loss <span class="math inline">\(c_t(x_t)\)</span>. The central performance measure is the regret, defined as:</p>
<p><span class="math display">\[
R(T) = \sum_{t=1}^{T} c_t(x_t) - \min_{x \in F} \sum_{t=1}^{T} c_t(x)
\]</span></p>
<p>which compares the learner’s cumulative loss with that of the best fixed decision in hindsight. A simple and influential algorithm in this framework is Online Gradient Descent (OGD). Given a subgradient <span class="math inline">\(g_t \in \partial c_t(x_t)\)</span>, OGD performs a gradient step followed by projection back onto the feasible set:</p>
<p><span class="math display">\[
x_{t+1} = \Pi_F(x_t - \eta_t g_t)
\]</span></p>
<p>where <span class="math inline">\(\eta_t\)</span> is a step size and <span class="math inline">\(\Pi_F\)</span> denotes projection onto <span class="math inline">\(F\)</span>. With appropriately chosen step sizes, OGD achieves the classic bound <span class="math inline">\(R(T) = O(\sqrt{T})\)</span>, implying vanishing average regret. This framework provides the mathematical foundation for more complex online methods, such as bandits and reinforcement learning.</p>
<p>While OCO provides guarantees with respect to a fixed comparator, real-world data streams rarely remain stationary. The statistical relationship between inputs <span class="math inline">\(x\)</span> and outputs <span class="math inline">\(y\)</span> can change over time. This phenomenon is known as concept drift <span class="citation" data-cites="BkxeQdkH">[<a href="#ref-BkxeQdkH" role="doc-biblioref">49</a>]</span>. To cope with drift, online adaptive learning methods augment the basic OCO paradigm by explicitly adapting to distributional changes. A representative technique for drift detection is the Drift Detection Method (DDM) <span class="citation" data-cites="QbZy0vLM">[<a href="#ref-QbZy0vLM" role="doc-biblioref">50</a>]</span>. It monitors the online error rate <span class="math inline">\(\hat{p}\_i\)</span> after <span class="math inline">\(i\)</span> samples together with its standard deviation <span class="math inline">\(s\_i\)</span>. The method keeps track of the minimum values observed <span class="math inline">\((p\_{\min}, s\_{\min})\)</span>, and raises an alarm when</p>
<p><span class="math display">\[
\hat{p}_i + s_i \geq p_{\min} + \alpha \cdot s_{\min}
\]</span></p>
<p>for a predefined threshold <span class="math inline">\(\alpha\)</span>. This statistical test signals when the current error distribution significantly deviates from the past minimum, indicating concept drift. The old model is then discarded, and a new model is trained using the data accumulated since the warning level (i.e., from <span class="math inline">\(k_w\)</span> to <span class="math inline">\(k_d\)</span>). After reinitialization, <span class="math inline">\((p_{\min}, s_{\min})\)</span> are reset, and the updated model continues processing the incoming stream. Beyond DDM, the Early Drift Detection Method (EDDM) <span class="citation" data-cites="jbY4pWUH">[<a href="#ref-jbY4pWUH" role="doc-biblioref">51</a>]</span> monitors the distribution of distances between errors, making it more sensitive to gradual drifts. Ensemble-based methods such as Bagging-ADWIN and Boosting-ADWIN <span class="citation" data-cites="yml0Vh7r">[<a href="#ref-yml0Vh7r" role="doc-biblioref">52</a>]</span>, which couple classical ensemble learning with adaptive sliding windows, have also demonstrated strong performance on evolving data streams.</p>
<p>Though adaptive learning addresses the challenge of non-stationarity, it still operates under a full-information feedback model: after each round, the entire loss function <span class="math inline">\(c_t(\cdot)\)</span> is revealed. In many interactive systems, however, such complete feedback is unavailable. This setting motivates the study of partial-information online learning, captured by the classic Multi-Armed Bandit (MAB) framework <span class="citation" data-cites="18RlJp2YO">[<a href="#ref-18RlJp2YO" role="doc-biblioref">53</a>]</span>. Within the MAB, the agent repeatedly chooses among a finite set of actions (“arms”) and receives the reward of that arm. The central challenge is the exploration–exploitation trade-off. Classical strategies include <span class="math inline">\(\epsilon\)</span>-greedy <span class="citation" data-cites="jlS72VrS">[<a href="#ref-jlS72VrS" role="doc-biblioref">54</a>]</span>, which with probability <span class="math inline">\(1-\epsilon\)</span> selects the current best arm and with probability <span class="math inline">\(\epsilon\)</span> explores. Upper Confidence Bound (UCB) algorithms <span class="citation" data-cites="17mBgrHWB">[<a href="#ref-17mBgrHWB" role="doc-biblioref">55</a>]</span> emphasize the “Optimism in the Face of Uncertainty,” where at each round <span class="math inline">\(t\)</span> with arm <span class="math inline">\(a\)</span>:</p>
<p><span class="math display">\[
\mathrm{UCB}_a(t) = \hat{\mu}_a + \sqrt{\frac{2\ln t}{n_a}}
\]</span></p>
<p>The arm <span class="math inline">\(\arg\max_a \mathrm{UCB}_a(t)\)</span> is chosen at each round, achieving sublinear regret. Their key limitation—ignoring contextual side information—leads naturally to Contextual Bandits. Formally, at each round <span class="math inline">\(t\)</span>, the agent now observes a context <span class="math inline">\(x_t\)</span> and the goal is to learn a policy <span class="math inline">\(\pi : X \to A\)</span> that maximizes expected cumulative reward. A representative algorithm is LinUCB, which assumes a linear relationship between context and reward; at round <span class="math inline">\(t\)</span>, LinUCB selects</p>
<p><span class="math display">\[
a_t = \arg \max_{a \in \mathcal{A}} \left( x_t^\top \hat{\theta}_a + \alpha \sqrt{x_t^\top A_a^{-1} x_t} \right)
\]</span></p>
<p>where <span class="math inline">\(\hat{\theta}_a\)</span> is the estimated weight and <span class="math inline">\(A_a\)</span> is the design matrix. Contextual bandits improve by personalizing decisions to the observed environment. They have been widely applied in news recommendation and adaptive experimentation. However, CB assumes actions do not affect future contexts, which motivates the richer framework of reinforcement learning.</p>
<p>RL formalizes sequential decision-making under uncertainty through the lens of Markov Decision Processes (MDPs) <span class="citation" data-cites="SldOo5zA">[<a href="#ref-SldOo5zA" role="doc-biblioref">56</a>]</span>. An MDP is defined by a state space <span class="math inline">\(S\)</span>, an action space <span class="math inline">\(A\)</span>, transition dynamics <span class="math inline">\(P(s&#39; \mid s, a)\)</span>, a reward function <span class="math inline">\(r(s, a)\)</span>, and a discount factor <span class="math inline">\(\gamma \in [0, 1)\)</span>. At each step <span class="math inline">\(t\)</span>, the agent observes a state <span class="math inline">\(s_t\)</span>, selects an action <span class="math inline">\(a_t \sim \pi(\cdot \mid s_t)\)</span>, receives a reward <span class="math inline">\(r_t\)</span>, and transitions to a new state <span class="math inline">\(s_{t+1}\)</span>. The goal is to learn a policy <span class="math inline">\(\pi\)</span> that maximizes the expected cumulative discounted reward:</p>
<p><span class="math display">\[
J(\pi) = \mathbb{E}_{\pi} \left[ \sum_{t=0}^{\infty} \gamma^t r_t \right]
\]</span></p>
<p>Classical solution approaches include value-based methods (e.g., Q-learning) <span class="citation" data-cites="jggKMQFt">[<a href="#ref-jggKMQFt" role="doc-biblioref">57</a>]</span>, which estimate action-value functions and act greedily with respect to them, and policy-based methods (e.g., policy gradient, actor–critic) <span class="citation" data-cites="5ivkUecW">[<a href="#ref-5ivkUecW" role="doc-biblioref">58</a>]</span> <span class="citation" data-cites="YAPw7zCk">[<a href="#ref-YAPw7zCk" role="doc-biblioref">59</a>]</span>, which directly optimize the policy parameters via stochastic gradient ascent. These methods establish theoretical guarantees and have enabled applications ranging from game-playing (e.g., Go, Atari) <span class="citation" data-cites="BLbPVL8P">[<a href="#ref-BLbPVL8P" role="doc-biblioref">60</a>]</span> to robotics.</p>
<p>The trajectory from online convex optimization to reinforcement learning highlights how the statistical study of streaming, interactive data has evolved into increasingly expressive frameworks for context-adaptive inference. As data collection becomes more interactive and non-stationary, the ability to learn from context and feedback loops is central to adaptive intelligence. This theme naturally connects to the next section on multimodal data, where adaptivity must also span across modalities.</p>
<h3 id="multimodal-data">Multimodal data</h3>
<p>With the advancement of the digitalization process, research and application have begun to simultaneously involve various types of data such as images <span class="citation" data-cites="13xFsnpVI">[<a href="#ref-13xFsnpVI" role="doc-biblioref">61</a>]</span>, audio <span class="citation" data-cites="1F4l9zw7H">[<a href="#ref-1F4l9zw7H" role="doc-biblioref">62</a>]</span>, and text <span class="citation" data-cites="u6KGkrtr">[<a href="#ref-u6KGkrtr" role="doc-biblioref">63</a>]</span>. These data are not only high-dimensional but also show clear structural and representational differences. Text consists of discrete symbolic sequences that contain semantic and grammatical structures; images are spatially organized pixel matrices that reflect local spatial correlations and global patterns; and audio is a continuous waveform signal that captures dynamic temporal characteristics. Compared with earlier single-type numerical or functional data, multimodal data are more heterogeneous and complex. New challenges asked researchers to explore how to establish semantic correspondences across modalities for cross-modal alignment, build joint models through shared latent spaces, and dynamically adjust inter-modal dependencies according to different tasks or contexts.</p>
<p>From a statistical perspective, representation learning <span class="citation" data-cites="SVoGevDg">[<a href="#ref-SVoGevDg" role="doc-biblioref">39</a>]</span> can be regarded as a generalization of traditional linear dimension reduction techniques such as Principal Component Analysis (PCA) and Factor Analysis (FA). While PCA and FA aim to identify low-dimensional subspaces that capture maximum variance or shared covariance structure through linear projections, representation learning extends this idea to nonlinear mappings that can model highly complex and structured data. Representation learning addresses this challenge by automatically learning low-dimensional embeddings that map complex observations into a shared latent space. The basic idea is to learn a nonlinear mapping</p>
<p><span class="math display">\[
f_{\theta}: \mathcal{X} \rightarrow \mathcal{Z}
\]</span></p>
<p>where <span class="math inline">\(\mathcal{X}\)</span> represents the original multimodal input space (e.g., images, text, or audio), and <span class="math inline">\(\mathcal{Z}\)</span> denotes the shared latent space. The context-adaptive nature of representation learning lies in its ability to adjust feature extraction pathways according to the input modality and to align heterogeneous data within a unified semantic space. Recent theoretical work provides a unified explanation of this adaptability. The Contexture Theory formalizes representation learning as approximating the expectation operator induced by the context variable, linking supervised, self-supervised, and manifold learning under a common spectral framework <span class="citation" data-cites="1BlaGxmbh">[<a href="#ref-1BlaGxmbh" role="doc-biblioref">64</a>]</span>. This perspective highlights that constructing informative context variables can yield greater efficiency than merely scaling model capacity, offering a rigorous theoretical basis for context-aware representation learning.</p>
<p>To infer hidden structures, traditional latent variable models in statistics such as Factor Analysis and Gaussian Mixture Models (GMM) require estimating the posterior distribution <span class="math inline">\(p(z|x)\)</span>. However, when the generative process <span class="math inline">\(p_\theta(x|z)\)</span> is highly nonlinear, the posterior becomes intractable. Variational inference approximates the true posterior by introducing a tractable distribution <span class="math inline">\(q(z|x)\)</span> and minimizing their divergence, typically via the Kullback–Leibler (KL) divergence.</p>
<p>Amortized inference introduces a major innovation by parameterizing the approximate posterior as a learnable function <span class="math inline">\(q_\phi(z|x)\)</span> shared across all samples. This amortizes the cost of inference, enabling efficient posterior estimation via a neural network encoder. Variational Autoencoders (VAEs) <span class="citation" data-cites="NLVTJ9Lj">[<a href="#ref-NLVTJ9Lj" role="doc-biblioref">65</a>]</span> can be regarded as nonlinear extensions of classical latent variable models. VAEs jointly train the generative model <span class="math inline">\(p_\theta(x|z)\)</span> and the inference model <span class="math inline">\(q_\phi(z|x)\)</span> by maximizing the Evidence Lower Bound (ELBO):</p>
<p><span class="math display">\[
\mathcal{L}(\theta, \phi; x) = \mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)] - D_{KL}[q_\phi(z|x) \,||\, p(z)]
\]</span></p>
<p>The model can dynamically adjust posterior estimation <span class="math inline">\(q_\phi(z|x)\)</span> according to input characteristics or modality, achieving efficient and scalable probabilistic reasoning.</p>
<p>In many real-world scenarios, the amount of labeled data for each task may be limited. Meta-learning, or learning to learn, provides a framework for acquiring inductive knowledge that enables rapid adaptation to new tasks with few examples. The central idea is to learn a meta-model that captures transferable structures across tasks and can quickly adjust its parameters when facing a new context. It involves a two-level optimization process:</p>
<p><span class="math display">\[
\min_{\theta} \sum_{T_i \sim p(T)} \mathcal{L}_{T_i}\big(U(\theta, T_i)\big)
\]</span></p>
<p>where <span class="math inline">\(\theta\)</span> denotes the meta-parameters shared across tasks, <span class="math inline">\(T_i\)</span> represents individual tasks sampled from a task distribution <span class="math inline">\(p(T)\)</span>, and <span class="math inline">\(U(\theta, T_i)\)</span> indicates the task-specific adaptation process, such as gradient updates.</p>
<p>Two main paradigms have been developed. Gradient-based meta-learning, exemplified by Model-Agnostic Meta-Learning (MAML) <span class="citation" data-cites="JE5FRU4v">[<a href="#ref-JE5FRU4v" role="doc-biblioref">66</a>]</span>, optimizes an initialization of <span class="math inline">\(\theta\)</span> that allows fast convergence on new tasks with a few gradient steps. In contrast, metric-based meta-learning <span class="citation" data-cites="1EGW21dqi">[<a href="#ref-1EGW21dqi" role="doc-biblioref">44</a>]</span> focuses on learning a task-invariant representation space in which samples from the same class or semantic context remain close in distance metrics.</p>
<p>Meta-learning can be viewed as a hierarchical model where the outer loop learns the hyperprior over tasks and the inner loop performs task-specific inference. The model internalizes the variability across tasks into shared meta-parameters and dynamically adjusts its learning strategy when exposed to new contextual distributions. Such an approach bridges the gap between multi-task learning and context-aware adaptation, providing a scalable solution for heterogeneous and data-sparse environments.</p>
<h3 id="large-scale-pre-trained-data">Large-scale pre-trained data</h3>
<p>With the rapid expansion of digital ecosystems and the emergence of web-scale data collection, research has entered a new stage characterized by massive, heterogeneous, and weakly supervised datasets. In recent years, a vast amount of cross-domain data has been centrally collected—such as large-scale text corpora <span class="citation" data-cites="urJgpE6q">[<a href="#ref-urJgpE6q" role="doc-biblioref">67</a>]</span> and multimodal alignment data <span class="citation" data-cites="17tnf46zM">[<a href="#ref-17tnf46zM" role="doc-biblioref">68</a>]</span> that combine text, images, audio, and sound—representing an unprecedented scale and diversity of information sources. Unlike earlier multimodal datasets that were carefully curated for specific experimental designs, these pretraining datasets are gathered under open, non-controlled conditions and often lack explicit task labels or unified structures.</p>
<p>The central modeling challenge has shifted from fitting a single data distribution to extracting transferable and generalizable patterns that remain stable across heterogeneous contexts. This transition has motivated the development of foundation models and related adaptive paradigms, which aim to capture universal representations, perform context-dependent inference, and achieve robust generalization across domains.</p>
<p>Foundation models <span class="citation" data-cites="1GbAsSOZV">[<a href="#ref-1GbAsSOZV" role="doc-biblioref">69</a>]</span> represent a fundamental paradigm shift in machine learning toward building universal systems trained on massive and heterogeneous data sources. Unlike earlier models that were designed for specific tasks or modalities, foundation models learn general-purpose representations through large-scale pretraining, which can be adapted to downstream tasks with minimal fine-tuning. Given a massive dataset <span class="math inline">\(\mathcal{D} = \lbrace x_i \rbrace_{i=1}^N\)</span> sampled from diverse domains, the objective is to learn a parameterized function <span class="math inline">\(f_\theta\)</span> that captures broadly transferable patterns:</p>
<p><span class="math display">\[
\theta^* = \arg\min_\theta \; \mathbb{E}_{x \sim \mathcal{D}} \, \ell(f_\theta(x))
\]</span></p>
<p>where <span class="math inline">\(\ell\)</span> represents a pretraining objective such as next-token prediction or contrastive alignment.</p>
<p>The context-adaptive nature of foundation models arises from their ability to integrate knowledge from large and diverse data distributions, enabling emergent behaviors such as cross-modal transfer, domain generalization, and zero-shot reasoning. They thus redefine the notion of generalization—from fitting within a context to adapting across contexts—by embedding statistical invariances into large-scale learned representations.</p>
<p>In-context learning (ICL) <span class="citation" data-cites="1EkVeYD9V">[<a href="#ref-1EkVeYD9V" role="doc-biblioref">70</a>]</span> describes the ability of large language or multimodal models to adapt to new tasks through contextual examples provided at inference time, without updating model parameters. Given a prompt sequence <span class="math inline">\(\lbrace(x_i, y_i)\rbrace_{i=1}^k\)</span> followed by a query <span class="math inline">\(x_{k+1}\)</span>, the model generates a prediction <span class="math inline">\(\hat{y}_{k+1}\)</span> by conditioning on the in-context information:</p>
<p><span class="math display">\[
\hat{y}_{k+1} = f_\theta(x_{k+1} \mid x_1, y_1, \dots, x_k, y_k)
\]</span></p>
<p>This phenomenon suggests that pretrained models can perform implicit meta-learning within their internal representations, dynamically adjusting inference behavior according to contextual input. From a statistical perspective, ICL transforms the adaptation process from an external optimization (parameter update) to an internal inference mechanism conditioned on observed data, thus embodying a new form of context-adaptive reasoning.</p>
<h2 id="principles-of-context-adaptive-inference">Principles of Context-Adaptive Inference</h2>
<p>What makes a model adaptive? When is it good for a model to be adaptive? While the appeal of adaptivity lies in flexibility and personalized inference, not all adaptivity is beneficial. This section formalizes the core principles that underlie adaptive modeling and situates them within both classical statistics and recent advances in machine learning.</p>
<p>Adaptivity is best understood as a structured set of design principles rather than a single mechanism. Each principle described below highlights a different axis along which models can incorporate or restrict adaptation. Flexibility captures the representational capacity needed for adaptation, while signals of heterogeneity determine when adaptation is justified. Modularity helps organize adaptation into interpretable and transferable units, and selectivity guards against overfitting by controlling when adaptation is triggered. Data efficiency limits how finely we can adapt in practice, and tradeoffs remind us that adaptation is never free of cost. Together, these principles delineate both the potential and the pitfalls of adaptive systems.</p>
<p>We organize this section around six core principles: flexibility, heterogeneity signals, modularity, selectivity, data efficiency, and tradeoffs. Afterward, we discuss failure modes and conclude with a synthesis that connects these ideas to practical implications.</p>
<h3 id="adaptivity-requires-flexibility">1. Adaptivity requires flexibility</h3>
<p>The first principle concerns model capacity. A model must be able to represent multiple behaviors if it is to adapt. Without sufficient representational richness, adaptation becomes superficial, amounting only to noise-fitting rather than meaningful personalization. Flexibility provides the foundation for models to express diverse responses across individuals, groups, or environments, rather than enforcing a single global rule.</p>
<p>Flexibility may arise from different modeling strategies. In classical statistics, regression models with interaction effects explicitly capture how predictors influence outcomes differently across contexts, while hierarchical and multilevel models let effects vary systematically across groups. Varying-coefficient models extend this further by allowing regression coefficients to evolve smoothly with contextual covariates <span class="citation" data-cites="ugXwusl0">[<a href="#ref-ugXwusl0" role="doc-biblioref">1</a>]</span>. In machine learning, meta-learning and mixture-of-experts architectures <span class="citation" data-cites="ljL7YbZD">[<a href="#ref-ljL7YbZD" role="doc-biblioref">71</a>]</span> offer dynamic allocation of capacity, training models to specialize on tasks or inputs as needed. Together, these approaches illustrate the common principle that without flexibility, adaptation has no meaningful space in which to operate.</p>
<h3 id="adaptivity-requires-a-signal-of-heterogeneity">2. Adaptivity requires a signal of heterogeneity</h3>
<p>Flexibility alone is not enough; a model also requires observable signals that indicate how and why adaptation should occur. Without such signals, adaptive systems risk reacting to random fluctuations rather than capturing meaningful structure. In statistics, varying-coefficient regressions illustrate this idea by allowing parameters to change smoothly with observed covariates <span class="citation" data-cites="ugXwusl0">[<a href="#ref-ugXwusl0" role="doc-biblioref">1</a>]</span>, while hierarchical models assume systematic group differences that provide a natural signal for adaptive pooling.</p>
<p>In machine learning, contextual bandits adapt decisions to side information that characterizes the current environment, while benchmarks like WILDS highlight that real-world datasets often contain distributional shifts and subgroup heterogeneity <span class="citation" data-cites="PKjSQOD">[<a href="#ref-PKjSQOD" role="doc-biblioref">72</a>]</span>. Recent work extends this further, modeling time-varying changes in continuous temporal domain generalization <span class="citation" data-cites="Fto8UbOH">[<a href="#ref-Fto8UbOH" role="doc-biblioref">73</a>]</span> or leveraging diversity across experts to separate stable from unstable patterns <span class="citation" data-cites="2Asz98yx">[<a href="#ref-2Asz98yx" role="doc-biblioref">74</a>]</span>. Across applications, from medicine to online platforms, heterogeneity signals provide the essential cues that justify adaptation.</p>
<h3 id="modularity-improves-adaptivity">3. Modularity improves adaptivity</h3>
<p>Organizing adaptation into modular units improves interpretability and robustness. Instead of spreading changes across an entire system, modularity restricts variation to well-defined subcomponents that can be recombined, reused, or replaced. This structure provides three advantages: targeted adaptation, transferability across tasks, and disentanglement of variation sources.</p>
<p>A canonical example is the mixture-of-experts framework, where a gating network routes inputs to specialized experts trained for different data regimes <span class="citation" data-cites="ljL7YbZD">[<a href="#ref-ljL7YbZD" role="doc-biblioref">71</a>]</span>. By decomposing capacity in this way, models not only gain efficiency but also clarify which components are responsible for specific adaptive behaviors. Recent advances extend this principle in modern architectures: modular domain experts <span class="citation" data-cites="18QzJqDj4">[<a href="#ref-18QzJqDj4" role="doc-biblioref">75</a>]</span>, adapter libraries for large language models <span class="citation" data-cites="UxVULYQ3">[<a href="#ref-UxVULYQ3" role="doc-biblioref">76</a>]</span>, and mixtures of LoRA experts <span class="citation" data-cites="UskQdlu3">[<a href="#ref-UskQdlu3" role="doc-biblioref">77</a>]</span>. In applications ranging from language processing to computer vision, modularity has become a cornerstone of scalable adaptivity.</p>
<h3 id="adaptivity-implies-selectivity">4. Adaptivity implies selectivity</h3>
<p>Adaptation must not occur indiscriminately. Overreacting to noise leads to overfitting, defeating the purpose of adaptation. Selectivity provides the discipline that ensures adaptive mechanisms respond only when supported by reliable evidence.</p>
<p>Classical statistics formalized this principle through methods such as Lepski’s rule for bandwidth selection, which balances bias and variance in nonparametric estimation <span class="citation" data-cites="TBU9RF9F">[<a href="#ref-TBU9RF9F" role="doc-biblioref">78</a>]</span>. Aggregation methods such as the weighted majority algorithm show how selective weighting of multiple models can improve robustness <span class="citation" data-cites="dSHoyg5K">[<a href="#ref-dSHoyg5K" role="doc-biblioref">79</a>]</span>. In modern machine learning, Bayesian rules can activate test-time updates only when uncertainty is manageable <span class="citation" data-cites="FpwH3vaM">[<a href="#ref-FpwH3vaM" role="doc-biblioref">80</a>]</span>, while confidence-based strategies prevent unstable adjustments by holding back adaptation under weak signals <span class="citation" data-cites="D2dv1dGG">[<a href="#ref-D2dv1dGG" role="doc-biblioref">81</a>]</span>. Sparse expert models apply the same principle architecturally, activating only a few experts for easy inputs but engaging more capacity for difficult cases <span class="citation" data-cites="1CQ8u4q3f">[<a href="#ref-1CQ8u4q3f" role="doc-biblioref">82</a>]</span>. These safeguards demonstrate that good adaptation is selective adaptation.</p>
<h3 id="adaptivity-is-bounded-by-data-efficiency">5. Adaptivity is bounded by data efficiency</h3>
<p>Even with flexibility, heterogeneity, modularity, and selectivity in place, the scope of adaptation is fundamentally constrained by the availability of data. Fine-grained adaptation requires sufficient samples to estimate context-specific effects reliably. When data are scarce, adaptive systems risk inflating variance, capturing noise, or overfitting to idiosyncratic patterns. This limitation transcends individual methods and reflects a general statistical truth.</p>
<p>Meta-learning research illustrates this tension, as few-shot frameworks show both the promise of cross-task generalization and the sharp degradation that occurs when task diversity or sample size is insufficient <span class="citation" data-cites="TQNdTc18">[<a href="#ref-TQNdTc18" role="doc-biblioref">83</a>]</span>. Bayesian analyses of scaling laws for in-context learning formalize how the reliability of adaptation grows with data <span class="citation" data-cites="18PGQ3fOX">[<a href="#ref-18PGQ3fOX" role="doc-biblioref">84</a>]</span>. To mitigate these limits, modular reuse strategies have been developed, including adapter libraries <span class="citation" data-cites="UxVULYQ3">[<a href="#ref-UxVULYQ3" role="doc-biblioref">76</a>]</span> and modular domain experts. Practical applications, from medicine to recommendation systems, highlight the same lesson: adaptation cannot outpace the data that supports it.</p>
<h3 id="adaptivity-is-not-a-free-lunch">6. Adaptivity is not a free lunch</h3>
<p>Adaptivity brings benefits yet inevitably incurs costs. It can reduce bias and improve personalization, but at the expense of variance, computational resources, and stability. A model that adapts too readily may become fragile, inconsistent across runs, or difficult to interpret.</p>
<p>In statistical terms, this tension is captured by the classic bias and variance tradeoff <span class="citation" data-cites="bWiWgPrK">[<a href="#ref-bWiWgPrK" role="doc-biblioref">85</a>]</span>: increasing flexibility reduces systematic error but simultaneously increases estimation variance, especially in small-sample settings. Adaptive methods expand flexibility, which means they must also contend with this cost unless constrained by strong regularization or selectivity. In machine learning practice, these tradeoffs surface in multiple ways. Sparse expert models illustrate them clearly: while they scale efficiently, routing instability can cause experts to collapse or remain underused, undermining reliability <span class="citation" data-cites="BLSm3phD">[<a href="#ref-BLSm3phD" role="doc-biblioref">86</a>]</span>. Test-time adaptation can boost performance under distribution shift but may destabilize previously well-calibrated predictions. These examples show that adaptation is powerful but never free.</p>
<h3 id="when-adaptivity-fails-common-failure-modes">When Adaptivity Fails: Common Failure Modes</h3>
<p>The six principles describe when adaptation should succeed, but in practice, failures remain common. Understanding these failure modes is crucial for designing safeguards, as they reveal the vulnerabilities of adaptive methods when principles are ignored or misapplied. Failure does not imply that models lack adaptivity, but that adaptation proceeds in unstable or unjustified ways.</p>
<p><strong>Spurious adaptation.</strong> Models sometimes adapt to unstable or confounded features that correlate with outcomes only transiently. This phenomenon is closely related to shortcut learning in deep networks, where spurious correlations masquerade as useful signals <span class="citation" data-cites="4fnOdPts PKjSQOD">[<a href="#ref-PKjSQOD" role="doc-biblioref">72</a>,<a href="#ref-4fnOdPts" role="doc-biblioref">87</a>]</span>. Such adaptation may appear effective during training but fails catastrophically under distribution shift. The lesson here is that models must rely on stable signals of heterogeneity, not superficial correlations.</p>
<p><strong>Overfitting in low-data contexts.</strong> Fine-grained adaptation requires sufficient signal. When the available data are limited, adaptive models tend to inflate variance and personalize to noise rather than meaningful structure. Meta-learning research illustrates this tension: although few-shot methods aim to generalize with minimal samples, they often degrade sharply when task diversity is low or heterogeneity is weak <span class="citation" data-cites="TQNdTc18">[<a href="#ref-TQNdTc18" role="doc-biblioref">83</a>]</span>. This failure mode underscores the principle that data efficiency sets unavoidable limits on adaptivity.</p>
<p><strong>Modularity mis-specification.</strong> Although modularity can improve interpretability and transfer, poorly designed modules or unstable routing mechanisms can create new sources of error. Group-shift robustness studies reveal that when partitions are misaligned with true structure, adaptive pooling can worsen disparities across groups <span class="citation" data-cites="11l8svMmM">[<a href="#ref-11l8svMmM" role="doc-biblioref">88</a>]</span>. Similarly, analyses of mixture-of-experts models show that mis-specified routing can cause experts to collapse or remain underutilized <span class="citation" data-cites="BLSm3phD">[<a href="#ref-BLSm3phD" role="doc-biblioref">86</a>]</span>. These cases highlight that modularity is beneficial only when aligned with meaningful heterogeneity.</p>
<p><strong>Feedback loops.</strong> Adaptive models can also alter the very distributions they rely on, especially in high-stakes applications such as recommendation, hiring, or credit scoring. This creates feedback loops where bias is reinforced rather than corrected. For example, an adaptive recommender system that over-personalizes may restrict exposure to diverse content, reshaping user behavior in ways that amplify initial bias. The selective labels problem in algorithmic evaluation illustrates how unobserved counterfactuals complicate learning from adaptively collected data <span class="citation" data-cites="MGkiKe9y">[<a href="#ref-MGkiKe9y" role="doc-biblioref">89</a>]</span>. These examples show that adaptation must be evaluated with attention to long-term interactions, not only short-term accuracy.</p>
<p>Taken together, these failure modes illustrate that adaptivity is double-edged: the same mechanisms that enable personalization and robustness can also entrench bias, waste data efficiency, or destabilize models if not carefully designed and monitored.</p>
<div id="fig:adaptive-failures" class="fignos">
<figure>
<img src="images/adaptive_failures.png" style="width:80.0%" alt="Figure 4: Failure Modes of Context-Adaptive Models. (A) Mode Collapse: adaptive fits diverge from the true stable relationship. (B) Overfitting in Low-Data Contexts: adaptation follows noise rather than signal. (C) Modularity Mis-Specification: incorrect partitions obscure the true structure. (D) Feedback Loops: adaptive decisions reshape the very data they rely on." />
<figcaption aria-hidden="true"><span>Figure 4:</span> Failure Modes of Context-Adaptive Models.
(A) <strong>Mode Collapse</strong>: adaptive fits diverge from the true stable relationship.
(B) <strong>Overfitting in Low-Data Contexts</strong>: adaptation follows noise rather than signal.
(C) <strong>Modularity Mis-Specification</strong>: incorrect partitions obscure the true structure.
(D) <strong>Feedback Loops</strong>: adaptive decisions reshape the very data they rely on.</figcaption>
</figure>
</div>
<p>Having examined when and why adaptivity fails, we now synthesize these insights into a set of guiding principles for practical model design.</p>
<h3 id="synthesis-and-implications">Synthesis and Implications</h3>
<p>The principles and failure modes together provide a coherent framework for context-adaptive inference. Flexibility and heterogeneity define the capacity and justification for adaptation, ensuring that models have room to vary and meaningful signals to guide that variation. Modularity and selectivity organize adaptation into structured, interpretable, and disciplined forms, while data efficiency and tradeoffs impose the practical limits that prevent overreach. Failure modes remind us that these principles are not optional: neglecting them can lead to spurious adaptation, instability, or entrenched bias.</p>
<p>For practitioners, these insights translate into a design recipe. Begin by ensuring sufficient flexibility, but constrain it through modular structures that make adaptation interpretable and transferable. Seek out reliable signals of heterogeneity that justify adaptation, and incorporate explicit mechanisms of selectivity to guard against noise. Respect the limits imposed by data efficiency, recognizing that fine-grained personalization requires sufficient statistical support. Always weigh the tradeoffs explicitly, balancing personalization against stability, efficiency against interpretability, and short-term gains against long-term robustness. Evaluation criteria should extend beyond predictive accuracy to include calibration, fairness across subgroups, stability under distributional shift, and resilience to feedback loops.</p>
<p>By connecting classical statistical models with modern adaptive architectures, this framework provides both a conceptual map and practical guidance. It highlights that context-adaptive inference is not a single technique but a set of principles that shape how adaptivity should be designed and deployed. When applied responsibly, these principles enable models that are flexible yet disciplined, personalized yet robust, and efficient yet interpretable. Building on these conceptual principles, we next examine how context-adaptive inference can be made computationally and statistically efficient.</p>
<h3 id="context-aware-efficiency-principles-and-design">Context-Aware Efficiency Principles and Design</h3>
<p>The efficiency of context-adaptive methods hinges on several key design principles that balance computational tractability with statistical accuracy. These principles guide the development of methods that can scale to large datasets while maintaining interpretability and robustness.</p>
<p>Context-aware efficiency often relies on sparsity assumptions that limit the number of context-dependent parameters. This can be achieved through group sparsity, which encourages entire groups of context-dependent parameters to be zero simultaneously <span class="citation" data-cites="kfKaakAe">[<a href="#ref-kfKaakAe" role="doc-biblioref">90</a>]</span>, hierarchical regularization that applies different regularization strengths to different levels of context specificity <span class="citation" data-cites="kX2zf6UE PWhr4ijC">[<a href="#ref-kX2zf6UE" role="doc-biblioref">91</a>,<a href="#ref-PWhr4ijC" role="doc-biblioref">92</a>]</span>, and adaptive thresholding that dynamically adjusts sparsity levels based on context complexity.</p>
<p>Efficient context-adaptive inference can be achieved through computational strategies that allocate resources based on context. Early stopping terminates optimization early for contexts where convergence is rapid <span class="citation" data-cites="45Kr1uvy">[<a href="#ref-45Kr1uvy" role="doc-biblioref">93</a>]</span>, while context-dependent sampling uses different sampling strategies for different contexts <span class="citation" data-cites="18KQ6Vlb2">[<a href="#ref-18KQ6Vlb2" role="doc-biblioref">94</a>]</span>. Caching and warm-starting leverage solutions from similar contexts to accelerate optimization, particularly effective when contexts exhibit smooth variation <span class="citation" data-cites="W3XPrQXH">[<a href="#ref-W3XPrQXH" role="doc-biblioref">95</a>]</span>.</p>
<p>The design of context-aware methods often involves balancing computational efficiency with interpretability. Linear context functions are more interpretable but may require more parameters, while explicit context encoding improves interpretability but may increase computational cost. Local context modeling provides better interpretability but may be less efficient for large-scale applications. These trade-offs must be carefully considered based on the specific requirements of the application domain, as demonstrated in recent work on adaptive optimization methods <span class="citation" data-cites="1G9auqG3f">[<a href="#ref-1G9auqG3f" role="doc-biblioref">96</a>]</span>.</p>
<h3 id="adaptivity-is-bounded-by-data-efficiency-1">Adaptivity is bounded by data efficiency</h3>
<p>Recent work underscores a practical limit: stronger adaptivity demands more informative data per context. When contexts are fine-grained or rapidly shifting, the effective sample size within each context shrinks, and models risk overfitting local noise rather than learning stable, transferable structure. Empirically, few-shot behaviors in foundation models improve with scale yet remain sensitive to prompt composition and example distribution, indicating that data efficiency constraints persist even when capacity is abundant <span class="citation" data-cites="rtNEROOT 6Y4uv63y 61xQEdRS">[<a href="#ref-rtNEROOT" role="doc-biblioref">97</a>,<a href="#ref-6Y4uv63y" role="doc-biblioref">98</a>,<a href="#ref-61xQEdRS" role="doc-biblioref">99</a>]</span>. Complementary scaling studies quantify how performance depends on data, model size, and compute, implying that adaptive behaviors are ultimately limited by sample budgets per context and compute allocation <span class="citation" data-cites="6W1y3ZrT 5q4aFZMi 18PGQ3fOX">[<a href="#ref-18PGQ3fOX" role="doc-biblioref">84</a>,<a href="#ref-6W1y3ZrT" role="doc-biblioref">100</a>,<a href="#ref-5q4aFZMi" role="doc-biblioref">101</a>]</span>. In classical and modern pipelines alike, improving data efficiency hinges on pooling information across related contexts (via smoothness, structural coupling, or amortized inference) while enforcing capacity control and early stopping to avoid brittle, context-specific artifacts <span class="citation" data-cites="45Kr1uvy">[<a href="#ref-45Kr1uvy" role="doc-biblioref">93</a>]</span>. These considerations motivate interpretation methods that report not only attributions but also context-conditional uncertainty and stability, clarifying when adaptive behavior is supported by evidence versus when it reflects data scarcity.</p>
<h4 id="formalization-data-efficiency-constraints-on-adaptivity">Formalization: data-efficiency constraints on adaptivity</h4>
<p>Let contexts take values in a measurable space <span class="math inline">\(\mathcal{C}\)</span>, and suppose the per-context parameter is <span class="math inline">\(\theta(c) \in \Theta\)</span>.<br />
For observation <span class="math inline">\((x,y,c)\)</span>, consider a conditional model <span class="math inline">\(p_\theta(y \mid x,c)\)</span> with loss <span class="math inline">\(\ell(\theta; x,y,c)\)</span>.<br />
For a context neighborhood <span class="math inline">\(\mathcal{N}_\delta(c) = \{c&#39;: d(c,c&#39;) \le \delta\}\)</span> under metric <span class="math inline">\(d\)</span>, define the effective sample size available to estimate <span class="math inline">\(\theta(c)\)</span> by</p>
<p><span class="math display">\[
N_{\text{eff}}(c,\delta)
= \sum_{i=1}^n w_\delta(c_i,c),
\quad
w_\delta(c_i,c) \propto K\!\left(\frac{d(c_i,c)}{\delta}\right),
\quad
\sum_i w_\delta(c_i,c)=1,
\]</span></p>
<p>where <span class="math inline">\(K\)</span> is a kernel.<br />
A kernel-regularized estimator with smoothness penalty</p>
<p><span class="math display">\[
\mathcal{R}(\theta) = \int \|\nabla_c \theta(c)\|^2\,\mathrm{d}c
\]</span></p>
<p>solves</p>
<p><span class="math display">\[
\widehat{\theta}
= \arg\min_{\theta \in \Theta}
\frac{1}{n} \sum_{i=1}^n \ell(\theta; x_i, y_i, c_i)
+ \lambda\, \mathcal{R}(\theta).
\]</span></p>
<p>Assuming local Lipschitzness in <span class="math inline">\(c\)</span> and <span class="math inline">\(L\)</span>-smooth, <span class="math inline">\(\mu\)</span>-strongly convex risk in <span class="math inline">\(\theta\)</span>, a standard bias–variance decomposition yields for each component <span class="math inline">\(j\)</span></p>
<p><span class="math display">\[
\mathbb{E}\!\left[\|\widehat{\theta}j(c) - \theta_j(c)\|^2\right]
\lesssim
\underbrace{\frac{\sigma^2}{N_{\text{eff}}(c,\delta)}}_{\text{variance}}
+ \underbrace{\delta^{2\alpha}}_{\text{approx. bias}}
+ \underbrace{\lambda^2}_{\text{reg. bias}},
\quad \alpha &gt; 0,
\]</span></p>
<p>which exhibits the adaptivity–data trade-off: finer locality (small <span class="math inline">\(\delta\)</span>) increases resolution but reduces <span class="math inline">\(N_{\text{eff}}\)</span>, inflating variance.<br />
Practical procedures pick <span class="math inline">\(\delta\)</span> and <span class="math inline">\(\lambda\)</span> to balance these terms (e.g., via validation), and amortized approaches replace <span class="math inline">\(\theta(c)\)</span> by <span class="math inline">\(f_\phi(c)\)</span> with shared parameters <span class="math inline">\(\phi\)</span> to increase <span class="math inline">\(N_{\text{eff}}\)</span> through parameter sharing.
For computation, an early-stopped first-order method with step size <span class="math inline">\(\eta\)</span> and <span class="math inline">\(T(c)\)</span> context-dependent iterations satisfies (for smooth, strongly convex risk) the bound</p>
<p><span class="math display">\[
\mathcal{L}\!\big(\theta^{(T(c))}\big) - \mathcal{L}\!\big(\theta^{\star}\big)
\;\le\;
(1-\eta\mu)^{T(c)}\!\left(\mathcal{L}\!\big(\theta^{(0)}\big) - \mathcal{L}\!\big(\theta^{\star}\big)\right)
+ \frac{\eta L \sigma^2}{2\mu\,N_{\text{eff}}(c,\delta)}\,.
\]</span></p>
<p>linking compute allocation <span class="math inline">\(T(c)\)</span> and data availability <span class="math inline">\(N_{\text{eff}}(c,\delta)\)</span> to the attainable excess risk at context <span class="math inline">\(c\)</span>.</p>
<h3 id="formal-optimization-view-of-context-aware-efficiency">Formal optimization view of context-aware efficiency</h3>
<p>Let <span class="math inline">\(f_\phi : \mathcal{X} \times \mathcal{C} \to \mathcal{Y}\)</span> be a context-conditioned predictor with shared parameters <span class="math inline">\(\phi\)</span>.<br />
Given per-context compute budgets <span class="math inline">\(T(c)\)</span> and a global regularizer <span class="math inline">\(\Omega(\phi)\)</span>, a resource-aware training objective is</p>
<p><span class="math display">\[
\min_{\phi}\;
\mathbb{E}_{(x,y,c)\sim \mathcal{D}}
\ell\big(f_\phi(x,c),y\big)
+ \lambda\,\Omega(\phi)
\quad
\text{s.t.}\quad
\mathbb{E}_{c}\,\mathcal{C}\big(f_\phi; T(c), c\big) \le B,
\]</span></p>
<p>where <span class="math inline">\(\mathcal{C}(\cdot)\)</span> models compute or latency.<br />
The Lagrangian relaxation is</p>
<p><span class="math display">\[
\min_{\phi}\;
\mathbb{E}_{(x,y,c)}
\ell\big(f_\phi(x,c),y\big)
+ \lambda\,\Omega(\phi)
+ \gamma\,\mathbb{E}_{c}\,\mathcal{C}\big(f_\phi; T(c), c\big),
\]</span></p>
<p>which trades off accuracy and compute via <span class="math inline">\(\gamma\)</span>.<br />
For mixture-of-experts or sparsity-inducing designs, let <span class="math inline">\(\phi = (\phi_1, \ldots, \phi_M)\)</span> and define a gating function <span class="math inline">\(\pi_\phi(m \mid c)\)</span>.<br />
A compute-aware sparsity penalty can be written as</p>
<p><span class="math display">\[
\Omega(\phi)
= \sum_{m=1}^M \alpha_m\,\|\phi_m\|_2^2
+ \tau\, \mathbb{E}_{c} \sum_{m=1}^M \pi_\phi(m\mid c),
\]</span></p>
<p>encouraging few active modules per context.<br />
Under smoothness and strong convexity, the optimality conditions yield the KKT stationarity conditions</p>
<p><span class="math display">\[
\nabla_\phi \Big( \mathbb{E}\,\ell
+ \lambda\,\Omega
+ \gamma\,\mathbb{E}_c\,\mathcal{C} \Big)
= 0,
\quad
\gamma\,\big( \mathbb{E}_c\,\mathcal{C} - B \big) = 0,
\quad
\gamma \ge 0.
\]</span></p>
<p>This perspective clarifies that context-aware efficiency arises from jointly selecting representation sharing, per-context compute allocation <span class="math inline">\(T(c)\)</span>, and sparsity in active submodules subject to resource budgets.</p>
<p>Together, these efficiency principles and formal analyses bridge conceptual foundations with implementation. In the next section, we turn to explicit adaptive models that instantiate these ideas through structured parameterization and estimation.</p>
<h2 id="explicit-adaptivity-structured-estimation-of-fc">Explicit Adaptivity: Structured Estimation of <span class="math inline">\(f(c)\)</span></h2>
<p>In classical statistical modeling, all observations are typically assumed to share a common set of parameters. However, modern datasets often display significant heterogeneity across individuals, locations, or experimental conditions, making this assumption unrealistic in many real-world applications. To better capture such heterogeneity, recent approaches model parameters as explicit functions of observed context, formalized as <span class="math inline">\(\theta_i = f(c_i)\)</span>, where <span class="math inline">\(f\)</span> maps each context to a sample-specific parameter <span class="citation" data-cites="ugXwusl0">[<a href="#ref-ugXwusl0" role="doc-biblioref">1</a>]</span>.</p>
<p>A familiar example of explicit adaptivity is multi-task learning, where context is defined by task identity.
Traditional multi-task learning (left) assigns each task its own head on top of shared representations,
while context-flagged models (right) pass task identity directly as an input, enabling richer parameter sharing.
This illustrates how explicit conditioning on context variables can unify tasks within a single model and
provides an intuitive entry point to more general forms of explicit adaptivity (Figure <a href="#fig:mtl-context">5</a>).</p>
<div id="fig:mtl-context" class="fignos">
<figure>
<img src="images/mtl_context.png" style="width:75.0%" alt="Figure 5: Multi-task learning as explicit adaptivity. In traditional MTL (left), each task has its own head on top of shared layers. In context-flagged models (right), the task identity is provided as an input, enabling a shared model to adapt across tasks." />
<figcaption aria-hidden="true"><span>Figure 5:</span> Multi-task learning as explicit adaptivity. In traditional MTL (left), each task has its own head on top of shared layers. In context-flagged models (right), the task identity is provided as an input, enabling a shared model to adapt across tasks.</figcaption>
</figure>
</div>
<p>This section systematically reviews explicit adaptivity methods, with a focus on structured estimation of <span class="math inline">\(f(c)\)</span>. We begin by revisiting classical varying-coefficient models, which provide a conceptual and methodological foundation for modeling context-dependent effects. We then categorize recent advances in explicit adaptivity according to three principal strategies for estimating <span class="math inline">\(f(c)\)</span>: (1) smooth nonparametric models that generalize classical techniques, (2) structurally constrained models that incorporate domain-specific knowledge such as spatial or network structure, and (3) learned function approximators that leverage machine learning methods for high-dimensional or complex contexts. Finally, we summarize key theoretical developments and highlight promising directions for future research in this rapidly evolving field.</p>
<h3 id="classical-varying-coefficient-models-a-foundation">Classical Varying-Coefficient Models: A Foundation</h3>
<p>Varying-coefficient models (VCMs) are a foundational tool for modeling heterogeneity, as they allow model parameters to vary smoothly with observed context variables <span class="citation" data-cites="ugXwusl0 hRp04fhf">[<a href="#ref-ugXwusl0" role="doc-biblioref">1</a>,<a href="#ref-hRp04fhf" role="doc-biblioref">102</a>]</span>. In their original formulation, the regression coefficients are treated as nonparametric functions of low-dimensional covariates, such as time or age. The standard VCM takes the form</p>
<p><span class="math display">\[
y_i = \sum_{j=1}^{p} \beta_j(c_i) x_{ij} + \varepsilon_i
\]</span></p>
<p>where each <span class="math inline">\(\beta_j(c)\)</span> is an unknown smooth function, typically estimated using kernel smoothing, local polynomials, or penalized splines.</p>
<p>This approach provides greater flexibility than fixed-coefficient models and is widely used for longitudinal and functional data analysis. The assumption of smoothness makes estimation and theoretical analysis more tractable, but also imposes limitations. Classical VCMs work best when the context is low-dimensional and continuous. They may struggle with abrupt changes, discontinuities, or high-dimensional and structured covariates. In such cases, interpretability and accuracy can be compromised, motivating the development of a variety of modern extensions, which will be discussed in the following sections.</p>
<h3 id="advances-in-modeling-fc">Advances in Modeling <span class="math inline">\(f(c)\)</span></h3>
<p>Recent years have seen substantial progress in the modeling of <span class="math inline">\(f(c)\)</span>, the function mapping context to model parameters. These advances can be grouped into three major strategies: (1) smooth non-parametric models that extend classical flexibility; (2) structurally constrained approaches that encode domain knowledge such as spatial or network topology; and (3) high-capacity machine learning methods for high-dimensional, unstructured contexts. Each strategy addresses specific challenges in modeling heterogeneity, and together they provide a comprehensive toolkit for explicit adaptivity.</p>
<h4 id="smooth-non-parametric-models">Smooth Non-parametric Models</h4>
<p>This family of models generalizes the classical VCM by expressing <span class="math inline">\(f(c)\)</span> as a flexible, smooth function estimated with basis expansions and regularization. Common approaches include spline-based methods, local polynomial regression, and RKHS-based frameworks. For instance, developed a semi-nonparametric VCM using RKHS techniques for imaging genetics, enabling the model to capture complex nonlinear effects. Such methods are central to generalized additive models, supporting both flexibility and interpretability. Theoretical work has shown that penalized splines and kernel methods offer strong statistical guarantees in moderate dimensions, although computational cost and overfitting can become issues as the dimension of <span class="math inline">\(c\)</span> increases. These estimators occupy the lower-capacity but more interpretable end of the explicit adaptivity spectrum, forming a conceptual baseline for more complex architectures discussed below.</p>
<h4 id="structured-regularization-for-graphical-and-network-models">Structured Regularization for Graphical and Network Models</h4>
<p>The origins of structurally constrained models can be traced to early work on covariance selection. Dempster (1972) demonstrated that zeros in the inverse covariance matrix correspond directly to conditional independencies, introducing the principle that sparsity reflects structure <span class="citation" data-cites="1AAscRzye">[<a href="#ref-1AAscRzye" role="doc-biblioref">103</a>]</span>. This principle was formalized in Lauritzen’s (1996) influential monograph, which systematized probabilistic graphical models and showed how independence assumptions can be embedded into estimation procedures <span class="citation" data-cites="moNSKvBk">[<a href="#ref-moNSKvBk" role="doc-biblioref">104</a>]</span>. Together, these works established the conceptual foundation that explicit structure can guide inference in high-dimensional settings.</p>
<p>As high-dimensional data became common, scalable estimation procedures emerged to make these ideas practical. Meinshausen and Bühlmann (2006) proposed neighborhood selection, recasting graph recovery as a series of sparse regression problems that infer conditional dependencies node by node <span class="citation" data-cites="eFgbj4Kw">[<a href="#ref-eFgbj4Kw" role="doc-biblioref">105</a>]</span>. Shortly thereafter, Friedman, Hastie, and Tibshirani (2008) developed the graphical lasso, a convex penalized likelihood method that directly estimates sparse precision matrices <span class="citation" data-cites="m4KsbXUW">[<a href="#ref-m4KsbXUW" role="doc-biblioref">106</a>]</span>. These contributions showed that sparsity-inducing penalties could recover large network structures reliably, thereby providing concrete tools for estimating <span class="math inline">\(f(c)\)</span> when context corresponds to a structured dependency pattern such as a graph.</p>
<p>Building on these advances, later research recognized that networks themselves may vary across contexts. Guo, Levina, Michailidis, and Zhu (2011) introduced penalties that jointly estimate multiple graphical models, encouraging sparsity within each network while borrowing strength across related groups <span class="citation" data-cites="hxZIBmjM">[<a href="#ref-hxZIBmjM" role="doc-biblioref">107</a>]</span>. Danaher, Wang, and Witten (2014) extended this framework with the Joint Graphical Lasso, which balances shared structure and context-specific edges across multiple populations <span class="citation" data-cites="JDoK9thg">[<a href="#ref-JDoK9thg" role="doc-biblioref">108</a>]</span>. These developments illustrate how structured regularization transforms explicit adaptivity into a principled strategy: instead of estimating networks independently, one can pool information selectively across contexts (where context <span class="math inline">\(c\)</span> is the group or task identity), making the estimation of the parameter function <span class="math inline">\(f(c)\)</span> both interpretable and statistically efficient.</p>
<p><strong>Piecewise-Constant and Partition-Based Models.</strong>
Here, model parameters are allowed to remain constant within specific regions or clusters of the context space, rather than vary smoothly. Approaches include classical grouped estimators and modern partition models, which may learn changepoints using regularization tools like total variation penalties or the fused lasso. This framework is particularly effective for data with abrupt transitions or heterogeneous subgroups.</p>
<p>A key design principle is that explicit splits of the context space can emulate distinct tasks, clarifying where parameters should be shared or separated. By introducing hierarchical partitions, we can capture heterogeneity at multiple levels: sample-level variation within each context, and task-level switching across contexts. This perspective connects classical partition-based models with multi-task learning, highlighting how explicit splits of context define where parameters should be shared versus differentiated (Figure <a href="#fig:context-splits">6</a>).</p>
<div id="fig:context-splits" class="fignos">
<figure>
<img src="images/context_splits.png" style="width:75.0%" alt="Figure 6: Hierarchical splits of context enable multi-level adaptivity. Explicit adaptivity can partition the context space into piecewise models, with parameters indexed both by context c and task identity (i,j). Such splits allow sample-level heterogeneity to be captured within contexts, while high-level partitions mimic task boundaries and enable task switching." />
<figcaption aria-hidden="true"><span>Figure 6:</span> Hierarchical splits of context enable multi-level adaptivity. Explicit adaptivity can partition
the context space into piecewise models, with parameters indexed both by context <span class="math inline">\(c\)</span> and task
identity <span class="math inline">\((i,j)\)</span>. Such splits allow sample-level heterogeneity to be captured within contexts,
while high-level partitions mimic task boundaries and enable task switching.</figcaption>
</figure>
</div>
<p>A subtle but important point is that the boundary between “parametric” and “nonparametric” adaptivity is porous.
If we fit <strong>simple parametric models within each context</strong> – for observed contexts <span class="math inline">\(c\)</span> or latent subcontexts <span class="math inline">\(Z\)</span> – and then <strong>aggregate across contexts</strong>, the resulting conditional</p>
<p><span class="math display">\[
P(Y\mid X,C) \;=\; \int P(Y\mid X,C,Z)\, dP(Z\mid C)
\]</span></p>
<p>can display rich, multimodal behavior that looks nonparametric. In other words, <strong>global flexibility can emerge from compositional, context-specific parametrics</strong>.
When component families are identifiable (or suitably regularized) and the context-to-mixture map is constrained (e.g., smoothness/TV/sparsity over <span class="math inline">\(c\)</span>), the aggregate model remains estimable and interpretable while avoiding overflexible, ill-posed mixtures.</p>
<div id="fig:compositional-inference" class="fignos">
<figure>
<img src="images/compositional-inference.png" style="width:85.0%" alt="Figure 7: Compositional inference: nonparametric flexibility from parametric context-specific models. (A) Overall conditional P(Y \mid X, C). (B) Context-specific components P(Y \mid X, C, Z=z_i) for latent subgroups Z. (C) Recombination via marginalization \int_Z P(Y \mid X, C, Z). (D) Aggregated distribution showing how structured parametric pieces yield multimodal, nonparametric-like behavior." />
<figcaption aria-hidden="true"><span>Figure 7:</span> Compositional inference: nonparametric flexibility from parametric context-specific models.
(A) Overall conditional <span class="math inline">\(P(Y \mid X, C)\)</span>.
(B) Context-specific components <span class="math inline">\(P(Y \mid X, C, Z=z_i)\)</span> for latent subgroups <span class="math inline">\(Z\)</span>.
(C) Recombination via marginalization <span class="math inline">\(\int_Z P(Y \mid X, C, Z)\)</span>.
(D) Aggregated distribution showing how structured parametric pieces yield multimodal, nonparametric-like behavior.</figcaption>
</figure>
</div>
<p>This perspective motivates flexible function approximators: trees and neural networks can be read as learning either the <strong>context-to-mixture weights</strong> or <strong>local parametric maps</strong>, providing similar global flexibility with different inductive biases.</p>
<p><strong>Structured Regularization for Spatial, Graph, and Network Data.</strong>
When context has known spatial or network structure, regularization terms can promote similarity among neighboring coefficients or nodes. For example, spatially varying-coefficient models have been applied to problems in geographical analysis and econometrics, where local effects are expected to vary across adjacent regions <span class="citation" data-cites="3EfUtiJg LUKqKQYa">[<a href="#ref-3EfUtiJg" role="doc-biblioref">109</a>,<a href="#ref-LUKqKQYa" role="doc-biblioref">110</a>]</span>. On networked data, the network VCM of <span class="citation" data-cites="10QS2bf0y">[<a href="#ref-10QS2bf0y" role="doc-biblioref">111</a>]</span> generalizes these ideas by learning both the latent positions and the parameter functions on graphs, allowing the model to accommodate complex relational heterogeneity. Such structural constraints allow models to leverage domain knowledge, improving efficiency and interpretability where smooth models may struggle. These regularization principles can also be extended to temporal, hierarchical, or multilevel contexts, where smooth transitions or cross-level coupling may be encoded through Laplacian penalties or nested-group regularizers tailored to the structure of <span class="math inline">\(c\)</span>.</p>
<p>Beyond spatial and single-network constraints, Bayesian approaches allow explicit modeling of multiple related graphical models across contexts. Rather than estimating each network independently or pooling across all data, these methods place structured priors that encourage information sharing when appropriate. For example, <span class="citation" data-cites="721WoKJr">[<a href="#ref-721WoKJr" role="doc-biblioref">112</a>]</span> introduced Bayesian inference for GGMs with lattice structure, demonstrating how spatial priors can capture context-dependence across neighboring sites. Building on this idea, <span class="citation" data-cites="1Da8QJneg">[<a href="#ref-1Da8QJneg" role="doc-biblioref">113</a>]</span> proposed a Bayesian framework with a Markov random field prior and spike-and-slab formulation to learn when edges should be shared across sample groups, improving estimation and quantifying inter-context similarity. More recently, <span class="citation" data-cites="x5cbX2Cv">[<a href="#ref-x5cbX2Cv" role="doc-biblioref">114</a>]</span> extended these principles to covariate-dependent graph learning, where network structure varies smoothly with observed covariates. Their dual group spike-and-slab prior enables multi-level selection at node, covariate, and local levels, providing a flexible and interpretable framework for heterogeneous biological networks. Together, these advances illustrate how Bayesian structural priors make adaptivity explicit in graphical models, supporting both efficient estimation and scientific interpretability.</p>
<h4 id="learned-function-approximators">Learned Function Approximators</h4>
<p>As context dimensionality and data complexity grow, explicit smoothness assumptions become insufficient, motivating high-capacity learners that approximate <span class="math inline">\(f(c)\)</span> directly from data. A third class of methods is rooted in modern machine learning, leveraging high-capacity models to approximate <span class="math inline">\(f(c)\)</span> directly from data. These approaches are especially valuable when the context is high-dimensional or unstructured, where classical assumptions may no longer be sufficient.</p>
<p><strong>Tree-Based Ensembles.</strong>
Gradient boosting decision trees (GBDTs) and related ensemble methods are well suited to tabular and mixed-type data. A representative example is Tree Boosted Varying-Coefficient Models, introduced by Zhou and Hooker (2019), where GBDTs are applied to estimate context-dependent coefficient functions within a VCM framework <span class="citation" data-cites="LJ5esEGZ">[<a href="#ref-LJ5esEGZ" role="doc-biblioref">115</a>]</span>. This approach offers a useful balance among flexibility, predictive accuracy, and interpretability, while typically being easier to train and tune than deep neural networks. More recently, Zakrisson and Lindholm (2024) proposed a tree-based varying coefficient model that incorporates cyclic gradient boosting machines (CGBM). Their method enables dimension-wise early stopping and provides feature importance measures, thereby enhancing interpretability and offering additional regularization <span class="citation" data-cites="121VRWHxY">[<a href="#ref-121VRWHxY" role="doc-biblioref">116</a>]</span>.</p>
<p>Overall, tree-based VCMs achieve strong predictive performance and retain a model structure that lends itself to interpretation, particularly when combined with tools such as SHAP for explaining model outputs. A recent extension of this line of research is the Bayesian tree-based varying-coefficient model VCBART <span class="citation" data-cites="pxvmdyAr">[<a href="#ref-pxvmdyAr" role="doc-biblioref">117</a>]</span>. VCBART integrates the flexibility of Bayesian Additive Regression Trees (BART) into the varying-coefficient framework, allowing the estimation of complex effect modifiers without imposing restrictive functional assumptions or requiring intensive hyperparameter tuning. Compared with classical kernel or spline-based estimators, VCBART provides coherent uncertainty quantification and improved scalability for high-dimensional or multivariate covariate effects. Empirical studies in social and spatial applications show that VCBART effectively captures nonlinear context–response interactions, marking a promising step toward unifying Bayesian inference and ensemble learning within varying-coefficient modeling.</p>
<p><strong>Deep Neural Networks.</strong>
For contexts defined by complex, high-dimensional features such as images, text, or sequential data, deep neural networks offer unique advantages for modeling <span class="math inline">\(f(c)\)</span>. These architectures can learn adaptive, data-driven representations that capture intricate relationships beyond the scope of classical models. Applications include personalized medicine, natural language processing, and behavioral science, where outcomes may depend on subtle or latent features of the context.</p>
<p>The decision between these machine learning approaches depends on the specific characteristics of the data, the priority placed on interpretability, and computational considerations. Collectively, these advances have significantly broadened the scope of explicit adaptivity, making it feasible to model heterogeneity in ever more complex settings.</p>
<h3 id="key-theoretical-advances">Key Theoretical Advances</h3>
<p>The expanding landscape of varying-coefficient models (VCMs) has been supported by substantial theoretical progress, which secures the validity of flexible modeling strategies and guides their practical use. The nature of these theoretical results often reflects the core structural assumptions of each model class.</p>
<p><strong>Theory for Smooth Non-parametric Models.</strong>
For classical VCMs based on kernel smoothing, local polynomial estimation, or penalized splines, extensive theoretical work has characterized their convergence rates and statistical efficiency. Under standard regularity conditions, these estimators are known to achieve minimax optimality for function estimation in moderate dimensions <span class="citation" data-cites="ugXwusl0">[<a href="#ref-ugXwusl0" role="doc-biblioref">1</a>]</span>. More specifically, Lu, Zhang, and Zhu (2008) established both consistency and asymptotic normality for penalized spline estimators when using a sufficient number of knots and appropriate penalty terms <span class="citation" data-cites="Z6951tJe">[<a href="#ref-Z6951tJe" role="doc-biblioref">118</a>]</span>, enabling valid inference through confidence intervals and hypothesis testing. These results provide a solid theoretical foundation even in relatively complex modeling contexts.</p>
<p><strong>Theory for Structurally Constrained Models.</strong>
When discrete or network structure is incorporated into VCMs, theoretical analysis focuses on identifiability, regularization properties, and conditions for consistent estimation. For example, <span class="citation" data-cites="10QS2bf0y">[<a href="#ref-10QS2bf0y" role="doc-biblioref">111</a>]</span> provide non-asymptotic error bounds for estimators in network VCMs, demonstrating that consistency can be attained when the underlying graph topology satisfies certain connectivity properties. In piecewise-constant and partition-based models, results from change-point analysis and total variation regularization guarantee that abrupt parameter changes can be recovered accurately under suitable sparsity and signal strength conditions.</p>
<p><strong>Theory for High-Capacity and Learned Models.</strong>
The incorporation of machine learning models into VCMs introduces new theoretical challenges. For high-dimensional and sparse settings, oracle inequalities and penalized likelihood theory establish conditions for consistent variable selection and accurate estimation, as seen in methods based on boosting and other regularization techniques. In the context of neural network-based VCMs, the theory is still developing, with current research focused on understanding generalization properties and identifiability in non-convex optimization. This remains an active and important frontier for both statistical and machine learning communities.</p>
<p>These theoretical advances provide a rigorous foundation for explicit adaptivity, a wide range of complex and structured modeling scenarios.</p>
<h3 id="sparsity-and-incomplete-measurements-as-context">Sparsity and Incomplete Measurements as Context</h3>
<p>A central practical challenge in combining real-world datasets is inconsistent measurement: different cohorts or institutions often collect different subsets of features. One dataset may contain detailed laboratory values, another may focus on imaging or physiological measurements, and a third may emphasize clinical outcomes. If such cohorts are naively pooled, the resulting feature matrix is sparse and unbalanced. If incomplete samples are discarded, data efficiency collapses.</p>
<p>Context-adaptive models provide a natural resolution by treating <strong>measurement sparsity itself as context.</strong> Rather than ignoring missingness, the model learns to adjust its parameterization according to which features are observed. In effect, each measurement policy (labs-only, vitals-only, multimodal) defines a context, and explicit adaptivity allows estimation that respects these differences while still sharing information. This perspective reframes missingness from a nuisance into structured signal: it encodes which sources of evidence are available and how they should be combined. This perspective reframes missingness from a nuisance into structured signal: it encodes which sources of evidence are available and how they should be combined, reflecting ideas explored in recent multimodal learning frameworks that handle missing modalities <span class="citation" data-cites="Q3DSEqgH">[<a href="#ref-Q3DSEqgH" role="doc-biblioref">119</a>]</span>.</p>
<div id="fig:sparsity-context" class="fignos">
<figure>
<img src="images/measurement-sparsity-context.png" style="width:70.0%" alt="Figure 8: Patterns of missingness as context. Each dataset (e.g., cohort with labs, cohort with vitals, cohort with imaging) provides a different subset of measurements. Context-adaptive models allow integration by conditioning on measurement availability, enabling learning from fewer samples with more heterogeneous features." />
<figcaption aria-hidden="true"><span>Figure 8:</span> Patterns of missingness as context. Each dataset (e.g., cohort with labs, cohort with vitals, cohort with imaging) provides a different subset of measurements. Context-adaptive models allow integration by conditioning on measurement availability, enabling learning from fewer samples with more heterogeneous features.</figcaption>
</figure>
</div>
<p>Figure <a href="#fig:sparsity-context">8</a> illustrates this idea: each cohort contributes a different subset of measurements (lungs, labs, vitals), and explicit adaptivity enables integration across cohorts. By conditioning on measurement availability, we can achieve greater sample efficiency, learning from fewer individuals but with richer heterogeneous features.</p>
<p>Evaluation of missingness-as-context models should report <em>mask-stratified metrics</em>, including worst-group performance, following group-robust evaluation practice <span class="citation" data-cites="11l8svMmM PKjSQOD">[<a href="#ref-PKjSQOD" role="doc-biblioref">72</a>,<a href="#ref-11l8svMmM" role="doc-biblioref">88</a>]</span>. Robustness should be probed with <em>mask-shift stress tests</em>, training under one measurement policy and testing under another, to quantify degradation and the benefit of contextualization, as formalized in the Domain Adaptation under Missingness Shift (DAMS) setting <span class="citation" data-cites="lLSU1HNL PKjSQOD">[<a href="#ref-PKjSQOD" role="doc-biblioref">72</a>,<a href="#ref-lLSU1HNL" role="doc-biblioref">120</a>]</span>. When imputation is used, authors should assess <em>imputation realism</em> by holding out observed entries under realistic mask distributions and reporting MAE/RMSE and calibration for <span class="math inline">\(p(x_{\text{missing}}\mid x_{\text{observed}})\)</span> <span class="citation" data-cites="p6OGob17 twzF9qcj">[<a href="#ref-p6OGob17" role="doc-biblioref">121</a>,<a href="#ref-twzF9qcj" role="doc-biblioref">122</a>]</span>. For causal or estimation applications, conduct <em>ignorability sensitivity analyses</em>, contrasting MAR-based results with pattern-mixture or selection-model analyses under plausible MNAR mechanisms <span class="citation" data-cites="DfpD095S 11UHeGMyc">[<a href="#ref-DfpD095S" role="doc-biblioref">123</a>,<a href="#ref-11UHeGMyc" role="doc-biblioref">124</a>]</span>. Finally, include <em>ablations</em> that remove mask/indicator inputs—and, for trees, disable default-direction routing—to confirm that gains derive from modeling the mask signal rather than artifacts <span class="citation" data-cites="pTzEjoO6 T8dO7ymx">[<a href="#ref-pTzEjoO6" role="doc-biblioref">125</a>,<a href="#ref-T8dO7ymx" role="doc-biblioref">126</a>]</span>. Practical implementations of these ideas are widely available: <strong>GRU-D</strong> <span class="citation" data-cites="HZddfIob">[<a href="#ref-HZddfIob" role="doc-biblioref">127</a>]</span> and <strong>BRITS</strong> <span class="citation" data-cites="jPxFquZY">[<a href="#ref-jPxFquZY" role="doc-biblioref">128</a>]</span> provide mask- and time-aware sequence models, while <strong>GAIN</strong> <span class="citation" data-cites="twzF9qcj">[<a href="#ref-twzF9qcj" role="doc-biblioref">122</a>]</span> and <strong>VAEAC</strong> <span class="citation" data-cites="p6OGob17">[<a href="#ref-p6OGob17" role="doc-biblioref">121</a>]</span> offer open-source code for imputation under arbitrary masks. For tree ensembles, <strong>XGBoost</strong> supports sparsity-aware default-direction splits, making it straightforward to treat “NA” values as context without preprocessing <span class="citation" data-cites="8w9fI63O">[<a href="#ref-8w9fI63O" role="doc-biblioref">129</a>]</span>.</p>
<h3 id="context-aware-efficiency-principles-and-design-1">Context-Aware Efficiency Principles and Design</h3>
<p>The efficiency of context-adaptive methods hinges on several key design principles that balance computational tractability with statistical accuracy. These principles guide the development of methods that can scale to large datasets while maintaining interpretability and robustness.</p>
<p>One central principle is the use of sparsity assumptions to limit the number of context-dependent parameters. This can be achieved through group sparsity, which encourages entire groups of parameters to be zero simultaneously <span class="citation" data-cites="kfKaakAe">[<a href="#ref-kfKaakAe" role="doc-biblioref">90</a>]</span>, hierarchical regularization that applies different strengths of shrinkage to varying levels of context specificity <span class="citation" data-cites="PWhr4ijC">[<a href="#ref-PWhr4ijC" role="doc-biblioref">92</a>]</span>, and adaptive thresholding that dynamically adjusts sparsity levels in accordance with context complexity.</p>
<p>Efficiency can also be enhanced through computational strategies that allocate resources adaptively. Early stopping terminates optimization for contexts where convergence occurs rapidly <span class="citation" data-cites="45Kr1uvy">[<a href="#ref-45Kr1uvy" role="doc-biblioref">93</a>]</span>, while context-dependent sampling employs different sampling schemes across contexts <span class="citation" data-cites="18KQ6Vlb2">[<a href="#ref-18KQ6Vlb2" role="doc-biblioref">94</a>]</span>. Caching and warm-starting further accelerate optimization by leveraging solutions from similar contexts, particularly effective when contexts exhibit smooth variation <span class="citation" data-cites="W3XPrQXH">[<a href="#ref-W3XPrQXH" role="doc-biblioref">95</a>]</span>.</p>
<p>A further consideration is the balance between efficiency and interpretability. Linear context functions are highly interpretable but may require many parameters, while explicit context encodings improve transparency at the potential cost of higher computational overhead. Local context modeling provides fine-grained interpretability but may be less scalable to large applications. These trade-offs should be evaluated in light of application-specific requirements. For example, advanced adaptive optimizers like Adam can efficiently train complex, nonlinear models, but the resulting systems may be less interpretable than simpler alternatives <span class="citation" data-cites="1G9auqG3f">[<a href="#ref-1G9auqG3f" role="doc-biblioref">96</a>]</span>. In practice, such context-dependent computation appears in adaptive batching, per-context learning rates, and multi-fidelity optimization pipelines that dynamically adjust compute and precision depending on context complexity.</p>
<h3 id="synthesis-and-future-directions">Synthesis and Future Directions</h3>
<p>Selecting an appropriate modeling strategy for <span class="math inline">\(f(c)\)</span> involves weighing flexibility, interpretability, computational cost, and the extent of available domain knowledge. Learned function approximators, such as deep neural networks, offer unmatched capacity for modeling complex, high-dimensional relationships. However, classical smooth models and structurally constrained approaches often provide greater interpretability, transparency, and statistical efficiency. The choice of prior assumptions and the scalability of the estimation procedure are also central considerations in applied contexts.</p>
<p>Looking forward, several trends are shaping the field. One important direction is the integration of varying-coefficient models with foundation models from natural language processing and computer vision. By using pre-trained embeddings as context variables <span class="math inline">\(c_i\)</span>, it becomes possible to incorporate large amounts of prior knowledge and extend VCMs to multi-modal and unstructured data sources. Another active area concerns the principled combination of cross-modal contexts, bringing together information from text, images, and structured covariates within a unified VCM framework.</p>
<p>Advances in interpretability and visualization for high-dimensional or black-box coefficient functions are equally important. Developing tools that allow users to understand and trust model outputs is critical for the adoption of VCMs in sensitive areas such as healthcare and policy analysis.</p>
<p>Finally, closing the gap between methodological innovation and practical deployment remains a priority. Although the literature has produced many powerful variants of VCMs, practical adoption is often limited by the availability of software and the clarity of methodological guidance <span class="citation" data-cites="hRp04fhf">[<a href="#ref-hRp04fhf" role="doc-biblioref">102</a>]</span>. Continued investment in user-friendly implementations, open-source libraries, and empirical benchmarks will facilitate broader adoption and greater impact.</p>
<p>In summary, explicit adaptivity through structured estimation of <span class="math inline">\(f(c)\)</span> now forms a core paradigm at the interface of statistical modeling and machine learning. Future progress will focus not only on expanding the expressive power of these models, but also on making them more accessible, interpretable, and practically useful in real-world applications.</p>
<h2 id="implicit-adaptivity-emergent-contextualization-in-complex-models">Implicit Adaptivity: Emergent Contextualization in Complex Models</h2>
<p><strong>Introduction: From Explicit to Implicit Adaptivity.</strong></p>
<p>Traditional models often describe how parameters change by directly specifying a function of context, for example through expressions like <span class="math inline">\(\theta_i = f(c_i)\)</span>, where the link between context <span class="math inline">\(c_i\)</span> and parameters <span class="math inline">\(\theta_i\)</span> is fully explicit. In contrast, many modern machine learning systems adapt in fundamentally different ways. Large neural network architectures, particularly foundation models that are now central to state-of-the-art AI research <span class="citation" data-cites="1GbAsSOZV">[<a href="#ref-1GbAsSOZV" role="doc-biblioref">69</a>]</span>, exhibit forms of adaptation that do not arise from any predefined mapping. Instead, their flexibility emerges from the interaction between model structure and the breadth of training data—an effect we refer to as implicit adaptivity. They show a capacity for adaptation that does not arise from any predefined mapping. Instead, their flexibility emerges naturally from the structure of the model and the breadth of the data seen during training. This phenomenon is known as implicit adaptivity. This emergent phenomenon, referred to as implicit adaptivity, highlights how learning and inference can become intertwined within the model itself. Attention</p>
<p>Unlike explicit approaches, implicit adaptivity does not depend on directly mapping context to model parameters, nor does it always require context to be formally defined. Such models, by training on large and diverse datasets, internalize broad statistical regularities. As a result, they often display context-sensitive behavior at inference time, even when the notion of context is only implicit or distributed across the input. This capacity for emergent adaptation is especially prominent in foundation models, which can generalize to new tasks and domains without parameter updates, relying solely on the information provided within the input or prompt.</p>
<p>In this section, we offer a systematic review of the mechanisms underlying implicit adaptation. We first discuss the core architectural principles that support context-aware computation in neural networks. Next, we examine how meta-learning frameworks deliberately promote adaptation across diverse tasks. Finally, we focus on the advanced phenomenon of in-context learning in foundation models, which highlights the frontiers of implicit adaptivity in modern machine learning. Through this progression, we aim to clarify the foundations and significance of implicit adaptivity for current and future AI systems.</p>
<h3 id="foundations-of-implicit-adaptation">Foundations of Implicit Adaptation</h3>
<p>The capacity for implicit adaptation does not originate from a single mechanism, but reflects a range of capabilities grounded in fundamental principles of neural network design. Unlike approaches that adjust parameters by directly mapping context to coefficients, implicit adaptation emerges from the way information is processed within a model, even when the global parameters remain fixed. To provide a basis for understanding more advanced forms of adaptation, such as in-context learning, this section reviews the architectural components that enable context-aware computation. We begin with simple context-as-input models and then discuss the more dynamic forms of conditioning enabled by attention mechanisms.</p>
<h4 id="architectural-conditioning-via-context-inputs">Architectural Conditioning via Context Inputs</h4>
<p>In contrast to explicit parameter mapping, the simplest route to implicit adaptation is to feed context directly as part of the input. The simplest form of implicit adaptation appears in neural network models that directly incorporate context as part of their input. In models written as <span class="math inline">\(y_i = g([x_i, c_i]; \Phi)\)</span>, context features <span class="math inline">\(c_i\)</span> are concatenated with the primary features <span class="math inline">\(x_i\)</span>, and the mapping <span class="math inline">\(g\)</span> is determined by a single set of fixed global weights <span class="math inline">\(\Phi\)</span>. Even though these parameters do not change during inference, the network’s nonlinear structure allows it to capture complex interactions. As a result, the relationship between <span class="math inline">\(x_i\)</span> and <span class="math inline">\(y_i\)</span> can vary depending on the specific value of <span class="math inline">\(c_i\)</span>.</p>
<p>This basic yet powerful principle is central to many conditional prediction tasks. For example, personalized recommendation systems often combine a user embedding (as context) with item features to predict ratings. Similarly, in multi-task learning frameworks, shared networks learn representations conditioned on task or environment identifiers, which allows a single model to solve multiple related problems <span class="citation" data-cites="qfuiP7Hi">[<a href="#ref-qfuiP7Hi" role="doc-biblioref">130</a>]</span>.</p>
<h4 id="interaction-effects-and-attention-mechanisms">Interaction Effects and Attention Mechanisms</h4>
<p>Modern architectures go beyond simple input concatenation by introducing interaction layers that support richer context dependence. These can include feature-wise multiplications, gating modules, or context-dependent normalization. Among these innovations, the attention mechanism stands out as the foundation of the Transformer architecture <span class="citation" data-cites="rh7nCPVE">[<a href="#ref-rh7nCPVE" role="doc-biblioref">131</a>]</span>.</p>
<p>Attention allows a model to assign varying degrees of importance to different parts of an input sequence, depending on the overall context. In the self-attention mechanism, each element in a sequence computes a set of query, key, and value vectors. The model then evaluates the relevance of each element to every other element, and these relevance scores determine a weighted sum of the value vectors. This process enables the model to focus on the most relevant contextual information for each step in computation. The ability to adapt processing dynamically in this way is not dictated by explicit parameter functions, but emerges from the network’s internal organization. By enabling dynamic, input-dependent weighting, attention supports context-aware computation without altering global parameters, thereby setting the stage for advanced on-the-fly adaptation such as in-context learning.</p>
<h3 id="amortized-inference-and-meta-learning">Amortized Inference and Meta-Learning</h3>
<p>Moving beyond fixed architectures that implicitly adapt, another family of methods deliberately trains models to become efficient learners. These approaches, broadly termed meta-learning or “learning to learn,” distribute the cost of adaptation across a diverse training phase. As a result, models can make rapid, task-specific adjustments during inference. Rather than focusing on solving a single problem, these methods train models to learn the process of problem-solving itself. This perspective provides an important conceptual foundation for understanding the in-context learning capabilities of foundation models.</p>
<h4 id="amortized-inference">Amortized Inference</h4>
<p>Amortized inference represents a more systematic form of implicit adaptation. In this setting, a model learns a reusable function that enables rapid inference for new data points, effectively distributing the computational cost over the training phase. In traditional Bayesian inference, calculating the posterior distribution for each new data point is computationally demanding. Amortized inference addresses this challenge by training an “inference network” to approximate these calculations. A classic example is the encoder in a Variational Autoencoder (VAE), which is optimized to map high-dimensional observations directly to the parameters, such as mean and variance, of an approximate posterior distribution over a latent space <span class="citation" data-cites="EOUjThUk">[<a href="#ref-EOUjThUk" role="doc-biblioref">132</a>]</span>. The inference network thus learns a complex, black-box mapping from the data context to distributional parameters. Once learned, this mapping can be efficiently applied to any new input at test time, providing a fast feed-forward approximation to a traditionally costly inference process.</p>
<h4 id="meta-learning-learning-to-learn">Meta-Learning: Learning to Learn</h4>
<p>Meta-learning builds upon these ideas by training models on a broad distribution of related tasks. The explicit goal is to enable efficient adaptation to new tasks. Instead of optimizing performance for any single task, meta-learning focuses on developing a transferable adaptation strategy or a parameter initialization that supports rapid learning in novel settings <span class="citation" data-cites="GYqEQJ4V">[<a href="#ref-GYqEQJ4V" role="doc-biblioref">133</a>]</span>.</p>
<p>Gradient-based meta-learning frameworks such as Model-Agnostic Meta-Learning (MAML) illustrate this principle. In these frameworks, the model discovers a set of initial parameters that can be quickly adapted to a new task with only a small number of gradient updates <span class="citation" data-cites="JE5FRU4v">[<a href="#ref-JE5FRU4v" role="doc-biblioref">66</a>]</span>. Training proceeds in a nested loop: the inner loop simulates adaptation to individual tasks, while the outer loop updates the initial parameters to improve adaptability across tasks. As a result, the capacity for adaptation becomes encoded in the meta-learned parameters themselves. When confronted with a new task at inference, the model can rapidly achieve strong performance using just a few examples, without the need for a hand-crafted mapping from context to parameters. In this view, the capacity to adapt becomes encoded in the meta-learned parameters themselves, enabling rapid generalization from few examples without a hand-crafted map from context to coefficients and standing in clear contrast to explicit approaches.</p>
<h3 id="in-context-learning-in-foundation-models">In-Context Learning in Foundation Models</h3>
<p>The most powerful and, arguably, most enigmatic form of implicit adaptivity is in-context learning (ICL), an emergent capability of large-scale foundation models. This phenomenon has become a central focus of modern AI research, as it represents a significant shift in how models learn and adapt to new tasks. This section provides an expanded review of ICL, beginning with a description of the core phenomenon, then deconstructing the key factors that influence its performance, reviewing the leading hypotheses for its underlying mechanisms, and concluding with its current limitations and open questions.</p>
<h4 id="the-phenomenon-of-few-shot-in-context-learning">The Phenomenon of Few-Shot In-Context Learning</h4>
<p>First systematically demonstrated in large language models such as GPT-3 <span class="citation" data-cites="rtNEROOT">[<a href="#ref-rtNEROOT" role="doc-biblioref">97</a>]</span>, ICL is the ability of a model to perform a new task after being conditioned on just a few examples provided in its input prompt. Critically, this adaptation occurs entirely within a single forward pass, without any updates to the model’s weights. For instance, a model can be prompted with a few English-to-French translation pairs and then successfully translate a new word, effectively learning the task on the fly. This capability supports a broad range of applications, including few-shot classification, following complex instructions, and even inducing and applying simple algorithms from examples. Subsequent work has shown that the ability to generalize from few in-context examples can itself be enhanced through meta-training. MetaICL explicitly trains models across diverse meta-tasks, teaching them to infer and adapt within context at test time without gradient updates, thereby strengthening the implicit adaptability of large language models <span class="citation" data-cites="qvjULhAd">[<a href="#ref-qvjULhAd" role="doc-biblioref">134</a>]</span>.</p>
<h4 id="deconstructing-icl-key-influencing-factors">Deconstructing ICL: Key Influencing Factors</h4>
<p>The effectiveness of ICL is not guaranteed and depends heavily on several interacting factors, which have been the subject of extensive empirical investigation.</p>
<p><strong>The Role of Scale.</strong>
A critical finding is that ICL is an emergent ability that appears only after a model surpasses a certain threshold in scale (in terms of parameters, data, and computation). Recent work has shown that larger models do not just improve quantitatively at ICL; they may also learn in qualitatively different ways, suggesting that scale enables a fundamental shift in capability rather than a simple performance boost <span class="citation" data-cites="6Y4uv63y">[<a href="#ref-6Y4uv63y" role="doc-biblioref">98</a>]</span>.</p>
<p><strong>Prompt Engineering and Example Selection.</strong>
The performance of ICL is highly sensitive to the composition of the prompt. The format, order, and selection of the in-context examples can dramatically affect the model’s output. Counterintuitively, research has shown that the distribution of the input examples, rather than the correctness of their labels, often matters more for effective ICL. This suggests that the model is primarily learning a task format or an input-output mapping from the provided examples, rather than learning the underlying concepts from the labels themselves <span class="citation" data-cites="61xQEdRS">[<a href="#ref-61xQEdRS" role="doc-biblioref">99</a>]</span>.</p>
<h4 id="hypothesized-mechanisms-how-does-icl-work">Hypothesized Mechanisms: How Does ICL Work?</h4>
<p>The underlying mechanisms that enable ICL are not fully understood and remain an active area of research. Several leading hypotheses have emerged, viewing ICL through the lenses of meta-learning, Bayesian inference, and specific architectural components.</p>
<p><strong>ICL as Implicit Meta-Learning.</strong>
The most prominent theory posits that transformers learn to implement general-purpose learning algorithms within their forward pass. During pre-training on vast and diverse datasets, the model is exposed to a multitude of tasks and patterns. This process is thought to implicitly train the model as a meta-learner, allowing it to recognize abstract task structures within a prompt and then execute a learned optimization process on the provided examples to solve the task for a new query <span class="citation" data-cites="16Xv40Ngd gy3ao6P6">[<a href="#ref-16Xv40Ngd" role="doc-biblioref">135</a>,<a href="#ref-gy3ao6P6" role="doc-biblioref">136</a>]</span>.</p>
<p><strong>ICL as Implicit Bayesian Inference.</strong>
A complementary and powerful perspective understands ICL as a form of implicit Bayesian inference. In this view, the model learns a broad prior over a large class of functions during its pre-training phase. The in-context examples provided in the prompt act as evidence, which the model uses to perform a Bayesian update, resulting in a posterior predictive distribution for the final query. This framework provides a compelling explanation for how models can generalize from very few examples <span class="citation" data-cites="LybHVzmd">[<a href="#ref-LybHVzmd" role="doc-biblioref">137</a>]</span>. A complementary theoretical development interprets in-context learning as a rational adaptation process. From a Bayesian decision-theoretic standpoint, transformers can be viewed as implicitly balancing expected loss with strategy complexity, thereby achieving near-optimal adaptation under computational constraints <span class="citation" data-cites="Hhj8Woby">[<a href="#ref-Hhj8Woby" role="doc-biblioref">138</a>]</span>. This rational framing connects implicit adaptivity with classical principles of statistical inference.</p>
<p><strong>The Role of Induction Heads.</strong>
From a more mechanistic, architectural perspective, researchers have identified specific attention head patterns, dubbed “induction heads,” that appear to be crucial for ICL. These specialized heads are hypothesized to form circuits that can scan the context for repeated patterns and then copy or complete them, providing a basic mechanism for pattern completion and generalization from in-context examples <span class="citation" data-cites="m1iAIGu4">[<a href="#ref-m1iAIGu4" role="doc-biblioref">139</a>]</span>. Extending this mechanistic line, Dherin et al. (2025) demonstrate that stacking self-attention and MLP layers allows transformers to implicitly update internal representations during a single forward pass, effectively realizing dynamic context-specific weight adjustments without explicit training <span class="citation" data-cites="IncFoA7e">[<a href="#ref-IncFoA7e" role="doc-biblioref">140</a>]</span>. Such implicit internal updates offer a concrete mechanistic account of how context-dependent behavior arises.</p>
<h4 id="limitations-and-open-questions">Limitations and Open Questions</h4>
<p>Despite its remarkable capabilities, ICL faces significant limitations with respect to transparency, explicit control, and robustness. The adaptation process is opaque, making it difficult to debug or predict failure modes. Furthermore, performance can be brittle and highly sensitive to small changes in the prompt. As summarized in recent surveys, key open questions include developing a more complete theoretical understanding of ICL, improving its reliability, and establishing methods for controlling its behavior in high-stakes applications <span class="citation" data-cites="1EkVeYD9V">[<a href="#ref-1EkVeYD9V" role="doc-biblioref">70</a>]</span>.</p>
<h3 id="theoretical-bridges-between-varying-coefficient-models-and-in-context-learning">Theoretical Bridges Between Varying-Coefficient Models and In-Context Learning</h3>
<p>Recent theoretical work has uncovered deep connections between classical varying-coefficient models and the mechanisms underlying in-context learning in transformers.
Although these approaches arise from different traditions — one grounded in semi-parametric statistics, the other in large-scale deep learning — they can implement strikingly similar estimators.
This section formalizes these parallels and reviews key theoretical results establishing these bridges.</p>
<h4 id="varying-coefficient-models-as-kernel-regression">Varying-Coefficient Models as Kernel Regression</h4>
<p>Consider a semi-parametric varying-coefficient model in which each observation is governed by a parameter vector <span class="math inline">\(\theta_i\)</span> that depends smoothly on context <span class="math inline">\(c_i\)</span>.
For a new query context <span class="math inline">\(c^\ast\)</span>, the parameter estimate is obtained by solving a locally weighted likelihood problem:</p>
<p><span class="math display">\[
\widehat{\theta}(c^\ast)
= \arg\max_{\theta} \sum_{i=1}^n K_\lambda(c_i, c^\ast)\,\ell(x_i; \theta),
\]</span></p>
<p>where <span class="math inline">\(K_\lambda\)</span> is a kernel function that measures similarity between contexts and <span class="math inline">\(\ell\)</span> is the log-likelihood.</p>
<p>For regression with squared loss, this reduces to kernel ridge regression in the context space.
Let <span class="math inline">\(y = (y_1,\dots,y_n)^\top\)</span> and <span class="math inline">\(K \in \mathbb{R}^{n \times n}\)</span> be the Gram matrix with <span class="math inline">\(K_{ij} = k(c_i, c_j)\)</span>.
The prediction at <span class="math inline">\(c^\ast\)</span> is</p>
<p><span class="math display">\[
\widehat{y}(c^\ast) = k(c^\ast)^\top (K + \lambda I)^{-1} y,
\]</span></p>
<p>where <span class="math inline">\(k(c^\ast) = (k(c^\ast, c_1), \ldots, k(c^\ast, c_n))^\top\)</span>.
This expression highlights that varying-coefficient models perform kernel smoothing in the context space: nearby observations in context have greater influence on the parameter estimates at <span class="math inline">\(c^\ast\)</span>.</p>
<p>Equivalently, the fitted model can be written as</p>
<p><span class="math display">\[
\widehat{f}(x^\ast, c^\ast) = \sum_{i=1}^n \alpha_i(c^\ast)\, y_i,
\]</span></p>
<p>where <span class="math inline">\(\alpha_i(c^\ast)\)</span> are normalized kernel weights determined entirely by the context similarities and the regularization parameter <span class="math inline">\(\lambda\)</span>.</p>
<h4 id="transformers-as-ridge-and-kernel-regressors-in-context">Transformers as Ridge and Kernel Regressors In-Context</h4>
<p>A parallel line of research has demonstrated that transformers trained on simple regression tasks can learn to perform ridge or kernel regression entirely within their forward pass, without any explicit supervision to do so.</p>
<p>Akyürek et al. (2022) show that for linear regression tasks, transformers can learn to implement the ridge regression estimator</p>
<p><span class="math display">\[
\widehat{w} = (X^\top X + \lambda I)^{-1} X^\top y
\]</span></p>
<p>directly from a sequence of in-context examples. Each example <span class="math inline">\((x_i, y_i)\)</span> is represented as a token, and the query token attends to the support tokens to compute the prediction for <span class="math inline">\(x^\ast\)</span>; the attention mechanism learns to encode the solution to the regression problem <span class="citation" data-cites="16QjxmbmC">[<a href="#ref-16QjxmbmC" role="doc-biblioref">141</a>]</span>.</p>
<p>Building on this finding, von Oswald et al. (2023) show that gradient-based training of transformers over distributions of regression tasks leads them to perform in-context gradient descent, effectively realizing kernel regression with the learned attention kernel serving as <span class="math inline">\(k(c_i, c_j)\)</span> <span class="citation" data-cites="V6cbeqie">[<a href="#ref-V6cbeqie" role="doc-biblioref">142</a>]</span>. Garg et al. (2023) further analyze which function classes can be learned in-context, demonstrating that transformers can approximate a wide family of kernel smoothers when trained on synthetic regression tasks <span class="citation" data-cites="R7y5TKp9">[<a href="#ref-R7y5TKp9" role="doc-biblioref">143</a>]</span>.</p>
<p>Dai et al. (2023) provide a complementary theoretical view, arguing that transformers can implicitly implement compositional function families through their attention layers, and that in-context learning arises naturally from this functional representation <span class="citation" data-cites="16Xv40Ngd">[<a href="#ref-16Xv40Ngd" role="doc-biblioref">135</a>]</span>.</p>
<p>Finally, Reuter et al. (2025) propose a compelling Bayesian interpretation: transformers trained under in-context learning can perform full Bayesian inference for common statistical models such as generalized linear models and latent factor models. Concretely, they train transformers to infer complex posterior distributions in context, showing that the in-context forward pass can approximate posterior sampling comparable to MCMC or variational inference methods <span class="citation" data-cites="1BOhEiiMt">[<a href="#ref-1BOhEiiMt" role="doc-biblioref">144</a>]</span>.</p>
<p>In all these cases, the support set within the prompt plays an analogous role to the neighborhood in context space in varying-coefficient models. The query token’s prediction is formed by aggregating information from the support tokens via learned similarity weights, realized by the attention mechanism rather than an explicitly defined kernel function.</p>
<h4 id="synthesis-two-paths-to-the-same-estimators">Synthesis: Two Paths to the Same Estimators</h4>
<p>Taken together, these results reveal a common form:</p>
<p><span class="math display">\[
\widehat{f}(x^\ast, c^\ast) = \sum_{i=1}^n \alpha_i(c^\ast)\, y_i,
\]</span></p>
<p>where the weights <span class="math inline">\(\alpha_i(c^\ast)\)</span> depend on the relationship between the query context <span class="math inline">\(c^\ast\)</span> and support contexts <span class="math inline">\(\{c_i\}\)</span>.</p>
<ul>
<li>In varying-coefficient models, <span class="math inline">\(\alpha_i(c^\ast)\)</span> are determined explicitly by a user-chosen kernel <span class="math inline">\(K_\lambda\)</span>.</li>
<li>In transformers, <span class="math inline">\(\alpha_i(c^\ast)\)</span> emerge implicitly from the learned attention patterns and internal computations after pretraining.</li>
</ul>
<p>Both perspectives yield estimators of the same functional form, with explicit kernel weighting in VCMs and learned attention weighting in transformers. This correspondence motivates a unified view of context-adaptive inference, combining the interpretability of explicit modeling with the flexibility and scale of implicit computation. This bridge motivates a unified framework for studying context-adaptive inference: explicit methods provide interpretability and structure, while implicit methods provide flexibility and scalability. Understanding how these two meet offers a promising path toward adaptive, interpretable models at scale. This unified perspective is also extending to structured and tabular domains. TabICL introduces a foundation model architecture for large-scale tabular data, showing that in-context learning can efficiently scale to structured datasets via column-row attention mechanisms <span class="citation" data-cites="1DBdzlHLI">[<a href="#ref-1DBdzlHLI" role="doc-biblioref">145</a>]</span>. These results suggest that implicit adaptivity generalizes beyond text or vision into the broader landscape of structured scientific data.</p>
<h3 id="comparative-synthesis-implicit-versus-explicit-adaptivity">Comparative Synthesis: Implicit versus Explicit Adaptivity</h3>
<p>Implicit and explicit strategies reflect two complementary philosophies for modeling heterogeneity, each with distinct strengths and trade-offs. The optimal choice between these approaches depends on the goals of analysis, the structure and scale of available data, and the need for interpretability or regulatory compliance in the application domain.</p>
<p><strong>Implicit Adaptivity.</strong>
The principal advantage of implicit methods lies in their remarkable flexibility and scalability. Leveraging large-scale pre-training on diverse datasets, these models can effectively adapt to high-dimensional and unstructured contexts, such as raw text, images, or other complex sensory data, where explicitly specifying a context function <span class="math inline">\(f(c)\)</span> is infeasible. Because adaptation is performed internally during the model’s forward pass, inference is both rapid and adaptable. However, the mechanisms underlying this adaptability are typically opaque, making it challenging to interpret or control the model’s decision process. In applications like healthcare or autonomous systems, this lack of transparency can hinder trust, validation, and responsible deployment.</p>
<p><strong>Explicit Adaptivity.</strong>
In contrast, explicit models provide direct, interpretable mappings from context to parameters through functions such as <span class="math inline">\(f(c)\)</span>. This structure supports clear visualization, statistical analysis, and the formulation of scientific hypotheses. It also enables more direct scrutiny and control of the model’s reasoning. Nevertheless, explicit methods rely heavily on domain expertise to specify an appropriate functional form, and may struggle to accommodate unstructured or highly complex context spaces. If the assumed structure is misspecified, the model’s performance and generalizability can be severely limited.</p>
<p>In summary, these two paradigms illustrate a fundamental trade-off between expressive capacity and transparent reasoning. Practitioners should carefully weigh these considerations, often choosing or blending approaches based on the unique demands of the task. For clarity, a comparative table or figure can further highlight the strengths and limitations of each strategy across various real-world applications.</p>
<h3 id="open-challenges-and-the-motivation-for-interpretability">Open Challenges and the Motivation for Interpretability</h3>
<p>The rise of powerful implicit adaptation methods, particularly in-context learning, raises critical open research questions regarding their diagnosis, control, and reliability. As these models are deployed in increasingly high-stakes applications, understanding their failure modes is not just an academic exercise but a practical necessity <span class="citation" data-cites="1GbAsSOZV">[<a href="#ref-1GbAsSOZV" role="doc-biblioref">69</a>]</span>. It is important to develop systematic methods for assessing when and why in-context learning is likely to fail, and to create techniques for interpreting and, where possible, steering the adaptation process. Prompting strategies such as chain-of-thought demonstrate that structured context can sometimes steer internal computation, providing limited but useful handles on model behavior <span class="citation" data-cites="1FWgzslSV">[<a href="#ref-1FWgzslSV" role="doc-biblioref">146</a>]</span>. A thorough understanding of the theoretical limits and practical capabilities of implicit adaptivity remains a central topic for ongoing research.</p>
<p>These considerations motivate a growing search for techniques that can make the adaptation process more transparent by “making the implicit explicit.” Such methods aim to bridge the gap between the powerful but opaque capabilities of implicit models and the need for trustworthy, reliable AI. This research can be broadly categorized into several areas, including post-hoc interpretability approaches that seek to explain individual predictions <span class="citation" data-cites="OsqhaAcF">[<a href="#ref-OsqhaAcF" role="doc-biblioref">147</a>]</span>, surrogate modeling where a simpler, interpretable model is trained to mimic the complex model’s behavior, and strategies for extracting modular structure from trained models. A prime example of the latter is the line of work probing language models to determine if they have learned factual knowledge in a structured, accessible way <span class="citation" data-cites="1EfmL7jzn">[<a href="#ref-1EfmL7jzn" role="doc-biblioref">148</a>]</span>. By surfacing the latent structure inside these systems, researchers can enhance trust, promote modularity, and improve the readiness of adaptive models for deployment in real-world settings. This line of work provides a conceptual transition to subsequent sections, which explore the integration of interpretability with adaptive modeling.</p>
<h2 id="toward-explicit-modeling-of-implicit-adaptivity-local-models-surrogates-and-post-hoc-approximations">Toward Explicit Modeling of Implicit Adaptivity: Local Models, Surrogates and Post Hoc Approximations</h2>
<h3 id="motivation">Motivation</h3>
<p>Building on the prior discussion of implicit adaptivity, this section examines methods that expose, approximate, or control those adaptive mechanisms.<br />
Implicit adaptivity allows powerful models, including foundation models, to adjust behavior without explicitly representing a mapping from context to parameters <span class="citation" data-cites="1GbAsSOZV">[<a href="#ref-1GbAsSOZV" role="doc-biblioref">69</a>]</span>. This flexibility obscures the underlying mechanisms of adaptation, hindering modular reuse and systematic auditing. Making adaptivity explicit improves alignment with downstream goals, enables modular composition, and supports debugging and error attribution. It also fits the call for a more rigorous science of interpretability with defined objectives and evaluation criteria <span class="citation" data-cites="Cq4JGOGX 8C0DuyuD">[<a href="#ref-Cq4JGOGX" role="doc-biblioref">149</a>,<a href="#ref-8C0DuyuD" role="doc-biblioref">150</a>]</span>.<br />
This chapter reviews practical approaches for surfacing structure, the assumptions they rely on, and how to evaluate their faithfulness and utility.</p>
<p><strong>From Implicit to Explicit Adaptivity</strong><br />
Implicit adaptivity is hidden, flexible, and hard to audit, while explicit adaptivity surfaces modular structure that is structured, auditable, and controllable. The transition highlights three key trade-offs developed in this section: <strong>Fidelity vs. Interpretability</strong>, <strong>Local vs. Global Scope</strong>, and <strong>Approximation vs. Control</strong>.</p>
<div id="fig:implicit-to-explicit" class="fignos">
<figure>
<img src="images/explicit_from_implicit.png" style="width:80.0%" alt="Figure 9: From Implicit to Explicit Adaptivity. A black-box model (left) represents implicit adaptation, which is hidden and opaque. Making adaptivity explicit (right) exposes structured components that can be inspected and controlled. The axes below highlight the trade-offs between fidelity and interpretability, local and global scope, and approximation and control." />
<figcaption aria-hidden="true"><span>Figure 9:</span> From Implicit to Explicit Adaptivity. A black-box model (left) represents implicit adaptation, which is hidden and opaque. Making adaptivity explicit (right) exposes structured components that can be inspected and controlled. The axes below highlight the trade-offs between fidelity and interpretability, local and global scope, and approximation and control.</figcaption>
</figure>
</div>
<h3 id="approaches">Approaches</h3>
<p>Efforts to make implicit adaptation explicit span complementary strategies that differ in assumptions, granularity, and computational cost. We group them into six families:</p>
<ol type="1">
<li>surrogate modeling for local approximation,<br />
</li>
<li>prototype- and neighbor-based reasoning,<br />
</li>
<li>diagnostics for amortized inference,<br />
</li>
<li>disentanglement and bottleneck methods,<br />
</li>
<li>parameter extraction and probing, and<br />
</li>
<li>emerging approaches that leverage large language models as post-hoc explainers.</li>
</ol>
<h4 id="surrogate-modeling">Surrogate Modeling</h4>
<p>This line of work approximates a black-box <span class="math inline">\(h(x,c)\)</span> with an interpretable model in a small neighborhood, so that local behavior and a local view of <span class="math inline">\(f(c)\)</span> can be inspected. A formal template is</p>
<p><span class="math display">\[
\hat{g}_{x_0,c_0} = \arg\min_{g \in \mathcal{G}} \, \mathbb{E}_{(x,c) \sim \mathcal{N}_{x_0,c_0}} \left[ \ell\big(h(x,c), g(x,c)\big) \right] + \Omega(g),
\]</span></p>
<p>where <span class="math inline">\(\mathcal N_{x_0,c_0}\)</span> defines a locality (e.g., kernel weights), <span class="math inline">\(\ell\)</span> measures fidelity, and <span class="math inline">\(\Omega\)</span> controls complexity. Where <span class="math inline">\(\mathcal{G}\)</span> denotes a restricted hypothesis class, often composed of linear or other low-complexity functions chosen to enhance interpretability. A convenient local goodness-of-fit is</p>
<p><span class="math display">\[
R^2_{\text{local}}
= 1 - \frac{\sum_i w_i\,\big(h_i - g_i\big)^2}{\sum_i w_i\,\big(h_i - \bar h\big)^2},
\qquad
w_i \propto \kappa\!\big((x_i,c_i),(x_0,c_0)\big).
\]</span></p>
<p>LIME perturbs inputs and fits a locality-weighted linear surrogate <span class="citation" data-cites="uXcqmE5X">[<a href="#ref-uXcqmE5X" role="doc-biblioref">151</a>]</span>; SHAP / DeepSHAP provide additive attributions based on Shapley values <span class="citation" data-cites="rkxTwVjs">[<a href="#ref-rkxTwVjs" role="doc-biblioref">152</a>]</span>. Integrated Gradients and DeepLIFT link attribution to path-integrated sensitivity or reference-based contributions <span class="citation" data-cites="11Dolfu34 BvgiYaxe">[<a href="#ref-11Dolfu34" role="doc-biblioref">153</a>,<a href="#ref-BvgiYaxe" role="doc-biblioref">154</a>]</span>. These methods are most reliable when the model is near-linear in the chosen neighborhood and perturbations remain near the data manifold; consequently, a rigorous analysis involves stating the neighborhood definition, reporting the surrogate’s goodness-of-fit, and assessing stability across seeds and baselines.</p>
<h4 id="prototype-and-nearest-neighbor-methods">Prototype and Nearest-Neighbor Methods</h4>
<p>Here, a decision is grounded by reference to similar cases in representation space, which supports case-based explanations and modular updates. ProtoPNet learns a library of visual prototypes to implement “this looks like that” reasoning <span class="citation" data-cites="jO5B8YvG">[<a href="#ref-jO5B8YvG" role="doc-biblioref">155</a>]</span>. Deep <span class="math inline">\(k\)</span>-nearest neighbors audits predictions by querying neighbors in activation space and can flag distribution shift <span class="citation" data-cites="KvoFCtdS">[<a href="#ref-KvoFCtdS" role="doc-biblioref">156</a>]</span>. Influence functions link a prediction to influential training points for data-centric debugging <span class="citation" data-cites="167ItAuU7">[<a href="#ref-167ItAuU7" role="doc-biblioref">157</a>]</span>. This line of work connects naturally to exemplar models and contextual bandits, where decisions are justified via comparisons to context-matched exemplars. Reports include prototype coverage and diversity, neighbor quality checks, and the effect of editing prototypes or influential examples. These prototype-based approaches make local adaptation explicit by grounding predictions in reference cases, bridging the gap between black-box models and case-based reasoning frameworks.</p>
<h4 id="amortization-diagnostics">Amortization Diagnostics</h4>
<p>For amortized inference systems (e.g., VAEs), the encoder <span class="math inline">\(q_{\phi}(\theta\mid x)\)</span> can be treated as an implicit <span class="math inline">\(f(c)\)</span>. Diagnostics measure amortization gaps and identify suboptimal inference or collapse <span class="citation" data-cites="NGhi4Vf3">[<a href="#ref-NGhi4Vf3" role="doc-biblioref">158</a>]</span>. Useful outputs include calibration under shift and posterior predictive checks, together with ablations that vary encoder capacity or add limited iterative refinement. This clarifies when the learned mapping is faithful versus when it underfits the target posterior. Such diagnostics mirror classical checks for approximate Bayesian inference, where amortization gaps quantify the discrepancy between learned and exact posteriors.</p>
<h4 id="disentangled-and-bottlenecked-representations">Disentangled and Bottlenecked Representations</h4>
<p>While amortization diagnostics target model faithfulness, disentanglement aims to expose interpretable subspaces aligned with distinct contextual factors. The aim is to expose factors that align with distinct contextual causes, making changes traceable and controllable. <span class="math inline">\(\beta\)</span>-VAE encourages more factorized latents <span class="citation" data-cites="INMMN2Vz">[<a href="#ref-INMMN2Vz" role="doc-biblioref">159</a>]</span>, while the Deep Variational Information Bottleneck promotes predictive minimality that can suppress spurious context <span class="citation" data-cites="14BVLTOJq">[<a href="#ref-14BVLTOJq" role="doc-biblioref">160</a>]</span>. Concept-based methods such as TCAV and ACE map latent directions to human concepts and test sensitivity at the concept level <span class="citation" data-cites="1Cd8cgDM6 K4VkXXUf">[<a href="#ref-1Cd8cgDM6" role="doc-biblioref">161</a>,<a href="#ref-K4VkXXUf" role="doc-biblioref">162</a>]</span>. Fully unsupervised disentanglement is often ill-posed without inductive bias or weak supervision <span class="citation" data-cites="URCTSFCA">[<a href="#ref-URCTSFCA" role="doc-biblioref">163</a>]</span>. Quantitative evaluation of disentanglement can follow established metrics that assess factor independence, completeness, and informativeness <span class="citation" data-cites="12CgohhqK">[<a href="#ref-12CgohhqK" role="doc-biblioref">164</a>]</span>. Reports should include concept validity tests, factor stability across runs, and simple interventions that demonstrate controllability.</p>
<h4 id="parameter-extraction-and-probing">Parameter Extraction and Probing</h4>
<p>This family locates where adaptation is encoded and exposes handles for inspection or edits. Linear probes test what is linearly decodable from intermediate layers <span class="citation" data-cites="y1dj3AHA">[<a href="#ref-y1dj3AHA" role="doc-biblioref">165</a>]</span>; edge probing examines specific linguistic structure in contextualized representations <span class="citation" data-cites="11dRZlKVC">[<a href="#ref-11dRZlKVC" role="doc-biblioref">166</a>]</span>. Model editing methods such as ROME can modify stored factual associations directly in weights <span class="citation" data-cites="32o7NfNa">[<a href="#ref-32o7NfNa" role="doc-biblioref">167</a>]</span>, while “knowledge neurons” seek units linked to particular facts <span class="citation" data-cites="mvLZYXJQ">[<a href="#ref-mvLZYXJQ" role="doc-biblioref">168</a>]</span>. Evaluation involves quantifying pre- and post-edit behavior, assessing locality and persistence, and documenting side effects on unrelated capabilities. Collectively, these methods transform hidden internal adaptations into analyzable modular components.</p>
<h4 id="llms-as-post-hoc-explainers">LLMs as Post-hoc Explainers</h4>
<p>Recent work uses in-context prompting to elicit rationales, counterfactuals, or error hypotheses from large language models for a target system <span class="citation" data-cites="RvAOKYai">[<a href="#ref-RvAOKYai" role="doc-biblioref">169</a>]</span>. These explanations can be useful but must be validated for faithfulness, for example by checking agreement with surrogate attributions, reproducing input–output behavior, and testing stability to prompt variations. Explanations should be treated as statistical estimators with stated objectives and evaluation criteria <span class="citation" data-cites="8C0DuyuD">[<a href="#ref-8C0DuyuD" role="doc-biblioref">150</a>]</span>.</p>
<p>These methodological families differ in their assumptions and computational granularity, yet they all aim to render adaptation transparent and controllable. The following sections summarize their key trade-offs and conceptual challenges.</p>
<h3 id="trade-offs">Trade-offs</h3>
<h4 id="fidelity-vs.-interpretability">Fidelity vs. Interpretability</h4>
<p>High-fidelity surrogates capture the target model’s behavior more accurately, yet they often grow in complexity and lose readability. A crisp statement of the design goal is</p>
<p><span class="math display">\[
\min_{g\in\mathcal G}\ \underbrace{\phi_{\text{fid}}(g;U)}_{\text{faithfulness on use set }U}
+ \lambda\\underbrace{\psi_{\text{simplicity}}(g)}_{\text{sparsity / size / semantic load}},
\]</span></p>
<p>where <span class="math inline">\(\phi_{\text{fid}}\)</span> can be local <span class="math inline">\(R^2\)</span>, AUC, or rank correlation with <span class="math inline">\(h\)</span>, and <span class="math inline">\(\psi_{\text{simplicity}}\)</span> can be sparsity, tree depth, rule count, or active concept count. If a simple surrogate underfits, consider structured regularization (e.g., monotonic constraints, grouped sparsity, concept bottlenecks). If a complex surrogate is needed, accompany it with readable summaries (partial dependence snapshots, distilled rule sets, compact concept reports).</p>
<h4 id="local-vs.-global-scope">Local vs. Global Scope</h4>
<p>Local surrogates aim for <span class="math inline">\(g_{x_0,c_0}\approx h\)</span> only on <span class="math inline">\(\mathcal N_{x_0,c_0}\)</span>, whereas a global surrogate seeks <span class="math inline">\(g_{\text{global}}\approx h\)</span> across the domain, potentially smoothing away distinct regimes. Hybrid schemes combine both:</p>
<p><span class="math display">\[
g(x,c)=\sum_{k=1}^{K} w_k(x,c)\, g_k(x,c),
\qquad \sum_k w_k(x,c)=1,\quad w_k\ge 0,
\]</span></p>
<p>with local experts <span class="math inline">\(g_k\)</span> and soft assignment <span class="math inline">\(w_k\)</span>. Report the neighborhood definition, coverage (fraction of test cases with acceptable local fit), and disagreements between local and global views; flag regions where the global surrogate is unreliable.</p>
<h4 id="approximation-vs.-control">Approximation vs. Control</h4>
<p>Coarse modularization makes control and auditing simpler because edits act on a small number of levers, yet residual error can be large. Fine-grained extraction, such as neuron- or weight-level edits, can achieve precise behavioral changes but may introduce unintended side effects. Define the intended edit surface in advance (concepts, features, prototypes, submodules, parameters). For coarse modules, measure the residual gap to the base model and verify that edits improve target behavior without harming unaffected cases. For fine-grained edits, quantify locality and collateral effects using a held-out audit suite with counterfactuals, canary tasks, and out-of-distribution probes. Maintain versioned edits, enable rollback, and document the scope of validity.</p>
<p>These trade-offs are not merely design choices but determine the operational boundaries within which explicit representations can remain faithful to the original adaptive system.</p>
<h3 id="open-research-directions">Open Research Directions</h3>
<h4 id="reusable-modules">Reusable Modules</h4>
<p>The challenge of isolating reusable routines parallels the quest for parameter-efficient fine-tuning in large models, where adaptation must remain modular yet composable. A central question is whether we can isolate portable skills or routines from large models and reuse them across tasks without degrading overall capability <span class="citation" data-cites="1GbAsSOZV">[<a href="#ref-1GbAsSOZV" role="doc-biblioref">69</a>]</span>. Concretely, a reusable module should satisfy portability, isolation, composability, and stability. Promising directions include concept bottlenecks that expose human-aligned interfaces, prototype libraries as swappable reference sets, sparse adapters that confine changes to limited parameter subsets, and routing mechanisms that select modules based on context. Evaluation should track transfer performance, sample efficiency, interference on held-out capabilities, and robustness under domain shift.</p>
<h4 id="performance-gains">Performance Gains</h4>
<p>When does making structure explicit improve robustness or efficiency compared to purely implicit adaptation? Benefits are most likely when domain priors are reliable, data are scarce, or safety constraints limit free-form behavior. Explicit structure is promising when context topology is known (spatial or graph), when spurious correlations should be suppressed, and when explanations must be auditable. To assess this, fix capacity and training budget and vary only the explicit structure (prototypes, disentanglement, bottlenecks). Stress tests should cover diverse distributional challenges, including covariate shift, concept shift, long-tail classes, and adversarially correlated features. Account for costs such as concept annotation, extra hyperparameters, and potential in-domain accuracy loss.</p>
<h4 id="abstraction-level">Abstraction Level</h4>
<p>Another open issue is the appropriate level at which to represent structure: parameters (weights, neurons), functions (local surrogates, concept scorers, routing policies), or latent causes (disentangled or causal factors). Benchmarking under fixed capacity and identical data regimes is essential to isolate the contribution of explicit structure from mere model scaling effects. Choose based on the use case. For safety patches, lower-level handles allow precise edits but require guardrails and monitoring. For scientific or policy communication, function- or concept-level interfaces are often more stable and auditable. Optimize three objectives in tension: faithfulness to the underlying model, usability for the target audience, and stability under shift. Tooling should support movement between levels (e.g., distilling weight-level edits into concept summaries or lifting local surrogates into compact global reports). Selecting the proper level of abstraction thus defines not only interpretability but also the feasible scope of control.</p>
<h3 id="evaluation-and-reporting-standards-for-classical-post-hoc-methods">Evaluation and Reporting Standards for Classical Post-hoc Methods</h3>
<p>LIME, SHAP, and gradient-based methods such as Integrated Gradients and DeepLIFT remain common tools for context-adaptive interpretation. Their usefulness depends on careful design and transparent reporting. Explanations should be treated as statistical estimators with stated objectives and evaluation criteria <span class="citation" data-cites="Cq4JGOGX 8C0DuyuD">[<a href="#ref-Cq4JGOGX" role="doc-biblioref">149</a>,<a href="#ref-8C0DuyuD" role="doc-biblioref">150</a>]</span>. Carmichael &amp; Scheirer (2021) further propose a principled evaluation framework for feature-additive explainers, enabling measurement of misattribution even under known ground-truth additive models <span class="citation" data-cites="gOxiOg9S">[<a href="#ref-gOxiOg9S" role="doc-biblioref">170</a>]</span>.</p>
<h4 id="scope-and-locality">Scope and locality</h4>
<p>Local surrogate methods require a clear definition of the neighborhood in which the explanation is valid. The sampling scheme, kernel width, and surrogate capacity determine which aspects of the black box can be recovered. When context variables are present, the explanation should be conditioned on the relevant context and the valid region should be described.</p>
<h4 id="attribution-methods-in-practice">Attribution methods in practice</h4>
<p>Attribution based on gradients is sensitive to baseline selection, path construction, input scaling, and preprocessing. Baselines should have clear domain meaning, and sensitivity analyses should show how conclusions change under alternative baselines. For perturbation-based surrogates, report the perturbation distribution and any constraints that keep samples on the data manifold.</p>
<h4 id="faithfulness-and-robustness">Faithfulness and robustness</h4>
<p>Faithfulness and robustness should be checked rather than assumed. Useful checks include deletion and insertion curves, counterfactual tests, randomization tests, stability under small input and seed perturbations, and for local surrogates a local goodness-of-fit such as a neighborhood <span class="math inline">\(R^2\)</span>. The evaluation metric should match the stated objective of the explanation <span class="citation" data-cites="Cq4JGOGX 8C0DuyuD">[<a href="#ref-Cq4JGOGX" role="doc-biblioref">149</a>,<a href="#ref-8C0DuyuD" role="doc-biblioref">150</a>]</span>. Turbé et al. (2023) demonstrate evaluation of interpretability methods on time-series models using metrics such as <span class="math inline">\(\widetilde{\mathrm{AUC}}_{S}\)</span> and <span class="math inline">\(\widetilde{F_{1,S}}\)</span> to compare alignment with model internals <span class="citation" data-cites="RzmVBDLx">[<a href="#ref-RzmVBDLx" role="doc-biblioref">171</a>]</span>.</p>
<h4 id="minimal-reporting-checklist">Minimal reporting checklist</h4>
<table>
<colgroup>
<col style="width: 30%" />
<col style="width: 70%" />
</colgroup>
<thead>
<tr class="header">
<th>Item</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Data slice and context definition</strong></td>
<td>Specify the subset of data and contextual variables used for generating explanations, and describe the locality or neighborhood definition.</td>
</tr>
<tr class="even">
<td><strong>Surrogate specification and regularization details</strong></td>
<td>Report the family of surrogate models, chosen regularization strategy, and kernel or sampling parameters.</td>
</tr>
<tr class="odd">
<td><strong>Faithfulness and robustness metrics</strong></td>
<td>Include local <span class="math inline">\(R^2\)</span>, deletion/insertion area, counterfactual validity, and robustness under perturbations.</td>
</tr>
<tr class="even">
<td><strong>Sensitivity and uncertainty analysis</strong></td>
<td>Assess variation across baselines, random seeds, and small input perturbations, providing uncertainty estimates.</td>
</tr>
<tr class="odd">
<td><strong>Computational constraints</strong></td>
<td>Document runtime, hardware limitations, and approximation budgets that affect explanation quality.</td>
</tr>
<tr class="even">
<td><strong>Observed limitations and failure modes</strong></td>
<td>Summarize known weaknesses, unstable regions, or interpretability failures identified during validation.</td>
</tr>
</tbody>
</table>
<p><strong>Table 2. Minimal Reporting Checklist for Post-hoc Explanations</strong></p>
<h4 id="from-post-hoc-analysis-to-design">From post hoc analysis to design</h4>
<p>Insights from post-hoc analysis can inform proactive model design for control, auditing, and policy communication. In such cases, interpretability methods should not remain external diagnostics but serve as guides for architectures with built-in transparency. For example, Concept Bottleneck Models integrate interpretable concepts into the forward pass <span class="citation" data-cites="b0a9ckui">[<a href="#ref-b0a9ckui" role="doc-biblioref">172</a>]</span>. Similarly, Poursabzi-Sangdeh et al. (2021) conduct empirical user studies to highlight how interpretability design choices affect human use and model trust <span class="citation" data-cites="1ARNqdn4y">[<a href="#ref-1ARNqdn4y" role="doc-biblioref">173</a>]</span>. These contributions extend the vision of Doshi-Velez &amp; Kim (2017) toward a unified science of interpretable modeling, where explanation and model training are co-designed <span class="citation" data-cites="Cq4JGOGX">[<a href="#ref-Cq4JGOGX" role="doc-biblioref">149</a>]</span>. Taken together, these lines of work bridge black-box adaptation and structured inference and set the stage for designs where context-to-parameter mappings are specified, trained, and evaluated end to end.</p>
<h4 id="implications-for-classical-models">Implications for classical models</h4>
<p>These tools can also clarify how traditional models, for example, logistic regression with interaction terms or generalized additive models to admit a local adaptation view: a simple global form paired with context-sensitive weights or features. Reading such models through the lens of local surrogates and concept interfaces helps align classical estimation with modern, context-adaptive practice. Reinterpreting these classical estimators through the lens of explicit adaptivity situates them as early instances of structured context modeling, underscoring continuity between statistical modeling and modern machine learning.</p>
<p>Taken together, these strategies illustrate a gradual unification of interpretability, modularization, and adaptive modeling, paving the way toward a principled science of explicit context-aware inference.</p>
<h2 id="context-invariant-training-a-view-from-the-converse">Context-Invariant Training: A View from the Converse</h2>
<p>While the preceding sections emphasize the importance of modeling context to tailor predictions, an equally fundamental question concerns robustness: Can we learn representations such that a single predictor performs reliably across sites, cohorts, and time, despite environmental shifts and nuisance variation? Context-invariant training aims at out-of-distribution (OOD) generalization by emphasizing features whose associations with the target remain stable across environments, while suppressing spurious correlations that vary with nuisance contexts. Standard Empirical Risk Minimization (ERM) <span class="citation" data-cites="15RY4QKp3">[<a href="#ref-15RY4QKp3" role="doc-biblioref">174</a>]</span> often latches onto spurious, environment-specific correlations. In practice, this means using multiple environments during training and favoring representations that make a single readout perform well everywhere.</p>
<p>The seminal framework connecting modern deep learning to invariant prediction is Invariant Risk Minimization (IRM) <span class="citation" data-cites="1DeCFT6PA">[<a href="#ref-1DeCFT6PA" role="doc-biblioref">175</a>]</span>, which formulates robustness as learning causally stable predictors across multiple environments. IRM seeks a representation <span class="math inline">\(\Phi\)</span> such that a shared predictor <span class="math inline">\(w\)</span> minimizes the risk <span class="math inline">\(R^e(\cdot)\)</span> for every environment <span class="math inline">\(e\)</span>. The original formulation is a bi-level optimization problem that is computationally intractable. To make it solvable, Arjovsky et al. propose a surrogate version, IRMv1, which introduces a penalty ensuring that the per-environment risk gradient vanishes for a shared dummy classifier <span class="math inline">\(w=1\)</span>, thereby enforcing stationarity across environments. This construction connects invariance to out-of-distribution (OOD) generalization by encouraging predictors aligned with causal mechanisms that persist across environments.</p>
<p>However, subsequent analyses revealed important limitations. In linear settings, IRM often fails to recover the true invariant predictor, and in nonlinear regimes, performance can deteriorate sharply when test distributions deviate from the training domains <span class="citation" data-cites="1FLMzrLE9">[<a href="#ref-1FLMzrLE9" role="doc-biblioref">176</a>]</span>. This undermines IRM’s objective of handling distribution shift, where <span class="math inline">\(P(X)\)</span> changes while <span class="math inline">\(P(Y|X)\)</span> remains fixed. Thus, IRM offers no mechanism to reduce sensitivity when those shifts are amplified at test time. To mitigate these issues, Risk Extrapolation (REx) <span class="citation" data-cites="11IMWGprl">[<a href="#ref-11IMWGprl" role="doc-biblioref">177</a>]</span> extends the principle of invariance by optimizing directly over per-environment risk vectors. Two practical variants have been proposed: MM-REx and V-REx, which performs robust optimization over affine combinations of the environment risks (weights sum to 1, possibly negative), and V-REx, which minimizes the mean risk augmented by the variance of risks across environments.</p>
<p>Unlike IRM, which requires explicit environment labels, Beery et al. (2018) <span class="citation" data-cites="L6xa6qzg">[<a href="#ref-L6xa6qzg" role="doc-biblioref">178</a>]</span> propose CoRe, a method that assumes some samples share a common identifier. Features are decomposed into core components (whose class-conditional distribution is stable across domains) and style components (e.g., brightness, pose) that vary with domains. The CoRe estimator enforces robustness by penalizing the conditional variance of the loss within groups sharing the same label–identifier pair <span class="math inline">\((Y, ID)\)</span>.</p>
<h3 id="adversarial-robustness-as-context-invariant-training">Adversarial Robustness as Context-Invariant Training</h3>
<p>Whereas IRM seeks robustness across discrete environments, adversarial robustness can be regarded as its infinitesimal counterpart—focusing on perturbations within a local neighborhood of each input rather than across distinct domains. Those different environments can be interpreted as fine-grained, synthetic perturbations around each data point rather than distinct real-world domains.
Invariant learning generally seeks predictors whose performance remains stable when the data-generating context changes — for example, across hospitals, time periods, or demographic groups <span class="citation" data-cites="JhS4KMR7">[<a href="#ref-JhS4KMR7" role="doc-biblioref">179</a>]</span>. Adversarial robustness follows the same principle of invariance, but at a much finer scale: instead of using naturally occurring environments, it constructs synthetic “environments” through small, deliberate perturbations of the input data. These perturbations simulate local environmental shifts around each sample and expose the model to worst-case contexts.
From this perspective, adversarial robustness is essentially context-invariant learning under infinitesimal, adversarially generated environments.
Each adversarial example <span class="math inline">\(x&#39;=x+\delta\)</span> (where <span class="math inline">\(\|\delta\|_p \le \varepsilon\)</span>) can be interpreted as belonging to a neighboring environment of the original input x. Training the model to perform consistently under such local shifts enforces a form of fine-grained invariance that complements the coarse-grained invariance targeted by IRM.
The paper <span class="citation" data-cites="B0L8KJ8W">[<a href="#ref-B0L8KJ8W" role="doc-biblioref">180</a>]</span> addresses the vulnerability of deep learning models to adversarial attacks from the optimization view. Specifically, the authors interpret adversarial robustness as a min-max optimization problem, where the goal is to minimize the worst-case loss incurred by adversarial examples. Madry et al. (2018) introduce Projected Gradient Descent (PGD) as a universal first-order adversary. The generated perturbations are incorporated into the training process to improve robustness under local contextual shifts. In this view, the environments in IRM correspond to multiple data domains, while those in adversarial training correspond to local neighborhoods of each sample—both formulations share the same objective of minimizing performance variation across shifts in context. Formally, both IRM and adversarial training minimize performance variance across contexts—IRM across discrete environments, and adversarial training across continuous perturbation neighborhoods.</p>
<p><span class="citation" data-cites="ylSNfYug">[<a href="#ref-ylSNfYug" role="doc-biblioref">181</a>]</span> provably demonstrates the trade-off between robustness and accuracy in machine learning models. The authors argue that adversarial training, while improving robustness to adversarial perturbations, can decrease the model’s accuracy on clean data. This occurs because adversarial training forces the model to adjust its decision boundaries, which may lead to a loss in standard performance. The paper also shows that the representations learned by robust models align better with salient data characteristics and human perception, which suggests that robust models focus more on features that are meaningful and interpretable. At the same time, robust models tend to learn representations that align better with salient data characteristics and human perception, suggesting that robustness promotes the extraction of stable, semantically meaningful features, mirroring the goal of context-invariant learning at a smaller, instance-specific scale <span class="citation" data-cites="l5C1P3il">[<a href="#ref-l5C1P3il" role="doc-biblioref">182</a>]</span>.</p>
<p>This perspective is directly applicable to the challenges faced by LLM-based Agents as surveyed in <span class="citation" data-cites="CYfSdHYg">[<a href="#ref-CYfSdHYg" role="doc-biblioref">183</a>]</span>. An autonomous agent does not operate in a sterile, curated dataset; it operates in the wild. These fine-grained, synthetic perturbations provide a useful abstraction for understanding the robustness challenges faced by LLM-based agents:</p>
<p><strong>Perception Robustness</strong>: A small, imperceptible change to an image or a document an agent is analyzing (an adversarial perturbation) could cause it to completely misinterpret its environment and take a disastrous action.</p>
<p><strong>Tool-Use Robustness</strong>: A slight rephrasing of a user’s command could trick a non-robust agent into generating incorrect or malicious code for a tool to execute.
<!-- 
Related references:

- Towards Deep Learning Models Resistant to Adversarial Attacks [@arXiv:1706.06083]
- Robustness May Be at Odds with Accuracy [@arXiv:1805.12152]

- The Rise and Potential of Large Language Model Based Agents: A Survey [@arXiv:2309.07864] --></p>
<p>Hence, advances in adversarial robustness directly inform the design of safer, more context-stable autonomous agents.</p>
<h3 id="training-methods-for-context-invariant-models">Training methods for Context-Invariant Models</h3>
<p>While the principle of context-invariance is a powerful theoretical goal, several practical training methodologies have been developed to approximate it, primarily by enhancing robustness against group shifts. These methods vary in their assumptions, particularly regarding the availability of explicit group or environment labels for the training data.</p>
<p>A foundational approach, applicable when group labels are available, is Group Distributionally Robust Optimization (Group DRO). Unlike standard Empirical Risk Minimization (ERM) which minimizes the average loss over the entire dataset, formulated as:
<span class="math display">\[
\min_{f} \frac{1}{n} \sum_{i=1}^{n} L(f(x_i), y_i)
\]</span>
Group DRO’s objective is to minimize the loss on the worst-performing data group. This is formally expressed as a min-max problem:
<span class="math display">\[
\min_{f} \max_{g \in \mathcal{G}} \mathbb{E}_{(x,y) \sim P_g} [L(f(x), y)]
\]</span>
where <span class="math inline">\(\mathcal{G}\)</span> represents the set of all predefined groups and <span class="math inline">\(P_g\)</span> is the data distribution for a specific group <span class="math inline">\(g\)</span> <span class="citation" data-cites="Jm8Kx8HW">[<a href="#ref-Jm8Kx8HW" role="doc-biblioref">184</a>]</span>. However, the authors identify a critical pitfall: in modern, overparameterized neural networks, this method can fail. Such models can easily memorize the entire training set, reducing the worst-case training loss to zero without actually learning a generalizable solution. The key insight from this work is that <strong>strong regularization</strong> (such as a high L2 penalty or aggressive early stopping) is essential. Regularization prevents the model from perfectly fitting the training data, forcing it to learn simpler, more robust features that generalize better to the worst-case groups on unseen data.
The primary limitation of Group DRO is its reliance on fully annotated training data, a luxury seldom available in real-world scenarios. This challenge has spurred the development of methods that operate without explicit group information. These approaches cleverly leverage the inherent biases of standard models as a source of information.
A simple and highly effective heuristic is Just Train Twice (JTT) <span class="citation" data-cites="EXaywYVO">[<a href="#ref-EXaywYVO" role="doc-biblioref">185</a>]</span>. This method operates in two stages: first, a standard ERM model is trained for several epochs. Second, the training examples that this initial model misclassified are identified and upweighted. A new model is then trained from scratch on this reweighted dataset. The underlying assumption is that a standard model’s errors serve as an effective proxy for identifying examples from minority or difficult groups. By focusing the second stage of training on these hard examples, JTT improves worst-group performance without ever needing to know the group labels.
Providing a more formalized framework, Environment Inference for Invariant Learning (EIIL) aims to bridge the gap between unlabeled data and invariant learning algorithms like IRM <span class="citation" data-cites="10151coVE">[<a href="#ref-10151coVE" role="doc-biblioref">186</a>]</span>. Similar to JTT, EIIL begins by training a standard ERM model. It then uses the biases of this reference model to automatically partition the dataset into several inferred “environments.” For instance, examples the model confidently gets right might form one environment, while those it gets wrong form another. These algorithmically generated environment labels can then be fed into any off-the-shelf invariant learning method to train a final, robust model. EIIL essentially transforms the problem from one requiring manual labels to one where environments can be discovered directly from the data itself.
Collectively, these approaches demonstrate a continuum from fully supervised environment-aware optimization to self-supervised environment discovery, unified under the goal of achieving context-invariant generalization. Together, these methods illustrate a clear progression from fully-supervised techniques to more practical approaches that cleverly infer hidden data structure, all aiming to build models that are more robust and invariant to challenging shifts in context.</p>
<!-- - Just Train Twice: Improving Group Robustness without Training Group Information [@arXiv:2002.10384]
- Environment Inference for Invariant Learning [@arXiv:2110.14048]
- Distributionally Robust Neural Networks for Group Shifts [@arXiv:1911.08731] -->
<h2 id="applications-case-studies-evaluation-metrics-and-tools">Applications, Case Studies, Evaluation Metrics, and Tools</h2>
<p>This section surveys how context-adaptive methods manifest across domains, how their performance is assessed, and what tools enable them in practice.</p>
<h3 id="implementation-across-sectors">Implementation Across Sectors</h3>
<p>Many real-world environments are dynamic and unpredictable, meaning that models built on static assumptions often fail when conditions shift. To remain reliable, models must be able to adapt to changing inputs, contexts, and behaviors. This adaptability is especially important in high-stakes domains where decisions directly affect human well-being or carry significant financial consequences. Two prominent examples are healthcare and finance. In healthcare, context-adaptive models enable more personalized treatment decisions and support early intervention by capturing the evolving state of patients and diseases. In finance, these models capture rapidly changing market conditions, allowing forecasts and risk assessments to remain accurate in volatile times.</p>
<p>Healthcare is one of the domains that benefits greatly from context-aware models because clinical and biomedical data are often hierarchical, exhibiting nested structures and evolving over time. For example, patients may have repeated measurements (e.g., vitals, labs) nested within visits, and these visits are themselves nested within broader care episodes. At the same time, disease trajectories and treatment responses are highly dynamic, requiring models that can adapt to changing contexts rather than assuming static relationships. Several reviews highlight the importance of methods that explicitly account for such complexity in longitudinal and multilevel health data <span class="citation" data-cites="UqtGdFFv 1BXylhj0T">[<a href="#ref-UqtGdFFv" role="doc-biblioref">187</a>,<a href="#ref-1BXylhj0T" role="doc-biblioref">188</a>]</span>. One concrete example is a Bayesian multilevel time-varying joint model that captures complex structures while estimating diverse time-varying relationships, including both response–predictor and response–response dependencies <span class="citation" data-cites="ow7VHsto">[<a href="#ref-ow7VHsto" role="doc-biblioref">189</a>]</span>. Such models often employ hierarchical priors to borrow strength across patients while maintaining individualized inference. In this framework, time-varying coefficients are flexibly estimated using Bayesian P-splines, and inference is performed through Markov Chain Monte Carlo (MCMC). The result is a computationally efficient algorithm that provides interpretable modeling of patient outcomes as they evolve over time.</p>
<p>In finance, context-aware models are particularly valuable for capturing the complex dynamics that unfold both over time and across countries, sectors, and assets, which together drive macroeconomic and market behavior. For instance, cross-sectional dependencies, which capture interconnectedness at the same point in time, emerge when shocks propagate differently across regions or industries, while temporal dependencies, which capture persistence across time, arise from persistent volatility clustering and regime changes. Several reviews and comparative studies emphasize the need for methods that can adapt to such heterogeneity in modern financial data <span class="citation" data-cites="1HdkTgLul 167NV1YDH">[<a href="#ref-1HdkTgLul" role="doc-biblioref">190</a>,<a href="#ref-167NV1YDH" role="doc-biblioref">191</a>]</span>. A prominent line of work develops Bayesian matrix dynamic factor models (MDFMs), which provide a powerful framework for analyzing matrix-valued time series increasingly common in macro-finance applications <span class="citation" data-cites="4aDIdh6z">[<a href="#ref-4aDIdh6z" role="doc-biblioref">192</a>]</span>. These models incorporate multiple context-adaptive features. On the temporal side, an autoregressive factor process captures persistent comovement and improves recursive forecasting, while stochastic volatility, fat-tailed error distributions, and explicit COVID-19 outlier adjustments allow the model to remain robust under real-world market shocks. The approximate factorization reduces complexity from cubic to linear in the number of assets, making large-scale forecasting feasible.</p>
<h3 id="context-aware-efficiency-in-practice">Context-Aware Efficiency in Practice</h3>
<p>The principles of context-aware efficiency find practical applications across diverse domains, demonstrating how computational and statistical efficiency can be achieved through intelligent context utilization.</p>
<p>In healthcare applications, context-aware efficiency enables adaptive imaging protocols that adjust scan parameters based on patient context such as age, symptoms, and medical history, reducing unnecessary radiation exposure. Personalized screening schedules optimize screening frequency based on individual risk factors and previous results, while resource allocation systems efficiently distribute limited healthcare resources based on patient acuity and context.</p>
<p>Financial services leverage context-aware efficiency principles in risk assessment by adapting risk models based on market conditions, economic indicators, and individual borrower characteristics. Fraud detection systems use context-dependent thresholds and sampling strategies to balance detection accuracy with computational cost, while portfolio optimization dynamically adjusts rebalancing based on volatility regimes and transaction costs, as studied in regime-switching portfolio models <span class="citation" data-cites="E4f9DQPu">[<a href="#ref-E4f9DQPu" role="doc-biblioref">193</a>]</span>.</p>
<p>Industrial applications benefit from context-aware efficiency through predictive maintenance systems that adapt maintenance schedules based on equipment context including age, usage patterns, and environmental conditions <span class="citation" data-cites="jH6SpM8M">[<a href="#ref-jH6SpM8M" role="doc-biblioref">194</a>]</span>. Quality control implements context-dependent sampling strategies that focus computational resources on high-risk production batches, and inventory management uses context-aware forecasting to optimize stock levels across different product categories and market conditions.</p>
<p>A notable example of context-aware efficiency is adaptive clinical trial design, where trial parameters are dynamically adjusted based on accumulating evidence while maintaining statistical validity. Population enrichment refines patient selection criteria based on early trial results, and dose finding optimizes treatment dosages based on individual patient responses and safety profiles. These applications demonstrate how context-aware efficiency principles can lead to substantial improvements in both computational performance and real-world outcomes.</p>
<h3 id="formal-metrics-for-evaluating-context-aware-performance">Formal Metrics for Evaluating Context-Aware Performance</h3>
<p>Building on the theoretical framework introduced in earlier sections, we now formalize the evaluation criteria used to quantify context-adaptive behavior.<br />
These metrics capture predictive accuracy, adaptation efficiency, transferability, and robustness under contextual variation.</p>
<p>Let <span class="math inline">\(\mathcal{C}\)</span> denote the context space and <span class="math inline">\(\mathcal{D}_{\mathrm{test}}\)</span> a test distribution over <span class="math inline">\((x, y, c)\)</span>.<br />
For a predictor <span class="math inline">\(\hat{f}\)</span>, define the context-conditional risk as</p>
<p><span class="math display">\[
\mathcal{R}(\hat{f}\mid c)
= \mathbb{E}\!\left[\, \ell\big(\hat{f}(x,c), y\big) \,\middle|\, c \,\right],
\qquad
\mathcal{R}(\hat{f})
= \mathbb{E}_{c\sim \mathcal{D}_{\mathrm{test}}}\!\left[\, \mathcal{R}(\hat{f}\mid c) \,\right].
\]</span></p>
<p>A context-stratified evaluation reports <span class="math inline">\(\mathcal{R}(\hat{f}\mid c)\)</span> across predefined bins or via a smoothed estimate<br />
<span class="math inline">\(\int \mathcal{R}(\hat{f}\mid c)\,\mathrm{d}\Pi(c)\)</span> for a reference measure <span class="math inline">\(\Pi\)</span> that weights regions of the context space.</p>
<h4 id="adaptation-efficiency">Adaptation Efficiency</h4>
<p>To evaluate how rapidly a model benefits from in-context examples,<br />
let <span class="math inline">\(S_k(c)=\{(x_j, y_j, c)\}_{j=1}^k\)</span> denote <span class="math inline">\(k\)</span> examples available within context <span class="math inline">\(c\)</span>.<br />
Define the adaptation efficiency as</p>
<p><span class="math display">\[
\mathrm{AE}_k(c)
= \mathcal{R}(\hat{f}_0 \mid c)
- \mathcal{R}(\hat{f}_{S_k} \mid c),
\qquad
\mathrm{AE}_k
= \mathbb{E}_{c}\!\left[\, \mathrm{AE}_k(c) \,\right],
\]</span></p>
<p>where <span class="math inline">\(\hat{f}_0\)</span> is the non-adapted baseline and <span class="math inline">\(\hat{f}_{S_k}\)</span> the adapted predictor.<br />
The function <span class="math inline">\(k \mapsto \mathrm{AE}_k\)</span> summarizes few-shot adaptation gains across different context sizes.</p>
<h4 id="transfer-performance">Transfer Performance</h4>
<p>Transfer across source and target contexts, <span class="math inline">\(\mathcal{C}_{\mathrm{src}} \to \mathcal{C}_{\mathrm{tgt}}\)</span>,<br />
with shared representation <span class="math inline">\(\phi\)</span>, can be measured by</p>
<p><span class="math display">\[
\mathrm{TP}(\phi)
= \mathcal{R}_{\mathcal{C}_{\mathrm{tgt}}}\!\big(\hat{f}_{\phi}\big)
- \mathcal{R}_{\mathcal{C}_{\mathrm{tgt}}}\!\big(\hat{f}_{\mathrm{scratch}}\big),
\]</span></p>
<p>quantifying performance retained when transferring <span class="math inline">\(\phi\)</span> from source to target contexts compared with training from scratch.</p>
<h4 id="robustness-to-context-shift">Robustness to Context Shift</h4>
<p>To assess stability under distributional perturbations,<br />
let <span class="math inline">\(Q\)</span> denote a family of admissible context shifts (e.g., <span class="math inline">\(f\)</span>-divergence or Wasserstein balls over context marginals).<br />
Then the robustness score is defined as</p>
<p><span class="math display">\[
\mathrm{RS}(\hat{f}; Q)
= \sup_{\widetilde{\mathcal{D}}\in Q}
\left[
\mathcal{R}_{\widetilde{\mathcal{D}}}(\hat{f})
- \mathcal{R}_{\mathcal{D}_{\mathrm{test}}}(\hat{f})
\right],
\]</span></p>
<p>where higher values indicate greater sensitivity to contextual changes.</p>
<p>These metrics provide a unified quantitative view of context-aware performance.<br />
They complement the theoretical efficiency results developed in Section 4<br />
and serve as practical diagnostics for evaluating real-world adaptivity across diverse applications.</p>
<h3 id="context-aware-efficiency-in-practice-1">Context-Aware Efficiency in Practice</h3>
<p>The principles of context-aware efficiency find practical applications across diverse domains, demonstrating how computational and statistical efficiency can be achieved through intelligent context utilization.</p>
<p>In healthcare applications, context-aware efficiency enables adaptive imaging protocols that adjust scan parameters based on patient context such as age, symptoms, and medical history, reducing unnecessary radiation exposure. Personalized screening schedules optimize screening frequency based on individual risk factors and previous results, while resource allocation systems efficiently distribute limited healthcare resources based on patient acuity and context.</p>
<p>Financial services leverage context-aware efficiency principles in risk assessment by adapting risk models based on market conditions, economic indicators, and individual borrower characteristics. Fraud detection systems use context-dependent thresholds and sampling strategies to balance detection accuracy with computational cost, while portfolio optimization dynamically adjusts rebalancing frequency based on market volatility and transaction costs <span class="citation" data-cites="jH6SpM8M">[<a href="#ref-jH6SpM8M" role="doc-biblioref">194</a>]</span>.</p>
<p>Industrial applications derive clear benefits from context-aware efficiency. In predictive maintenance, systems adapt maintenance schedules using equipment context such as age, usage history, and environmental conditions. For example, recent surveys of predictive maintenance in Industry 4.0 identify architectures that integrate sensor data, remaining-useful-life models, and context-aware scheduling policies <span class="citation" data-cites="2CxZoj5J 8MSb5kh8">[<a href="#ref-2CxZoj5J" role="doc-biblioref">195</a>,<a href="#ref-8MSb5kh8" role="doc-biblioref">196</a>]</span>. In quality control, context-dependent sampling directs inspection efforts to high-risk units, reducing waste and computational cost. Inventory management likewise benefits from context-aware forecasting models that incorporate demand volatility, seasonality, and external signals; recent work shows that such approaches outperform traditional forecasts in retail settings <span class="citation" data-cites="1C8sIEO7D">[<a href="#ref-1C8sIEO7D" role="doc-biblioref">197</a>]</span>.</p>
<p>A notable example of context-aware efficiency is adaptive clinical trial design, where trial parameters are dynamically adjusted based on accumulating evidence while maintaining statistical validity. Population enrichment refines patient selection criteria based on early trial results, and dose finding optimizes treatment dosages based on individual patient responses and safety profiles. These applications demonstrate how context-aware efficiency principles can lead to substantial improvements in both computational performance and real-world outcomes.</p>
<h3 id="contextualized-network-inference">Contextualized Network Inference</h3>
<p>One domain where context-adaptive models have shown particular promise is in network inference for genomics. Traditional approaches assume that all samples can be pooled into a single network, or that cohorts can be partitioned into homogeneous groups. These assumptions are often unrealistic: cancer, for example, exhibits both cross-patient heterogeneity and within-patient shifts in gene regulation.</p>
<p>Contextualized network models address this challenge by learning archetypal networks and then representing each sample as a mixture of these archetypes, weighted by its observed context. This formulation allows researchers to move beyond average-case networks and uncover mechanisms of disease, heterogeneity across patients, driver mutations, and structural hazards.</p>
<p>Such contextualized networks have been applied in TCGA cancer genomics to identify patient-specific driver modules.</p>
<div id="fig:contextualized-networks" class="fignos">
<figure>
<img src="images/contextualized_networks.png" style="width:90.0%" alt="Figure 10: Contextualized networks enable inference of archetypal and sample-specific mixtures, unlocking new biological insights such as mechanisms of disease, disease heterogeneity, structural hazards, and driver mutations." />
<figcaption aria-hidden="true"><span>Figure 10:</span> Contextualized networks enable inference of archetypal and sample-specific mixtures, unlocking new biological insights such as mechanisms of disease, disease heterogeneity, structural hazards, and driver mutations.</figcaption>
</figure>
</div>
<h3 id="performance-evaluation">Performance Evaluation</h3>
<p>Evaluating context-adaptive models requires careful consideration of predictive accuracy, robustness to variability, and scalability, with the emphasis varying by domain. Key aspects of performance evaluation include the choice of metrics, the handling of uncertainty, and assessment under stress or rare-event conditions.</p>
<p>In healthcare, evaluation prioritizes patient-specific predictive accuracy and calibrated uncertainty. Common metrics include mean squared error (MSE), concordance indices (C-index), and calibration curves, which measure how well models capture longitudinal patient trajectories and provide reliable uncertainty estimates. Multi-target Bayesian approaches and survival models demonstrate the importance of capturing correlations across outcomes and assessing credible interval coverage to quantify predictive confidence <span class="citation" data-cites="138XfoxXm 1C0w0XTvj">[<a href="#ref-138XfoxXm" role="doc-biblioref">198</a>,<a href="#ref-1C0w0XTvj" role="doc-biblioref">199</a>]</span>. Evaluations in this domain also highlight trade-offs between model complexity, interpretability, and computational feasibility, since high-fidelity patient-level predictions can be costly to compute.</p>
<p>In finance and macro forecasting, performance evaluation emphasizes predictive accuracy under volatile conditions and resilience to structural breaks. Metrics such as root mean squared forecast error (RMSFE), log-likelihood, and stress-test performance are commonly used to assess how well models handle crises or abrupt shifts in data <span class="citation" data-cites="RkqVE8TP nTk9nk8h">[<a href="#ref-RkqVE8TP" role="doc-biblioref">200</a>,<a href="#ref-nTk9nk8h" role="doc-biblioref">201</a>]</span>. Probabilistic metrics, including posterior predictive checks and uncertainty bounds, provide additional insight into the reliability of forecasts, while chaos-informed diagnostics can highlight vulnerabilities to extreme events <span class="citation" data-cites="gfPki3PM">[<a href="#ref-gfPki3PM" role="doc-biblioref">202</a>]</span>.</p>
<p>Across domains, consistent patterns emerge. Context-adaptive models outperform static baselines when variability is structured and partially predictable, but performance can degrade in data-sparse regimes or under unmodeled abrupt changes <span class="citation" data-cites="xoOsuDeH">[<a href="#ref-xoOsuDeH" role="doc-biblioref">203</a>]</span>. Evaluations therefore combine error-based measures, probabilistic calibration, and robustness tests to give a holistic view of model performance. The focus should be on these evaluation criteria, rather than the models themselves, to understand where and why context-adaptive approaches provide real advantages. Hence, evaluation protocols must jointly assess accuracy, calibration, and transferability under context perturbations.</p>
<h3 id="survey-of-tools">Survey of Tools</h3>
<p>There are many technological supports that have emerged to support context-adaptive modeling. These tools provide the infrastructure, memory, and efficiency mechanisms that allow models to operate effectively in dynamic environments.</p>
<p>Retrieval-augmented generation (RAG) has become a core support for adaptivity, enabling models to incorporate new knowledge at inference time instead of relying only on static parameters. Recent surveys outline how RAG architectures combine dense retrievers, re-rankers, and generators into pipelines that continuously update with external information. This allows models to remain aligned with changing knowledge bases <span class="citation" data-cites="yApXu0Vp">[<a href="#ref-yApXu0Vp" role="doc-biblioref">204</a>]</span>. Beyond improving factuality, RAG also underpins adaptive behavior in AI-generated content, where external retrieval reduces hallucination and provides domain-specific grounding <span class="citation" data-cites="1BPbcGQGq">[<a href="#ref-1BPbcGQGq" role="doc-biblioref">205</a>]</span>. These systems depend on efficient vector search. Tools such as FAISS use approximate nearest neighbor algorithms to index billions of embeddings with low latency, while Milvus integrates distributed storage to scale such systems across production environments <span class="citation" data-cites="1HqcEkGab">[<a href="#ref-1HqcEkGab" role="doc-biblioref">206</a>]</span>. Together, retrieval pipelines and vector databases constitute the infrastructure through which context-adaptive models dynamically expand their accessible knowledge.</p>
<p>While retrieval addresses external knowledge, memory systems support continuity within ongoing interactions. Research on AI memory frameworks emphasizes how models require mechanisms to persist relevant context, get rid of redundancy, and resurface information at appropriate times <span class="citation" data-cites="EsknayJ2">[<a href="#ref-EsknayJ2" role="doc-biblioref">207</a>]</span>. Recent implementations such as MemoryOS illustrate how adaptive memory systems can summarize past conversations, cluster related items, and strategically reinsert them into prompts, producing long-term coherence that can’t be achieved with static context windows alone <span class="citation" data-cites="QciOeUHY">[<a href="#ref-QciOeUHY" role="doc-biblioref">208</a>]</span>. These memory architectures extend adaptivity from the level of just accessing facts to maintaining evolving histories, allowing models to not just adjust to new data, but also to be more consistent and contextually aware of their interactions.</p>
<p>Another critical support lies in scaling sequence length. Standard transformers suffer quadratic complexity and degraded performance as contexts grow, making it difficult to adapt to long or streaming data. New serving infrastructures such as StreamingLLM introduce rolling caches that let models handle long inputs without full recomputation, while frameworks like vLLM use paged attention to manage GPU memory efficiently during extended inference <span class="citation" data-cites="D4deReQm PF464FGD">[<a href="#ref-D4deReQm" role="doc-biblioref">209</a>,<a href="#ref-PF464FGD" role="doc-biblioref">210</a>]</span>. This long-context support shifts adaptability from handling snapshots of information to maintaining awareness across evolving information streams.</p>
<h3 id="selection-and-usage-guidance">Selection and Usage Guidance</h3>
<p>Deploying context-adaptive models effectively requires careful alignment between model capabilities, domain needs, and practical constraints.</p>
<p>In healthcare, where data is often hierarchical and time-varying, Bayesian multilevel models and generalized varying-coefficient frameworks are well suited because they can flexibly capture nonlinear interactions and evolving patient trajectories. In finance, high-dimensional time series demand scalability, making matrix dynamic factor models more appropriate than fully specified multivariate systems.</p>
<p>Domain priorities should drive tool choice. Clinical applications often require interpretable models that clinicians can trust, favoring spline-based or single-index approaches even if they sacrifice some predictive accuracy. In contrast, finance applications typically prioritize forecasting performance under volatility, where more complex factor models can offer a competitive edge despite reduced transparency.</p>
<p>Many context-adaptive models rely on resource-intensive inference methods such as MCMC, which may limit scalability. Approximate inference techniques like variational Bayes or stochastic optimization can mitigate this burden for large datasets. In real-time decision settings, long-context processing methods such as StreamingLLM or KV-cache compression provide efficiency gains but require specialized engineering and hardware support.</p>
<p>Finally, tool selection should reflect whether the primary objective is scientific insight or operational decision-making. Biomedical research benefits most from flexible, interpretable models that generate new hypotheses, whereas domains like trading demand models capable of rapid adaptation, scalable inference, and strong predictive accuracy under uncertainty.</p>
<p>There is no one-size-fits-all context-adaptive model. Successful deployment depends not only on technical choices but also on aligning model adaptivity with domain-specific interpretability and governance requirements.</p>
<h2 id="future-trends-and-opportunities-with-foundation-models">Future Trends and Opportunities with Foundation Models</h2>
<h3 id="a-new-paradigm-for-context-adaptive-inference">A New Paradigm for Context-Adaptive Inference</h3>
<p>Recent advances in large-scale foundation models have fundamentally reshaped the landscape of context-adaptive inference. Trained on vast and diverse datasets with self-supervised objectives, these models internalize broad statistical regularities across language, vision, and multimodal data <span class="citation" data-cites="1GbAsSOZV">[<a href="#ref-1GbAsSOZV" role="doc-biblioref">69</a>]</span>. Unlike earlier approaches that relied on hand-crafted features or narrowly scoped models, foundation models can process and structure complex, high-dimensional contexts that were previously intractable.</p>
<p>Their impact is clear in natural language processing, where large language models achieve strong zero-shot and few-shot generalization, and in computer vision, where multimodal encoders such as CLIP align images and text into a shared representation space <span class="citation" data-cites="17tnf46zM">[<a href="#ref-17tnf46zM" role="doc-biblioref">68</a>]</span>. These advances mark a shift from treating feature extraction and inference as separate stages toward unified systems that function simultaneously as representation learners and adaptive engines. At the same time, challenges remain, including high computational demands, the risk of amplifying societal biases, and the difficulty of interpreting learned representations <span class="citation" data-cites="At7PZDaG">[<a href="#ref-At7PZDaG" role="doc-biblioref">211</a>]</span>.</p>
<p>To understand their contribution to context-adaptive inference, we consider three dimensions: their role as universal context encoders, the mechanisms enabling dynamic adaptation, and their integration with formal statistical and causal reasoning.</p>
<h4 id="universal-context-encoders">Universal Context Encoders</h4>
<p>Foundation models act as general-purpose context encoders, transforming raw, unstructured data into meaningful representations without manual feature engineering. For textual data, models such as BERT learn embeddings that capture semantic and syntactic nuances, supporting tasks from classification to retrieval <span class="citation" data-cites="urJgpE6q">[<a href="#ref-urJgpE6q" role="doc-biblioref">67</a>]</span>. For visual and multimodal inputs, CLIP aligns images and text into a shared embedding space, enabling zero-shot classification and cross-modal retrieval <span class="citation" data-cites="17tnf46zM">[<a href="#ref-17tnf46zM" role="doc-biblioref">68</a>]</span>.</p>
<p>These representations effectively serve as context variables—latent, structured features that can feed directly into statistical models. Classical approaches such as regression or causal inference can thus operate on data that would otherwise remain unstructured. This capacity forms the basis for integrating representation learning with formal frameworks of context-adaptive inference.</p>
<h4 id="dynamic-adaptation-mechanisms">Dynamic Adaptation Mechanisms</h4>
<p>Foundation models enable dynamic adaptation primarily at inference time, allowing models to respond to new tasks without retraining. The most prominent mechanism is in-context learning (ICL), where models adapt behavior by conditioning on examples in a prompt, enabling rapid few-shot or zero-shot generalization <span class="citation" data-cites="R7y5TKp9">[<a href="#ref-R7y5TKp9" role="doc-biblioref">143</a>]</span>.</p>
<p>Scaling is supported by modular architectures such as Mixture-of-Experts (MoE), which route inputs to specialized sub-networks for sparse activation, increasing capacity without proportional compute <span class="citation" data-cites="mFiH1ERh">[<a href="#ref-mFiH1ERh" role="doc-biblioref">212</a>]</span>. Parameter-efficient fine-tuning (PEFT) methods such as LoRA show that models can be adapted by updating less than one percent of weights, achieving near full fine-tuning performance <span class="citation" data-cites="Pf2hk1xb">[<a href="#ref-Pf2hk1xb" role="doc-biblioref">213</a>]</span>.</p>
<p>Together, these approaches illustrate how adaptation can be achieved both flexibly and efficiently, balancing generalization and computational constraints.</p>
<h4 id="bridging-with-statistical-and-causal-reasoning">Bridging with Statistical and Causal Reasoning</h4>
<p>An emerging research direction integrates the representational capacity of foundation models with the rigor of statistical and causal inference. Language models can already extract relational patterns from text to propose or critique causal graphs <span class="citation" data-cites="6ALrEKtt">[<a href="#ref-6ALrEKtt" role="doc-biblioref">214</a>]</span>. Methods such as LMPriors treat foundation models as task-specific priors, improving sample efficiency in estimation and decision making <span class="citation" data-cites="Xtwwrjzy">[<a href="#ref-Xtwwrjzy" role="doc-biblioref">215</a>]</span>. Models can also generate natural-language rationales that clarify predictions and summarize statistical findings, enhancing interpretability and transparency <span class="citation" data-cites="RvAOKYai">[<a href="#ref-RvAOKYai" role="doc-biblioref">169</a>]</span>.</p>
<p>Consequently, foundation models serve as bridges between flexible representation learning and principled inference, offering a path toward adaptive systems that are both data-efficient and theoretically grounded.</p>
<h3 id="next-generation-methods-for-contextualized-adaptive-inference">Next-Generation Methods for Contextualized Adaptive Inference</h3>
<p>While current foundation models already enable impressive forms of adaptivity, the next phase of research looks toward methods that will shape the future of contextualized adaptive inference. These directions point ahead, emphasizing how models may be adapted, combined, and evaluated. The aim is not only greater power, but also more transparency and reliability in high-stakes settings. We highlight three forward-looking methodological trends: modular fine tuning and compositional adaptation, mechanistic insights into in-context learning, and new frameworks for reliability and calibration.</p>
<h4 id="modular-fine-tuning-and-compositional-adaptation">Modular Fine-Tuning and Compositional Adaptation</h4>
<p>Parameter-efficient fine-tuning approaches such as adapters and LoRA show that large models can be customized by updating only a small subset of parameters while preserving pretrained knowledge <span class="citation" data-cites="Pf2hk1xb">[<a href="#ref-Pf2hk1xb" role="doc-biblioref">213</a>]</span>. Future systems are expected to expand these ideas into compositional strategies, dynamically combining specialized modules optimized for different domains or contexts <span class="citation" data-cites="dO6oWLlh">[<a href="#ref-dO6oWLlh" role="doc-biblioref">216</a>]</span>.</p>
<p>Recent findings suggest that merging multiple LoRA modules can even outperform full fine-tuning, signaling a paradigm where adaptation arises from modular reuse rather than retraining <span class="citation" data-cites="1AEMaqZx6">[<a href="#ref-1AEMaqZx6" role="doc-biblioref">217</a>]</span>. Compositional adaptation thus points toward building libraries of reusable context-specific skills that can be flexibly assembled for new tasks.</p>
<h4 id="in-context-learning-and-mechanistic-insights">In-Context Learning and Mechanistic Insights</h4>
<p>Although in-context learning has revolutionized generalization, its internal mechanisms remain partly opaque. Evidence suggests that transformers may implement optimization-like updates during forward passes, effectively performing implicit gradient descent when processing examples <span class="citation" data-cites="V6cbeqie">[<a href="#ref-V6cbeqie" role="doc-biblioref">142</a>]</span>. Other analyses interpret ICL as implicit Bayesian inference, where the prompt provides evidence that reshapes the predictive distribution <span class="citation" data-cites="iZG9n7mW">[<a href="#ref-iZG9n7mW" role="doc-biblioref">218</a>]</span>.</p>
<p>Mechanistic studies further identify induction heads within transformer attention circuits as critical components for pattern induction and few-shot generalization <span class="citation" data-cites="m1iAIGu4">[<a href="#ref-m1iAIGu4" role="doc-biblioref">139</a>]</span>. Such insights are expected to inspire architectures that enhance both transparency and stability in adaptive learning.</p>
<h4 id="reliability-calibration-and-context-sensitive-evaluation">Reliability, Calibration, and Context-Sensitive Evaluation</h4>
<p>As adaptive models become more flexible, ensuring calibration and reliability across shifting contexts becomes crucial. Deep neural networks, including LLMs, are often miscalibrated, producing overconfident probabilities misaligned with true accuracy <span class="citation" data-cites="zPozLA0K">[<a href="#ref-zPozLA0K" role="doc-biblioref">219</a>]</span>.</p>
<p>Future research will increasingly embed uncertainty quantification into adaptive pipelines through deep ensembles, Bayesian ensembling, or conformal prediction to produce valid confidence intervals <span class="citation" data-cites="PKjSQOD">[<a href="#ref-PKjSQOD" role="doc-biblioref">72</a>]</span>. Evaluation protocols must also stress robustness under distributional shifts, testing whether models can sustain performance and express uncertainty under novel or adversarial conditions <span class="citation" data-cites="XxvKtGf4">[<a href="#ref-XxvKtGf4" role="doc-biblioref">220</a>]</span>.</p>
<p>By embedding calibration and robustness within design, adaptive inference can evolve toward a more trustworthy, auditable, and context-aware standard.</p>
<h3 id="expanding-frameworks-with-foundation-models">Expanding Frameworks with Foundation Models</h3>
<p>Foundation models refer to large-scale, general-purpose neural networks, predominantly transformer-based architectures, trained on vast datasets using self-supervised learning <span class="citation" data-cites="1GbAsSOZV">[<a href="#ref-1GbAsSOZV" role="doc-biblioref">69</a>]</span>. Their flexibility, scalability, and cross-domain generalization have transformed statistical modeling and data analysis.</p>
<p>LLMs such as GPT-4 <span class="citation" data-cites="17lpGtuH5">[<a href="#ref-17lpGtuH5" role="doc-biblioref">221</a>]</span> and LLaMA-3.1 <span class="citation" data-cites="yWg7tQr1">[<a href="#ref-yWg7tQr1" role="doc-biblioref">222</a>]</span> exemplify this progress, achieving state-of-the-art results in language understanding, summarization, and reasoning. Beyond NLP, foundation models extend to multimodal tasks <span class="citation" data-cites="17tnf46zM">[<a href="#ref-17tnf46zM" role="doc-biblioref">68</a>]</span>, text embeddings <span class="citation" data-cites="urJgpE6q">[<a href="#ref-urJgpE6q" role="doc-biblioref">67</a>]</span>, and even tabular and structured data <span class="citation" data-cites="rYveVDKJ">[<a href="#ref-rYveVDKJ" role="doc-biblioref">223</a>]</span>.</p>
<p>Adaptivity in these systems is largely realized through prompting, which conditions responses on user-provided context without additional fine-tuning <span class="citation" data-cites="12nAa0T4v">[<a href="#ref-12nAa0T4v" role="doc-biblioref">224</a>]</span>. Meanwhile, Mixture-of-Experts (MoE) architectures enhance scalability by routing computation to relevant submodels for efficiency <span class="citation" data-cites="mFiH1ERh">[<a href="#ref-mFiH1ERh" role="doc-biblioref">212</a>]</span>.</p>
<h4 id="foundation-models-as-context">Foundation Models as Context</h4>
<p>Foundation models offer significant opportunities by supplying context-aware information that enhances various stages of statistical modeling and inference:</p>
<p><strong>Feature Extraction and Interpretation:</strong> Foundation models transform raw, unstructured data into structured and interpretable representations. For example, targeted prompts enable LLMs to extract insightful features from text, providing meaningful insights and facilitating interpretability <span class="citation" data-cites="11RvF4F7q">[<a href="#ref-kAJDlMwy" role="doc-biblioref">227</a>]</span>. This allows statistical models to operate directly on semantically meaningful features rather than on raw, less interpretable data.</p>
<p><strong>Contextualized Representations for Downstream Modeling:</strong> Foundation models produce adaptable embeddings and intermediate representations useful as inputs for downstream models, such as decision trees or linear models <span class="citation" data-cites="R7y5TKp9">[<a href="#ref-R7y5TKp9" role="doc-biblioref">143</a>]</span>. These embeddings significantly enhance the training of both complex, black-box models <span class="citation" data-cites="1AazNaZYl">[<a href="#ref-1AazNaZYl" role="doc-biblioref">228</a>]</span> and simpler statistical methods like n-gram-based analyses <span class="citation" data-cites="HQXzkG4Q">[<a href="#ref-HQXzkG4Q" role="doc-biblioref">229</a>]</span>, thereby broadening the application scope and effectiveness of statistical approaches.</p>
<p><strong>Post-hoc Interpretability:</strong> Foundation models support interpretability by generating natural-language explanations for decisions made by complex models. This capability enhances transparency and trust in statistical inference, providing clear insights into how and why certain predictions or decisions are made <span class="citation" data-cites="XpHq6HEw">[<a href="#ref-XpHq6HEw" role="doc-biblioref">230</a>]</span>.</p>
<h4 id="recent-innovations-and-outlook">Recent Innovations and Outlook</h4>
<p>Several new architectures exemplify how foundation models advance context-sensitive inference through modularity and interpretability:</p>
<p><strong>FLAN-MoE</strong> (Fine-tuned Language Model with Mixture of Experts) <span class="citation" data-cites="zWwbz3cX">[<a href="#ref-zWwbz3cX" role="doc-biblioref">231</a>]</span> combines instruction tuning with expert selection, dynamically activating relevant sub-models based on the context. This method significantly improves performance across diverse NLP tasks, offering superior few-shot and zero-shot capabilities. It also facilitates interpretability through explicit expert activations. Future directions may explore advanced expert-selection techniques and multilingual capabilities.</p>
<p><strong>LMPriors</strong> (Pre-Trained Language Models as Task-Specific Priors) <span class="citation" data-cites="Xtwwrjzy">[<a href="#ref-Xtwwrjzy" role="doc-biblioref">215</a>]</span> leverages semantic insights from pre-trained models like GPT-3 to guide tasks such as causal inference, feature selection, and reinforcement learning. This method markedly enhances decision accuracy and efficiency without requiring extensive supervised datasets. However, it necessitates careful prompt engineering to mitigate biases and ethical concerns.</p>
<p><strong>Mixture of In-Context Experts</strong> (MoICE) <span class="citation" data-cites="Xtwwrjzy">[<a href="#ref-Xtwwrjzy" role="doc-biblioref">215</a>]</span> introduces a dynamic routing mechanism within attention heads, utilizing multiple Rotary Position Embeddings (RoPE) angles to effectively capture token positions in sequences. MoICE significantly enhances performance on long-context sequences and retrieval-augmented generation tasks by ensuring complete contextual coverage. Efficiency is achieved through selective router training, and interpretability is improved by explicitly visualizing attention distributions, providing detailed insights into the model’s reasoning process.</p>
<p>Collectively, these directions suggest a future in which foundation models evolve from passive representation learners into active, context-sensitive inference engines that unify adaptivity, efficiency, and interpretability within a principled framework.</p>
<h2 id="open-problems">Open Problems</h2>
<p>Rapid advances in context-adaptive modeling have created unprecedented opportunities while revealing fundamental challenges. This chapter identifies the central methodological questions and the broader ethical and societal challenges that will shape the future trajectory of context-adaptive inference. We begin by examining five interrelated technical questions—on modularity, the benefits of explicit structure, the level of abstraction, theoretical and practical barriers, and interpretability trade-offs—that together define the frontier of adaptive modeling research. We then turn to the broader outlook, focusing on the ethical and societal implications of deploying these powerful adaptive systems.</p>
<h3 id="open-research-questions">Open Research Questions</h3>
<p>Recent advances have broadened the scope of adaptive inference, but many questions remain unresolved. These open problems span five domains: (i) modularity and reusability of adaptive components, (ii) the conditions under which explicit structure improves robustness and generalization, (iii) the appropriate level of abstraction for intervention, (iv) theoretical, computational, and data-related barriers to adoption, and (v) the tension between interpretable-by-design and post-hoc interpretability. Together, these questions delineate a research agenda that bridges theoretical statistics, machine learning, and applied modeling, combining methodological depth with practical impact.</p>
<p>First, researchers need to examine whether skills and routines can be modularized in a way that allows portability across tasks without interference. Second, the field must clarify under what conditions explicit structure provides measurable benefits. Third, it remains unclear at which level of abstraction such structure should be imposed, whether at the level of parameters, functions, or latent factors. Fourth, adoption is limited by both theoretical and practical barriers, including identifiability, generalization, and computational feasibility. Finally, the community must address the tension between building models that are interpretable from the start and those that rely on post-hoc explanations. The following subsections provide a more detailed discussion of these five questions.</p>
<h4 id="can-reusable-modules-enable-portability-across-tasks">Can Reusable Modules Enable Portability Across Tasks?</h4>
<p>A central question is whether the skills or routines acquired by large models can be isolated and reused as portable modules across tasks without reducing overall performance <span class="citation" data-cites="1GbAsSOZV">[<a href="#ref-1GbAsSOZV" role="doc-biblioref">69</a>]</span>. The vision of modularity is to build an ecosystem of specialized components that can be composed when needed, instead of training a new large model for each task. Promising approaches operate at different levels: (i) representation-level constraints such as concept bottlenecks enforcing human-understandable features; (ii) memory-based mechanisms such as prototype libraries for case retrieval; and (iii) architecture-level designs such as sparse adapters or routing networks that activate context-relevant modules <span class="citation" data-cites="UskQdlu3 R3AtGoca">[<a href="#ref-UskQdlu3" role="doc-biblioref">77</a>,<a href="#ref-R3AtGoca" role="doc-biblioref">232</a>]</span>.</p>
<p>Applications illustrate the promise of this research. In healthcare, diagnostic modules could be reused across diseases. In natural language processing, syntax-aware modules might be applied across languages. However, modularity also introduces risks: interactions between modules may cause interference or instability in generalization, and poorly aligned components may propagate or amplify existing biases. Future work should therefore design evaluation protocols that test not only portability and composability, but also isolation of unintended side effects and robustness to distribution shift <span class="citation" data-cites="yWg7tQr1">[<a href="#ref-yWg7tQr1" role="doc-biblioref">222</a>]</span>.</p>
<h4 id="what-are-the-theoretical-and-practical-benefits-of-explicit-structure">What Are the Theoretical and Practical Benefits of Explicit Structure?</h4>
<p>Clarifying the theoretical and practical benefits of explicit structure is an important open question. Implicit adaptation is highly flexible, but explicit structure may provide stronger guarantees of robustness and generalization under distribution shift. Practical benefits include greater interpretability, improved debugging, and the ability to incorporate domain knowledge directly.</p>
<p>To advance this agenda, systematic comparisons with implicit approaches are needed. Stress testing under covariate shift, concept drift, long-tail distributions, and adversarial correlations is particularly important, and benchmarks such as WILDS provide a useful starting point <span class="citation" data-cites="PKjSQOD">[<a href="#ref-PKjSQOD" role="doc-biblioref">72</a>]</span>. At the same time, researchers must weigh the costs of explicit structure. These costs include additional annotation, increased hyperparameter complexity, and potential reductions in in-domain accuracy <span class="citation" data-cites="4fnOdPts">[<a href="#ref-11l8svMmM" role="doc-biblioref">88</a>]</span>. A comprehensive evaluation framework that quantifies both theoretical guarantees and practical trade-offs remains to be established.</p>
<h4 id="at-what-level-of-abstraction-should-explicit-structure-be-imposed">At What Level of Abstraction Should Explicit Structure Be Imposed?</h4>
<p>Determining the appropriate level of abstraction for intervention remains a challenge. Parameter-level edits provide precise control but are brittle and can have unpredictable side effects <span class="citation" data-cites="16Xv40Ngd">[<a href="#ref-16Xv40Ngd" role="doc-biblioref">135</a>]</span>. Concept-level interventions provide stability and interpretability but may fail to capture the model’s internal computations in full detail <span class="citation" data-cites="J9eXUymQ">[<a href="#ref-J9eXUymQ" role="doc-biblioref">233</a>]</span>.</p>
<p>Intermediate levels may offer a balance. For example, function-level interventions or local surrogate models can capture mid-level abstractions that combine precision with stability. More importantly, future work should aim to develop methods that allow translation across levels. For instance, low-level parameter edits could be distilled into high-level conceptual summaries, while abstract concepts might be operationalized through concrete parameter changes. Such tools would make adaptive models more interpretable and more controllable in practice.</p>
<h4 id="what-theoretical-and-practical-barriers-remain">What Theoretical and Practical Barriers Remain?</h4>
<p>Several barriers continue to limit the adoption of adaptive models. On the theoretical side, researchers have yet to establish strong guarantees for identifiability and generalization under distribution shift <span class="citation" data-cites="11l8svMmM">[<a href="#ref-11l8svMmM" role="doc-biblioref">88</a>]</span>. Extending these guarantees to high-dimensional and multimodal data remains an unsolved challenge.</p>
<p>Practical barriers are equally important. Training and deploying adaptive models requires significant computational and memory resources. Data limitations, such as biased sampling and noisy feedback, reduce reliability. Evaluation frameworks remain centered on accuracy, with insufficient attention to fairness, stability, and long-term robustness. Finally, the absence of standardized tools and implementation guidelines prevents many practitioners from applying state-of-the-art methods beyond research settings <span class="citation" data-cites="hRp04fhf">[<a href="#ref-hRp04fhf" role="doc-biblioref">102</a>]</span>.</p>
<h4 id="interpretable-by-design-vs-post-hoc-interpretability-what-is-the-right-path-forward">Interpretable-by-Design vs Post-hoc Interpretability: What Is the Right Path Forward?</h4>
<p>A final open question concerns the balance between interpretable-by-design approaches and post-hoc interpretability. Interpretable-by-design models, such as varying coefficient models, provide transparency and faithfulness from the outset but may restrict predictive performance <span class="citation" data-cites="ugXwusl0">[<a href="#ref-ugXwusl0" role="doc-biblioref">1</a>]</span>. Post-hoc methods allow powerful foundation models to be explained after training, but explanations may be incomplete or unfaithful to the model’s internal reasoning <span class="citation" data-cites="1GbAsSOZV">[<a href="#ref-1GbAsSOZV" role="doc-biblioref">69</a>]</span>.</p>
<p>Progress in both directions suggests that the future lies in integration rather than a binary choice. Hybrid models may embed interpretable structures at their core while using post-hoc tools for flexibility. Promising directions include benchmarks that jointly evaluate adaptivity and interpretability, as well as human-in-the-loop workflows that allow domain experts to constrain and validate model adaptation in practice.</p>
<h3 id="broader-challenges-and-future-outlook">Broader Challenges and Future Outlook</h3>
<p>Emerging paradigms such as Agentic Context Engineering (ACE) push this vision further by treating the context itself as an adaptive, evolving entity. In this framework, language models continuously refine and regenerate their own contexts through feedback, reflection, and planning, enabling self-improving adaptation cycles across time <span class="citation" data-cites="1Ft0Gvajp">[<a href="#ref-1Ft0Gvajp" role="doc-biblioref">234</a>]</span>. While the previous section focused on research questions that can be addressed by new methods, theory, and experiments, broader challenges remain that extend beyond purely technical considerations. These challenges concern the responsible deployment of adaptive models in real-world environments, where issues such as ethics, fairness, and regulatory compliance play a critical role. Adaptive systems used in sensitive domains such as healthcare and finance must satisfy principles of interpretability, auditability, and accountability to prevent harm and maintain public trust <span class="citation" data-cites="1GbAsSOZV">[<a href="#ref-1GbAsSOZV" role="doc-biblioref">69</a>]</span>. Collaboration between regulators, practitioners, and researchers is essential to establish transparent auditing standards and verifiable documentation for adaptive decisions.</p>
<p>Another set of challenges arises from the dynamic interaction between adaptive models and their environments. Feedback loops may amplify small initial biases, leading to systematic disadvantages for certain groups over time. Examples can be seen in credit scoring, hiring, and online recommendation systems, where early decisions influence future data collection and can entrench inequalities <span class="citation" data-cites="MGkiKe9y">[<a href="#ref-MGkiKe9y" role="doc-biblioref">89</a>]</span>. Addressing these risks requires methods that anticipate long-term effects, including simulation studies, formal analyses of dynamic systems, and model designs that incorporate fairness constraints directly during learning.</p>
<p>Looking ahead, the long-term vision for adaptive modeling is to develop systems that are not only powerful but also trustworthy. Progress requires moving beyond accuracy as the dominant evaluation criterion to include fairness, stability, and transparency. Human oversight should be an integral part of adaptive pipelines, enabling experts to guide and validate model behavior in practice. Sustainability is another important dimension, as the computational and environmental costs of adaptive models continue to grow. By combining technical innovation with responsible deployment, the field can ensure that adaptive inference contributes to both scientific progress and societal benefit.</p>
<h2 id="conclusion">Conclusion</h2>
<h3 id="overview-of-insights">Overview of Insights</h3>
<p>This review established a unifying framework for understanding context-adaptive inference across both explicit statistical models and implicit adaptation in modern foundation models. By tracing how adaptation appears in parameterized functions such as varying-coefficient models and in emergent processes like in-context learning, we showed that these paradigms share a common estimator form and theoretical foundation.</p>
<p>Across the literature, a consistent pattern emerges: adaptivity becomes effective when context, computation, and interpretation are aligned. The principles of context-aware efficiency integrate these aspects, clarifying when adaptation enhances robustness and when it introduces instability. Within this perspective, model design choices can be connected to measurable outcomes such as data efficiency, modularity, and transferability, grounding the abstract notion of adaptivity in verifiable performance.</p>
<p>The unified view presented in this review connects statistical inference with ideas from machine learning and cognitive modeling, where adaptive reasoning and context-sensitive generalization are regarded as key components of intelligent behavior. Cognitive theories have long emphasized that efficient adaptation arises from internal models that balance precision and flexibility, an idea now mirrored in recent computational analyses of in-context learning <span class="citation" data-cites="Hhj8Woby">[<a href="#ref-Hhj8Woby" role="doc-biblioref">138</a>]</span>. By bridging these perspectives, this framework provides both a conceptual foundation and a practical guide for developing adaptive systems that are interpretable, reliable, and scalable.</p>
<h4 id="context-aware-efficiency-a-unifying-framework">Context-Aware Efficiency: A Unifying Framework</h4>
<p>The principles of context-aware efficiency emerge as a unifying theme across the diverse methods surveyed in this review. This framework provides a systematic approach to designing methods that are both computationally tractable and statistically principled.</p>
<p>Several fundamental insights emerge from our analysis. Rather than being a nuisance parameter, context provides information that can be leveraged to improve both statistical and computational efficiency. Methods that adapt their computational strategy based on context often achieve better performance than those that use fixed approaches. The design of context-aware methods requires careful consideration of how to balance computational efficiency with interpretability and regulatory compliance.</p>
<p>Recent studies also demonstrate that context-adaptive strategies can emerge spontaneously in large models trained on diverse tasks, linking computational efficiency to rational inference principles <span class="citation" data-cites="IncFoA7e">[<a href="#ref-IncFoA7e" role="doc-biblioref">140</a>]</span>. These findings suggest that implicit adaptation can serve as a computational analog of Bayesian updating, where context dynamically reweights prior knowledge to improve generalization. Similar ideas have been explored in meta-learning frameworks such as MetaICL, which meta-trains language models to acquire reusable adaptation strategies through exposure to varied task distributions <span class="citation" data-cites="qvjULhAd">[<a href="#ref-qvjULhAd" role="doc-biblioref">134</a>]</span>.</p>
<p>Future research in context-aware efficiency should focus on developing methods that can efficiently handle high-dimensional, multimodal context information, creating systems that can adaptively allocate computational resources based on context complexity and urgency, investigating how efficiency principles learned in one domain can be transferred to others, and ensuring that context-aware efficiency methods can be deployed in regulated environments while maintaining interpretability <span class="citation" data-cites="1Ft0Gvajp">[<a href="#ref-1Ft0Gvajp" role="doc-biblioref">234</a>]</span>.</p>
<p>The development of context-aware efficiency principles has implications beyond statistical modeling. More efficient methods reduce computational costs and environmental impact, enabling sustainable computing practices. Efficient methods also democratize AI by enabling deployment of sophisticated models on resource-constrained devices. Furthermore, context-aware efficiency enables deployment of personalized models in time-critical applications, supporting real-time decision making.</p>
<p>As we move toward an era of increasingly personalized and context-aware statistical inference, the principles outlined in this review provide a foundation for developing methods that are both theoretically sound and practically useful.</p>
<h3 id="future-directions">Future Directions</h3>
<p>Looking ahead, the evolution of context-adaptive inference will likely proceed along four interconnected paths.</p>
<h4 id="theoretical-foundations">Theoretical Foundations</h4>
<p>Future research should formalize implicit adaptation within a consistent statistical framework, linking neural computation to principles of efficiency, identifiability, and invariance. Clarifying these theoretical connections will support better understanding of when implicit adaptation approximates explicit statistical reasoning and how both approaches can be integrated. Recent advances have begun to view in-context learning as an emergent form of structure induction, suggesting that large models implicitly learn compositional representations that approximate rational inference processes <span class="citation" data-cites="Hhj8Woby">[<a href="#ref-Hhj8Woby" role="doc-biblioref">138</a>]</span>.</p>
<h4 id="modular-and-compositional-methods">Modular and Compositional Methods</h4>
<p>Progress in parameter-efficient fine-tuning, compositional adaptation, and reusable modules will make large models more flexible and controllable. Building libraries of specialized components that can be dynamically combined will promote efficient reuse and domain transfer while maintaining interpretability. Work on tabular in-context learning, such as the TabICL architecture, illustrates how these principles can scale to structured data domains while preserving modular control and generalization <span class="citation" data-cites="1DBdzlHLI">[<a href="#ref-1DBdzlHLI" role="doc-biblioref">145</a>]</span>.</p>
<h4 id="evaluation-and-reliability">Evaluation and Reliability</h4>
<p>Developing standardized benchmarks that jointly assess robustness, calibration, and interpretability is essential for advancing both theory and application. Future evaluation frameworks should emphasize context-stratified performance, long-term stability, and transparent reporting of adaptation behavior under distribution shifts. Ongoing analyses of the stability and transience of in-context strategies <span class="citation" data-cites="IncFoA7e">[<a href="#ref-IncFoA7e" role="doc-biblioref">140</a>]</span> underscore the importance of evaluating not only short-term generalization but also the persistence and reproducibility of adaptive behavior across training regimes.</p>
<h4 id="responsible-and-sustainable-deployment">Responsible and Sustainable Deployment</h4>
<p>As adaptive systems become embedded in decision-making processes, integrating fairness auditing, human oversight, and energy efficiency into their design will be critical for ensuring public trust. Addressing the environmental cost of large-scale adaptation and developing resource-conscious algorithms will also contribute to sustainable computing practices. Emerging work on efficient foundation models and rational adaptation frameworks <span class="citation" data-cites="1Ft0Gvajp">[<a href="#ref-1Ft0Gvajp" role="doc-biblioref">234</a>]</span> highlights how technical design and ethical responsibility can be jointly optimized in real-world deployment.</p>
<p>Together, these directions outline a path toward the next generation of adaptive models that are both powerful and trustworthy. Progress will depend on combining rigorous statistical understanding with transparent design and responsible deployment, moving steadily toward the broader goal of making implicit adaptation explicit and accountable.</p>
<h2 class="page_break_before" id="references">References</h2>
<!-- Explicitly insert bibliography here -->
<div id="refs" class="references csl-bib-body" role="doc-bibliography">
<div id="ref-ugXwusl0" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">1. </div><div class="csl-right-inline"><strong>Varying-Coefficient Models</strong> <div class="csl-block">Trevor Hastie, Robert Tibshirani</div> <em>Journal of the Royal Statistical Society Series B: Statistical Methodology</em> (1993-09-01) <a href="https://doi.org/gmfvmb">https://doi.org/gmfvmb</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1111/j.2517-6161.1993.tb01939.x">10.1111/j.2517-6161.1993.tb01939.x</a></div></div>
</div>
<div id="ref-TULLRYDp" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">2. </div><div class="csl-right-inline"><strong>Bayesian Edge Regression in Undirected Graphical Models to Characterize Interpatient Heterogeneity in Cancer</strong> <div class="csl-block">Zeya Wang, Veerabhadran Baladandayuthapani, Ahmed O Kaseb, Hesham M Amin, Manal M Hassan, Wenyi Wang, Jeffrey S Morris</div> <em>Journal of the American Statistical Association</em> (2022-01-05) <a href="https://doi.org/gt68hr">https://doi.org/gt68hr</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1080/01621459.2021.2000866">10.1080/01621459.2021.2000866</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/36090952">36090952</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9454401">PMC9454401</a></div></div>
</div>
<div id="ref-l6vMkIsa" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">3. </div><div class="csl-right-inline"><strong>Statistical estimation in varying coefficient models</strong> <div class="csl-block">Jianqing Fan, Wenyang Zhang</div> <em>The Annals of Statistics</em> (1999-10-01) <a href="https://doi.org/dsxd4s">https://doi.org/dsxd4s</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1214/aos/1017939139">10.1214/aos/1017939139</a></div></div>
</div>
<div id="ref-1CkxORTSX" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">4. </div><div class="csl-right-inline"><strong>Time-Varying Coefficient Model Estimation Through Radial Basis Functions</strong> <div class="csl-block">Juan Sosa, Lina Buitrago</div> <em>arXiv</em> (2021-03-02) <a href="https://arxiv.org/abs/2103.00315">https://arxiv.org/abs/2103.00315</a></div>
</div>
<div id="ref-SfCo6pSp" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">5. </div><div class="csl-right-inline"><strong>Contextual Explanation Networks</strong> <div class="csl-block">Maruan Al-Shedivat, Avinava Dubey, Eric P Xing</div> <em>arXiv</em> (2017) <a href="https://doi.org/gt68h9">https://doi.org/gt68h9</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.1705.10301">10.48550/arxiv.1705.10301</a></div></div>
</div>
<div id="ref-HYsEq2UQ" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">6. </div><div class="csl-right-inline"><strong>Contextualized Machine Learning</strong> <div class="csl-block">Benjamin Lengerich, Caleb N Ellington, Andrea Rubbi, Manolis Kellis, Eric P Xing</div> <em>arXiv</em> (2023) <a href="https://doi.org/gt68jg">https://doi.org/gt68jg</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2310.11340">10.48550/arxiv.2310.11340</a></div></div>
</div>
<div id="ref-4cK1tiec" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">7. </div><div class="csl-right-inline"><strong>Contextualized: Heterogeneous Modeling Toolbox</strong> <div class="csl-block">Caleb N Ellington, Benjamin J Lengerich, Wesley Lo, Aaron Alvarez, Andrea Rubbi, Manolis Kellis, Eric P Xing</div> <em>Journal of Open Source Software</em> (2024-05-08) <a href="https://doi.org/gt68h8">https://doi.org/gt68h8</a> <div class="csl-block">DOI: <a href="https://doi.org/10.21105/joss.06469">10.21105/joss.06469</a></div></div>
</div>
<div id="ref-Rt6voTFN" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">8. </div><div class="csl-right-inline"><strong>Learning to Estimate Sample-specific Transcriptional Networks for 7000 Tumors</strong> <div class="csl-block">Caleb N Ellington, Benjamin J Lengerich, Thomas BK Watkins, Jiekun Yang, Abhinav Adduri, Sazan Mahbub, Hanxi Xiao, Manolis Kellis, Eric P Xing</div> <em>openRxiv</em> (2023-12-04) <a href="https://doi.org/gt68h7">https://doi.org/gt68h7</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1101/2023.12.01.569658">10.1101/2023.12.01.569658</a></div></div>
</div>
<div id="ref-grNza1Og" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">9. </div><div class="csl-right-inline"><strong>NOTMAD: Estimating Bayesian Networks with Sample-Specific Structures and Parameters</strong> <div class="csl-block">Ben Lengerich, Caleb Ellington, Bryon Aragam, Eric P Xing, Manolis Kellis</div> <em>arXiv</em> (2021) <a href="https://doi.org/gt68jc">https://doi.org/gt68jc</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2111.01104">10.48550/arxiv.2111.01104</a></div></div>
</div>
<div id="ref-nYipTPML" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">10. </div><div class="csl-right-inline"><strong>Contextualized Policy Recovery: Modeling and Interpreting Medical Decisions with Adaptive Imitation Learning</strong> <div class="csl-block">Jannik Deuschel, Caleb N Ellington, Yingtao Luo, Benjamin J Lengerich, Pascal Friederich, Eric P Xing</div> <em>arXiv</em> (2023) <a href="https://doi.org/gt68jf">https://doi.org/gt68jf</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2310.07918">10.48550/arxiv.2310.07918</a></div></div>
</div>
<div id="ref-esxxcr9l" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">11. </div><div class="csl-right-inline"><strong>Automated interpretable discovery of heterogeneous treatment effectiveness: A COVID-19 case study</strong> <div class="csl-block">Benjamin J Lengerich, Mark E Nunnally, Yin Aphinyanaphongs, Caleb Ellington, Rich Caruana</div> <em>Journal of Biomedical Informatics</em> (2022-06) <a href="https://doi.org/gt68h5">https://doi.org/gt68h5</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1016/j.jbi.2022.104086">10.1016/j.jbi.2022.104086</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/35504543">35504543</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9055753">PMC9055753</a></div></div>
</div>
<div id="ref-O1UU4a5P" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">12. </div><div class="csl-right-inline"><strong>Discriminative Subtyping of Lung Cancers from Histopathology Images via Contextual Deep Learning</strong> <div class="csl-block">Benjamin J Lengerich, Maruan Al-Shedivat, Amir Alavi, Jennifer Williams, Sami Labbaki, Eric P Xing</div> <em>openRxiv</em> (2020-06-26) <a href="https://doi.org/gt68h6">https://doi.org/gt68h6</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1101/2020.06.25.20140053">10.1101/2020.06.25.20140053</a></div></div>
</div>
<div id="ref-9S6tI5yv" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">13. </div><div class="csl-right-inline"><strong>Contextual Feature Selection with Conditional Stochastic Gates</strong> <div class="csl-block">Ram Dyuthi Sristi, Ofir Lindenbaum, Shira Lifshitz, Maria Lavzin, Jackie Schiller, Gal Mishne, Hadas Benisty</div> <em>arXiv</em> (2023) <a href="https://doi.org/gt68jh">https://doi.org/gt68jh</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2312.14254">10.48550/arxiv.2312.14254</a></div></div>
</div>
<div id="ref-lAsTg3IH" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">14. </div><div class="csl-right-inline"><strong>Estimating time-varying networks</strong> <div class="csl-block">Mladen Kolar, Le Song, Amr Ahmed, Eric P Xing</div> <em>The Annals of Applied Statistics</em> (2010-03-01) <a href="https://doi.org/b3rn6q">https://doi.org/b3rn6q</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1214/09-aoas308">10.1214/09-aoas308</a></div></div>
</div>
<div id="ref-k6r0UwSv" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">15. </div><div class="csl-right-inline"><strong>When Personalization Harms: Reconsidering the Use of Group Attributes in Prediction</strong> <div class="csl-block">Vinith M Suriyakumar, Marzyeh Ghassemi, Berk Ustun</div> <em>arXiv</em> (2022) <a href="https://doi.org/gt68jd">https://doi.org/gt68jd</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2206.02058">10.48550/arxiv.2206.02058</a></div></div>
</div>
<div id="ref-WlwUpYp" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">16. </div><div class="csl-right-inline"><strong>Learning Sample-Specific Models with Low-Rank Personalized Regression</strong> <div class="csl-block">Benjamin Lengerich, Bryon Aragam, Eric P Xing</div> <em>arXiv</em> (2019) <a href="https://doi.org/gt68jb">https://doi.org/gt68jb</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.1910.06939">10.48550/arxiv.1910.06939</a></div></div>
</div>
<div id="ref-HzzgJQN0" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">17. </div><div class="csl-right-inline"><strong>Sketch-Based Anomaly Detection in Streaming Graphs</strong> <div class="csl-block">Siddharth Bhatia, Mohit Wadhwa, Kenji Kawaguchi, Neil Shah, Philip S Yu, Bryan Hooi</div> <em>arXiv</em> (2023-07-18) <a href="https://arxiv.org/abs/2106.04486">https://arxiv.org/abs/2106.04486</a></div>
</div>
<div id="ref-17ryvnoPR" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">18. </div><div class="csl-right-inline"><strong>The Design of Experiments</strong> <div class="csl-block">Ronald A Fisher</div> <em>Oliver &amp; Boyd</em> (1949)</div>
</div>
<div id="ref-8BCLzHII" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">19. </div><div class="csl-right-inline"><strong>Nouvelles méthodes pour la détermination des orbites des comètes</strong> <div class="csl-block">Adrien-Marie Legendre</div> <em>Courcier</em> (1805)</div>
</div>
<div id="ref-vKtb27Ba" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">20. </div><div class="csl-right-inline"><strong>Theoria motus corporum coelestium in sectionibus conicis solem ambientium</strong> <div class="csl-block">Carl Friedrich Gauss</div> <em>Perthes &amp; Besser</em> (1809)</div>
</div>
<div id="ref-kXBVO9y8" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">21. </div><div class="csl-right-inline"><strong>Generalized Linear Models</strong> <div class="csl-block">JA Nelder, RWM Wedderburn</div> <em>Journal of the Royal Statistical Society. Series A (General)</em> (1972) <a href="https://doi.org/dhq253">https://doi.org/dhq253</a> <div class="csl-block">DOI: <a href="https://doi.org/10.2307/2344614">10.2307/2344614</a></div></div>
</div>
<div id="ref-1HWqsAX0j" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">22. </div><div class="csl-right-inline"><strong>Generalized Linear Models</strong> <div class="csl-block">P McCullagh, JA Nelder</div> <em>Springer US</em> (1989) <a href="https://doi.org/brhr">https://doi.org/brhr</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1007/978-1-4899-3242-6">10.1007/978-1-4899-3242-6</a></div></div>
</div>
<div id="ref-15YaJttEf" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">23. </div><div class="csl-right-inline"><strong>Application of the Logistic Function to Bio-Assay</strong> <div class="csl-block">Joseph Berkson</div> <em>Journal of the American Statistical Association</em> (1944-09) <a href="https://doi.org/gn82f3">https://doi.org/gn82f3</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1080/01621459.1944.10500699">10.1080/01621459.1944.10500699</a></div></div>
</div>
<div id="ref-fzXXj6RE" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">24. </div><div class="csl-right-inline"><strong>The Regression Analysis of Binary Sequences</strong> <div class="csl-block">DR Cox</div> <em>Journal of the Royal Statistical Society Series B: Statistical Methodology</em> (1958-07-01) <a href="https://doi.org/gfzf66">https://doi.org/gfzf66</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1111/j.2517-6161.1958.tb00292.x">10.1111/j.2517-6161.1958.tb00292.x</a></div></div>
</div>
<div id="ref-18GHyIdqs" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">25. </div><div class="csl-right-inline"><strong>Statistical Methods for Research Workers</strong> <div class="csl-block">Ronald A Fisher</div> <em>Oliver &amp; Boyd</em> (1925)</div>
</div>
<div id="ref-1FmrthAK7" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">26. </div><div class="csl-right-inline"><strong>Herd effects on the growth of beef bulls from different sources tested together under grazing conditions</strong> <div class="csl-block">CA Morris</div> <em>New Zealand Journal of Agricultural Research</em> (1981-01) <a href="https://doi.org/fxp28n">https://doi.org/fxp28n</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1080/00288233.1981.10420865">10.1080/00288233.1981.10420865</a></div></div>
</div>
<div id="ref-3oGysJy4" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">27. </div><div class="csl-right-inline"><strong>Construct validity in psychological tests.</strong> <div class="csl-block">Lee J Cronbach, Paul E Meehl</div> <em>Psychological Bulletin</em> (1955-07) <a href="https://doi.org/dcsjjf">https://doi.org/dcsjjf</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1037/h0040957">10.1037/h0040957</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/13245896">13245896</a></div></div>
</div>
<div id="ref-1GKMYvnqB" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">28. </div><div class="csl-right-inline"><strong>Estimation of Genetic Parameters</strong> <div class="csl-block">Charles R Henderson</div> <em>Annals of Mathematical Statistics</em> (1950)</div>
</div>
<div id="ref-pC7P4enS" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">29. </div><div class="csl-right-inline"><strong>A Method of Estimating Comparative Rates from Clinical Data. Applications to Cancer of the Lung, Breast, and Cervix</strong> <div class="csl-block">JNCI: Journal of the National Cancer Institute</div> (1951-06) <a href="https://doi.org/g96wsb">https://doi.org/g96wsb</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1093/jnci/11.6.1269">10.1093/jnci/11.6.1269</a></div></div>
</div>
<div id="ref-xPNSAM4v" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">30. </div><div class="csl-right-inline"><strong>Recovery of inter-block information when block sizes are unequal</strong> <div class="csl-block">HD PATTERSON, R THOMPSON</div> <em>Biometrika</em> (1971) <a href="https://doi.org/c473hg">https://doi.org/c473hg</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1093/biomet/58.3.545">10.1093/biomet/58.3.545</a></div></div>
</div>
<div id="ref-7EbnZ6mY" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">31. </div><div class="csl-right-inline"><strong>Random-Effects Models for Longitudinal Data</strong> <div class="csl-block">Nan M Laird, James H Ware</div> <em>Biometrics</em> (1982-12) <a href="https://doi.org/b8dmsr">https://doi.org/b8dmsr</a> <div class="csl-block">DOI: <a href="https://doi.org/10.2307/2529876">10.2307/2529876</a></div></div>
</div>
<div id="ref-iS2cDavY" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">32. </div><div class="csl-right-inline"><strong>Estimation in generalized linear models with random effects</strong> <div class="csl-block">ROBERT SCHALL</div> <em>Biometrika</em> (1991) <a href="https://doi.org/bhxfht">https://doi.org/bhxfht</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1093/biomet/78.4.719">10.1093/biomet/78.4.719</a></div></div>
</div>
<div id="ref-sPnUGQ8l" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">33. </div><div class="csl-right-inline"><strong>Approximate Inference in Generalized Linear Mixed Models</strong> <div class="csl-block">NE Breslow, DG Clayton</div> <em>Journal of the American Statistical Association</em> (1993-03) <a href="https://doi.org/ggnhwn">https://doi.org/ggnhwn</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1080/01621459.1993.10594284">10.1080/01621459.1993.10594284</a></div></div>
</div>
<div id="ref-mLMS4pAF" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">34. </div><div class="csl-right-inline"><strong>Bayes Estimates for the Linear Model</strong> <div class="csl-block">DV Lindley, AFM Smith</div> <em>Journal of the Royal Statistical Society Series B: Statistical Methodology</em> (1972-09-01) <a href="https://doi.org/gg2vv3">https://doi.org/gg2vv3</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1111/j.2517-6161.1972.tb00885.x">10.1111/j.2517-6161.1972.tb00885.x</a></div></div>
</div>
<div id="ref-SXlx1fmn" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">35. </div><div class="csl-right-inline"><strong>Stochastic Relaxation, Gibbs Distributions, and the Bayesian Restoration of Images</strong> <div class="csl-block">Stuart Geman, Donald Geman</div> <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em> (1984-11) <a href="https://doi.org/bpv4j5">https://doi.org/bpv4j5</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1109/tpami.1984.4767596">10.1109/tpami.1984.4767596</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/22499653">22499653</a></div></div>
</div>
<div id="ref-Jk46kTvA" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">36. </div><div class="csl-right-inline"><strong>Bayesian Data Analysis</strong> <div class="csl-block">Andrew Gelman, John B Carlin, Hal S Stern, David B Dunson, Aki Vehtari, Donald B Rubin</div> <em>Chapman and Hall/CRC</em> (2013-11-27) <a href="https://doi.org/gqfx8g">https://doi.org/gqfx8g</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1201/b16018">10.1201/b16018</a></div></div>
</div>
<div id="ref-OXCuJonU" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">37. </div><div class="csl-right-inline"><strong>Functional Data Analysis</strong> <div class="csl-block">James Ramsay</div> <em>Encyclopedia of Statistics in Behavioral Science</em> (2005-04-15) <a href="https://doi.org/b9tmj6">https://doi.org/b9tmj6</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1002/0470013192.bsa239">10.1002/0470013192.bsa239</a></div></div>
</div>
<div id="ref-12q6JvZEW" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">38. </div><div class="csl-right-inline"><strong>Generalized Additive Models</strong> <div class="csl-block">Trevor Hastie, Robert Tibshirani</div> <em>Statistical Science</em> (1986-08-01) <a href="https://doi.org/cjx3mk">https://doi.org/cjx3mk</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1214/ss/1177013604">10.1214/ss/1177013604</a></div></div>
</div>
<div id="ref-SVoGevDg" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">39. </div><div class="csl-right-inline"><strong>Representation Learning: A Review and New Perspectives</strong> <div class="csl-block">Y Bengio, A Courville, P Vincent</div> <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em> (2013-08) <a href="https://doi.org/f42hw4">https://doi.org/f42hw4</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1109/tpami.2013.50">10.1109/tpami.2013.50</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/23787338">23787338</a></div></div>
</div>
<div id="ref-9k4OKrXL" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">40. </div><div class="csl-right-inline"><strong>Multitask Learning</strong> <div class="csl-block">Rich Caruana</div> <em>Machine Learning</em> (1997-07) <a href="https://doi.org/d3gsgj">https://doi.org/d3gsgj</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1023/a:1007379606734">10.1023/a:1007379606734</a></div></div>
</div>
<div id="ref-12JtL2o6T" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">41. </div><div class="csl-right-inline"><strong>A Survey on Transfer Learning</strong> <div class="csl-block">Sinno Jialin Pan, Qiang Yang</div> <em>IEEE Transactions on Knowledge and Data Engineering</em> (2010-10) <a href="https://doi.org/bc4vws">https://doi.org/bc4vws</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1109/tkde.2009.191">10.1109/tkde.2009.191</a></div></div>
</div>
<div id="ref-S1Lim9f9" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">42. </div><div class="csl-right-inline"><strong>Generalizing from a Few Examples</strong> <div class="csl-block">Yaqing Wang, Quanming Yao, James T Kwok, Lionel M Ni</div> <em>ACM Computing Surveys</em> (2020-06-12) <a href="https://doi.org/gg37m2">https://doi.org/gg37m2</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1145/3386252">10.1145/3386252</a></div></div>
</div>
<div id="ref-3wZYBtMr" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">43. </div><div class="csl-right-inline"><strong>Matching Networks for One Shot Learning</strong> <div class="csl-block">Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Koray Kavukcuoglu, Daan Wierstra</div> <em>arXiv</em> (2016) <a href="https://doi.org/g96wsd">https://doi.org/g96wsd</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.1606.04080">10.48550/arxiv.1606.04080</a></div></div>
</div>
<div id="ref-1EGW21dqi" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">44. </div><div class="csl-right-inline"><strong>Prototypical Networks for Few-shot Learning</strong> <div class="csl-block">Jake Snell, Kevin Swersky, Richard S Zemel</div> <em>arXiv</em> (2017) <a href="https://doi.org/g96wsf">https://doi.org/g96wsf</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.1703.05175">10.48550/arxiv.1703.05175</a></div></div>
</div>
<div id="ref-On9K3pxW" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">45. </div><div class="csl-right-inline"><strong>A contextual-bandit approach to personalized news article recommendation</strong> <div class="csl-block">Lihong Li, Wei Chu, John Langford, Robert E Schapire</div> <em>Proceedings of the 19th international conference on World wide web</em> (2010-04-26) <a href="https://doi.org/bpkcx9">https://doi.org/bpkcx9</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1145/1772690.1772758">10.1145/1772690.1772758</a></div></div>
</div>
<div id="ref-RauXHY7u" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">46. </div><div class="csl-right-inline"><strong>A New Approach to Linear Filtering and Prediction Problems</strong> <div class="csl-block">RE Kalman</div> <em>Journal of Basic Engineering</em> (1960-03-01) <a href="https://doi.org/dmftj3">https://doi.org/dmftj3</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1115/1.3662552">10.1115/1.3662552</a></div></div>
</div>
<div id="ref-10ULlHdY2" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">47. </div><div class="csl-right-inline"><strong>Online Learning: A Comprehensive Survey</strong> <div class="csl-block">Steven CH Hoi, Doyen Sahoo, Jing Lu, Peilin Zhao</div> <em>arXiv</em> (2018) <a href="https://doi.org/g9643t">https://doi.org/g9643t</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.1802.02871">10.48550/arxiv.1802.02871</a></div></div>
</div>
<div id="ref-5S8Ulwe8" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">48. </div><div class="csl-right-inline"><strong>Online Learning and Online Convex Optimization</strong> <div class="csl-block">Shai Shalev-Shwartz</div> <em>Foundations and Trends® in Machine Learning</em> (2012-03-29) <a href="https://doi.org/gc7rf4">https://doi.org/gc7rf4</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1561/2200000018">10.1561/2200000018</a></div></div>
</div>
<div id="ref-BkxeQdkH" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">49. </div><div class="csl-right-inline"><strong>A survey on concept drift adaptation</strong> <div class="csl-block">João Gama, Indrė Žliobaitė, Albert Bifet, Mykola Pechenizkiy, Abdelhamid Bouchachia</div> <em>ACM Computing Surveys</em> (2014-03) <a href="https://doi.org/gd893p">https://doi.org/gd893p</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1145/2523813">10.1145/2523813</a></div></div>
</div>
<div id="ref-QbZy0vLM" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">50. </div><div class="csl-right-inline"><strong>Learning with Drift Detection</strong> <div class="csl-block">João Gama, Pedro Medas, Gladys Castillo, Pedro Rodrigues</div> <em>Lecture Notes in Computer Science</em> (2004) <a href="https://doi.org/ckzcm4">https://doi.org/ckzcm4</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1007/978-3-540-28645-5_29">10.1007/978-3-540-28645-5_29</a></div></div>
</div>
<div id="ref-jbY4pWUH" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">51. </div><div class="csl-right-inline"><strong>Early Drift Detection Method</strong> <div class="csl-block">Manuel Baena-García, José del Campo-Ávila, Raul Fidalgo, Albert Bifet, Ricard Gavalda, Rafael Morales-Bueno</div> <em>Fourth International Workshop on Knowledge Discovery from Data Streams</em> (2006)</div>
</div>
<div id="ref-yml0Vh7r" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">52. </div><div class="csl-right-inline"><strong>New ensemble methods for evolving data streams</strong> <div class="csl-block">Albert Bifet, Geoff Holmes, Bernhard Pfahringer, Richard Kirkby, Ricard Gavaldà</div> <em>Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining</em> (2009-06-28) <a href="https://doi.org/dkxcrj">https://doi.org/dkxcrj</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1145/1557019.1557041">10.1145/1557019.1557041</a></div></div>
</div>
<div id="ref-18RlJp2YO" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">53. </div><div class="csl-right-inline"><strong>Some aspects of the sequential design of experiments</strong> <div class="csl-block">Herbert Robbins</div> <em>Bulletin of the American Mathematical Society</em> (1952) <a href="https://doi.org/c325nh">https://doi.org/c325nh</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1090/s0002-9904-1952-09620-8">10.1090/s0002-9904-1952-09620-8</a></div></div>
</div>
<div id="ref-jlS72VrS" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">54. </div><div class="csl-right-inline"><strong>Reinforcement Learning: An Introduction</strong> <div class="csl-block">Richard S Sutton, Andrew G Barto</div> <em>MIT Press</em> (1998) <div class="csl-block">ISBN: 978-0-262-19398-6</div></div>
</div>
<div id="ref-17mBgrHWB" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">55. </div><div class="csl-right-inline"><strong>Finite-time Analysis of the Multiarmed Bandit Problem</strong> <div class="csl-block">Peter Auer, Nicolò Cesa-Bianchi, Paul Fischer</div> <em>Machine Learning</em> (2002-05) <a href="https://doi.org/dxkxgv">https://doi.org/dxkxgv</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1023/a:1013689704352">10.1023/a:1013689704352</a></div></div>
</div>
<div id="ref-SldOo5zA" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">56. </div><div class="csl-right-inline"><strong>A Markovian Decision Process</strong> <div class="csl-block">Richard Bellman</div> <em>Journal of Mathematics and Mechanics</em> (1957)</div>
</div>
<div id="ref-jggKMQFt" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">57. </div><div class="csl-right-inline"><strong>Learning from Delayed Rewards</strong> <div class="csl-block">Christopher John Cornish Hellaby Watkins</div> <em>King's College, University of Cambridge</em> (1989)</div>
</div>
<div id="ref-5ivkUecW" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">58. </div><div class="csl-right-inline"><strong>Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning</strong> <div class="csl-block">Ronald J Williams</div> <em>Machine Learning</em> (1992-05) <a href="https://doi.org/b762gd">https://doi.org/b762gd</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1023/a:1022672621406">10.1023/a:1022672621406</a></div></div>
</div>
<div id="ref-YAPw7zCk" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">59. </div><div class="csl-right-inline"><strong>Neuronlike adaptive elements that can solve difficult learning control problems</strong> <div class="csl-block">Andrew G Barto, Richard S Sutton, Charles W Anderson</div> <em>IEEE Transactions on Systems, Man, and Cybernetics</em> (1983-09) <a href="https://doi.org/gddhk6">https://doi.org/gddhk6</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1109/tsmc.1983.6313077">10.1109/tsmc.1983.6313077</a></div></div>
</div>
<div id="ref-BLbPVL8P" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">60. </div><div class="csl-right-inline"><strong>Human-level control through deep reinforcement learning</strong> <div class="csl-block">Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, … Demis Hassabis</div> <em>Nature</em> (2015-02-25) <a href="https://doi.org/gc3h75">https://doi.org/gc3h75</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1038/nature14236">10.1038/nature14236</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/25719670">25719670</a></div></div>
</div>
<div id="ref-13xFsnpVI" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">61. </div><div class="csl-right-inline"><strong>Compression, restoration, resampling, ‘compressive sensing’: fast transforms in digital imaging</strong> <div class="csl-block">LP Yaroslavsky</div> <em>Journal of Optics</em> (2015-07-01) <a href="https://doi.org/g96wr9">https://doi.org/g96wr9</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1088/2040-8978/17/7/073001">10.1088/2040-8978/17/7/073001</a></div></div>
</div>
<div id="ref-1F4l9zw7H" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">62. </div><div class="csl-right-inline"><strong>Speech Analysis and Synthesis by Linear Prediction of the Speech Wave</strong> <div class="csl-block">BS Atal, Suzanne L Hanauer</div> <em>The Journal of the Acoustical Society of America</em> (1971-08-01) <a href="https://doi.org/cc657m">https://doi.org/cc657m</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1121/1.1912679">10.1121/1.1912679</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/4106390">4106390</a></div></div>
</div>
<div id="ref-u6KGkrtr" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">63. </div><div class="csl-right-inline"><strong>A vector space model for automatic indexing</strong> <div class="csl-block">G Salton, A Wong, CS Yang</div> <em>Communications of the ACM</em> (1975-11) <a href="https://doi.org/fw8vv8">https://doi.org/fw8vv8</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1145/361219.361220">10.1145/361219.361220</a></div></div>
</div>
<div id="ref-1BlaGxmbh" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">64. </div><div class="csl-right-inline"><strong>Contextures: Representations from Contexts</strong> <div class="csl-block">Runtian Zhai, Kai Yang, Che-Ping Tsai, Burak Varici, Zico Kolter, Pradeep Ravikumar</div> <em>arXiv</em> (2025) <a href="https://doi.org/g977nj">https://doi.org/g977nj</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2505.01557">10.48550/arxiv.2505.01557</a></div></div>
</div>
<div id="ref-NLVTJ9Lj" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">65. </div><div class="csl-right-inline"><strong>Auto-Encoding Variational Bayes</strong> <div class="csl-block">Diederik P Kingma, Max Welling</div> <em>arXiv</em> (2022-12-13) <a href="https://arxiv.org/abs/1312.6114">https://arxiv.org/abs/1312.6114</a></div>
</div>
<div id="ref-JE5FRU4v" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">66. </div><div class="csl-right-inline"><strong>Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks</strong> <div class="csl-block">Chelsea Finn, Pieter Abbeel, Sergey Levine</div> <em>arXiv</em> (2017) <a href="https://doi.org/g5v2js">https://doi.org/g5v2js</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.1703.03400">10.48550/arxiv.1703.03400</a></div></div>
</div>
<div id="ref-urJgpE6q" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">67. </div><div class="csl-right-inline"><strong>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</strong> <div class="csl-block">Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova</div> <em>arXiv</em> (2018) <a href="https://doi.org/hm65">https://doi.org/hm65</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.1810.04805">10.48550/arxiv.1810.04805</a></div></div>
</div>
<div id="ref-17tnf46zM" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">68. </div><div class="csl-right-inline"><strong>Learning Transferable Visual Models From Natural Language Supervision</strong> <div class="csl-block">Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, … Ilya Sutskever</div> <em>arXiv</em> (2021) <a href="https://doi.org/hs7z">https://doi.org/hs7z</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2103.00020">10.48550/arxiv.2103.00020</a></div></div>
</div>
<div id="ref-1GbAsSOZV" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">69. </div><div class="csl-right-inline"><strong>On the Opportunities and Risks of Foundation Models</strong> <div class="csl-block">Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, … Percy Liang</div> <em>arXiv</em> (2021) <a href="https://doi.org/hw3v">https://doi.org/hw3v</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2108.07258">10.48550/arxiv.2108.07258</a></div></div>
</div>
<div id="ref-1EkVeYD9V" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">70. </div><div class="csl-right-inline"><strong>A Survey on In-context Learning</strong> <div class="csl-block">Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Jingyuan Ma, Rui Li, Heming Xia, Jingjing Xu, Zhiyong Wu, Tianyu Liu, … Zhifang Sui</div> <em>arXiv</em> (2023) <a href="https://doi.org/gsv9x4">https://doi.org/gsv9x4</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2301.00234">10.48550/arxiv.2301.00234</a></div></div>
</div>
<div id="ref-ljL7YbZD" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">71. </div><div class="csl-right-inline"><strong>Adaptive Mixtures of Local Experts</strong> <div class="csl-block">Robert A Jacobs, Michael I Jordan, Steven J Nowlan, Geoffrey E Hinton</div> <em>Neural Computation</em> (1991-02) <a href="https://doi.org/cnsnqg">https://doi.org/cnsnqg</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1162/neco.1991.3.1.79">10.1162/neco.1991.3.1.79</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/31141872">31141872</a></div></div>
</div>
<div id="ref-PKjSQOD" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">72. </div><div class="csl-right-inline"><strong>WILDS: A Benchmark of in-the-Wild Distribution Shifts</strong> <div class="csl-block">Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, … Percy Liang</div> <em>arXiv</em> (2020) <a href="https://doi.org/g93rnp">https://doi.org/g93rnp</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2012.07421">10.48550/arxiv.2012.07421</a></div></div>
</div>
<div id="ref-Fto8UbOH" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">73. </div><div class="csl-right-inline"><strong>Continuous Temporal Domain Generalization</strong> <div class="csl-block">Zekun Cai, Guangji Bai, Renhe Jiang, Xuan Song, Liang Zhao</div> <em>arXiv</em> (2024) <a href="https://doi.org/g9582d">https://doi.org/g9582d</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2405.16075">10.48550/arxiv.2405.16075</a></div></div>
</div>
<div id="ref-2Asz98yx" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">74. </div><div class="csl-right-inline"><strong>LFME: A Simple Framework for Learning from Multiple Experts in Domain Generalization</strong> <div class="csl-block">Liang Chen, Yong Zhang, Yibing Song, Zhiqiang Shen, Lingqiao Liu</div> <em>arXiv</em> (2024) <a href="https://doi.org/g9582n">https://doi.org/g9582n</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2410.17020">10.48550/arxiv.2410.17020</a></div></div>
</div>
<div id="ref-18QzJqDj4" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">75. </div><div class="csl-right-inline"><strong>Scalable Multi-Domain Adaptation of Language Models using Modular Experts</strong> <div class="csl-block">Peter Schafhalter, Shun Liao, Yanqi Zhou, Chih-Kuan Yeh, Arun Kandoor, James Laudon</div> <em>arXiv</em> (2024) <a href="https://doi.org/g9582k">https://doi.org/g9582k</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2410.10181">10.48550/arxiv.2410.10181</a></div></div>
</div>
<div id="ref-UxVULYQ3" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">76. </div><div class="csl-right-inline"><strong>Towards Modular LLMs by Building and Reusing a Library of LoRAs</strong> <div class="csl-block">Oleksiy Ostapenko, Zhan Su, Edoardo Maria Ponti, Laurent Charlin, Nicolas Le Roux, Matheus Pereira, Lucas Caccia, Alessandro Sordoni</div> <em>arXiv</em> (2024) <a href="https://doi.org/g9582c">https://doi.org/g9582c</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2405.11157">10.48550/arxiv.2405.11157</a></div></div>
</div>
<div id="ref-UskQdlu3" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">77. </div><div class="csl-right-inline"><strong>Mixture of LoRA Experts</strong> <div class="csl-block">Xun Wu, Shaohan Huang, Furu Wei</div> <em>arXiv</em> (2024) <a href="https://doi.org/g93rnr">https://doi.org/g93rnr</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2404.13628">10.48550/arxiv.2404.13628</a></div></div>
</div>
<div id="ref-TBU9RF9F" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">78. </div><div class="csl-right-inline"><strong>Optimal pointwise adaptive methods in nonparametric estimation</strong> <div class="csl-block">OV Lepski, VG Spokoiny</div> <em>The Annals of Statistics</em> (1997-12-01) <a href="https://doi.org/fwzw52">https://doi.org/fwzw52</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1214/aos/1030741083">10.1214/aos/1030741083</a></div></div>
</div>
<div id="ref-dSHoyg5K" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">79. </div><div class="csl-right-inline"><strong>The Weighted Majority Algorithm</strong> <div class="csl-block">N Littlestone, MK Warmuth</div> <em>Information and Computation</em> (1994-02) <a href="https://doi.org/c8hw9h">https://doi.org/c8hw9h</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1006/inco.1994.1009">10.1006/inco.1994.1009</a></div></div>
</div>
<div id="ref-FpwH3vaM" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">80. </div><div class="csl-right-inline"><strong>Selective Test-Time Adaptation for Unsupervised Anomaly Detection using Neural Implicit Representations</strong> <div class="csl-block">Sameer Ambekar, Julia A Schnabel, Cosmin I Bercea</div> <em>arXiv</em> (2024) <a href="https://doi.org/g9582h">https://doi.org/g9582h</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2410.03306">10.48550/arxiv.2410.03306</a></div></div>
</div>
<div id="ref-D2dv1dGG" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">81. </div><div class="csl-right-inline"><strong>Test-Time Adaptation Induces Stronger Accuracy and Agreement-on-the-Line</strong> <div class="csl-block">Eungyeup Kim, Mingjie Sun, Christina Baek, Aditi Raghunathan, JZico Kolter</div> <em>arXiv</em> (2023) <a href="https://doi.org/g958z7">https://doi.org/g958z7</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2310.04941">10.48550/arxiv.2310.04941</a></div></div>
</div>
<div id="ref-1CQ8u4q3f" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">82. </div><div class="csl-right-inline"><strong>Harder Tasks Need More Experts: Dynamic Routing in MoE Models</strong> <div class="csl-block">Quzhe Huang, Zhenwei An, Nan Zhuang, Mingxu Tao, Chen Zhang, Yang Jin, Kun Xu, Kun Xu, Liwei Chen, Songfang Huang, Yansong Feng</div> <em>arXiv</em> (2024) <a href="https://doi.org/g9582b">https://doi.org/g9582b</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2403.07652">10.48550/arxiv.2403.07652</a></div></div>
</div>
<div id="ref-TQNdTc18" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">83. </div><div class="csl-right-inline"><strong>Unsupervised Learning via Meta-Learning</strong> <div class="csl-block">Kyle Hsu, Sergey Levine, Chelsea Finn</div> <em>arXiv</em> (2018) <a href="https://doi.org/g958zs">https://doi.org/g958zs</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.1810.02334">10.48550/arxiv.1810.02334</a></div></div>
</div>
<div id="ref-18PGQ3fOX" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">84. </div><div class="csl-right-inline"><strong>Bayesian scaling laws for in-context learning</strong> <div class="csl-block">Aryaman Arora, Dan Jurafsky, Christopher Potts, Noah D Goodman</div> <em>arXiv</em> (2024) <a href="https://doi.org/g9582m">https://doi.org/g9582m</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2410.16531">10.48550/arxiv.2410.16531</a></div></div>
</div>
<div id="ref-bWiWgPrK" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">85. </div><div class="csl-right-inline"><strong>An overview of statistical learning theory</strong> <div class="csl-block">VN Vapnik</div> <em>IEEE Transactions on Neural Networks</em> (1999) <a href="https://doi.org/fdvhmd">https://doi.org/fdvhmd</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1109/72.788640">10.1109/72.788640</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/18252602">18252602</a></div></div>
</div>
<div id="ref-BLSm3phD" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">86. </div><div class="csl-right-inline"><strong>A Closer Look into Mixture-of-Experts in Large Language Models</strong> <div class="csl-block">Ka Man Lo, Zeyu Huang, Zihan Qiu, Zili Wang, Jie Fu</div> <em>arXiv</em> (2024) <a href="https://doi.org/g9582f">https://doi.org/g9582f</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2406.18219">10.48550/arxiv.2406.18219</a></div></div>
</div>
<div id="ref-4fnOdPts" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">87. </div><div class="csl-right-inline"><strong>Shortcut Learning in Deep Neural Networks</strong> <div class="csl-block">Robert Geirhos, Jörn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias Bethge, Felix A Wichmann</div> <em>arXiv</em> (2020) <a href="https://doi.org/g93rnn">https://doi.org/g93rnn</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2004.07780">10.48550/arxiv.2004.07780</a></div></div>
</div>
<div id="ref-11l8svMmM" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">88. </div><div class="csl-right-inline"><strong>Distributionally Robust Neural Networks for Group Shifts: On the Importance of Regularization for Worst-Case Generalization</strong> <div class="csl-block">Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, Percy Liang</div> <em>arXiv</em> (2019) <a href="https://doi.org/g93rnm">https://doi.org/g93rnm</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.1911.08731">10.48550/arxiv.1911.08731</a></div></div>
</div>
<div id="ref-MGkiKe9y" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">89. </div><div class="csl-right-inline"><strong>The Selective Labels Problem</strong> <div class="csl-block">Himabindu Lakkaraju, Jon Kleinberg, Jure Leskovec, Jens Ludwig, Sendhil Mullainathan</div> <em>Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em> (2017-08-04) <a href="https://doi.org/ggd7hz">https://doi.org/ggd7hz</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1145/3097983.3098066">10.1145/3097983.3098066</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/29780658">29780658</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5958915">PMC5958915</a></div></div>
</div>
<div id="ref-kfKaakAe" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">90. </div><div class="csl-right-inline"><strong>Model Selection and Estimation in Regression with Grouped Variables</strong> <div class="csl-block">Ming Yuan, Yi Lin</div> <em>Journal of the Royal Statistical Society Series B: Statistical Methodology</em> (2005-12-21) <a href="https://doi.org/fvntrn">https://doi.org/fvntrn</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1111/j.1467-9868.2005.00532.x">10.1111/j.1467-9868.2005.00532.x</a></div></div>
</div>
<div id="ref-kX2zf6UE" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">91. </div><div class="csl-right-inline"><strong>Regression Shrinkage and Selection Via the Lasso</strong> <div class="csl-block">Robert Tibshirani</div> <em>Journal of the Royal Statistical Society Series B: Statistical Methodology</em> (1996-01-01) <a href="https://doi.org/gfn45m">https://doi.org/gfn45m</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1111/j.2517-6161.1996.tb02080.x">10.1111/j.2517-6161.1996.tb02080.x</a></div></div>
</div>
<div id="ref-PWhr4ijC" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">92. </div><div class="csl-right-inline"><strong>Data Analysis Using Regression and Multilevel/Hierarchical Models</strong> <div class="csl-block">Andrew Gelman, Jennifer Hill</div> <em>Cambridge University Press</em> (2006-12-18) <a href="https://doi.org/dbrqk6">https://doi.org/dbrqk6</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1017/cbo9780511790942">10.1017/cbo9780511790942</a></div></div>
</div>
<div id="ref-45Kr1uvy" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">93. </div><div class="csl-right-inline"><strong>Optimization Methods for Large-Scale Machine Learning</strong> <div class="csl-block">Léon Bottou, Frank E Curtis, Jorge Nocedal</div> <em>arXiv</em> (2016) <a href="https://doi.org/g958zf">https://doi.org/g958zf</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.1606.04838">10.48550/arxiv.1606.04838</a></div></div>
</div>
<div id="ref-18KQ6Vlb2" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">94. </div><div class="csl-right-inline"><strong>Contextual Bandits with Cross-learning</strong> <div class="csl-block">Santiago Balseiro, Negin Golrezaei, Mohammad Mahdian, Vahab Mirrokni, Jon Schneider</div> <em>arXiv</em> (2018) <a href="https://doi.org/g958zr">https://doi.org/g958zr</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.1809.09582">10.48550/arxiv.1809.09582</a></div></div>
</div>
<div id="ref-W3XPrQXH" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">95. </div><div class="csl-right-inline"><strong>Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers</strong> <div class="csl-block">Stephen Boyd</div> <em>Foundations and Trends® in Machine Learning</em> (2010) <a href="https://doi.org/d3kztk">https://doi.org/d3kztk</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1561/2200000016">10.1561/2200000016</a></div></div>
</div>
<div id="ref-1G9auqG3f" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">96. </div><div class="csl-right-inline"><strong>Adam: A Method for Stochastic Optimization</strong> <div class="csl-block">Diederik P Kingma, Jimmy Ba</div> <em>arXiv</em> (2014) <a href="https://doi.org/hnkr">https://doi.org/hnkr</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.1412.6980">10.48550/arxiv.1412.6980</a></div></div>
</div>
<div id="ref-rtNEROOT" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">97. </div><div class="csl-right-inline"><strong>Language Models are Few-Shot Learners</strong> <div class="csl-block">Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, … Dario Amodei</div> <em>arXiv</em> (2020) <a href="https://doi.org/gpmv43">https://doi.org/gpmv43</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2005.14165">10.48550/arxiv.2005.14165</a></div></div>
</div>
<div id="ref-6Y4uv63y" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">98. </div><div class="csl-right-inline"><strong>Emergent Abilities of Large Language Models</strong> <div class="csl-block">Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, … William Fedus</div> <em>arXiv</em> (2022) <a href="https://doi.org/jpr3">https://doi.org/jpr3</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2206.07682">10.48550/arxiv.2206.07682</a></div></div>
</div>
<div id="ref-61xQEdRS" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">99. </div><div class="csl-right-inline"><strong>Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?</strong> <div class="csl-block">Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, Luke Zettlemoyer</div> <em>arXiv</em> (2022) <a href="https://doi.org/gtkkf4">https://doi.org/gtkkf4</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2202.12837">10.48550/arxiv.2202.12837</a></div></div>
</div>
<div id="ref-6W1y3ZrT" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">100. </div><div class="csl-right-inline"><strong>Scaling Laws for Neural Language Models</strong> <div class="csl-block">Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, Dario Amodei</div> <em>arXiv</em> (2020) <a href="https://doi.org/gtb96w">https://doi.org/gtb96w</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2001.08361">10.48550/arxiv.2001.08361</a></div></div>
</div>
<div id="ref-5q4aFZMi" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">101. </div><div class="csl-right-inline"><strong>Training Compute-Optimal Large Language Models</strong> <div class="csl-block">Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, … Laurent Sifre</div> <em>arXiv</em> (2022) <a href="https://doi.org/gthszs">https://doi.org/gthszs</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2203.15556">10.48550/arxiv.2203.15556</a></div></div>
</div>
<div id="ref-hRp04fhf" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">102. </div><div class="csl-right-inline"><strong>Publication Trends on the Varying Coefficients Model: Estimating the Actual (Under)Utilization of a Highly Acclaimed Method for Studying Statistical Interactions</strong> <div class="csl-block">Assaf Botzer</div> <em>Publications</em> (2025-04-07) <a href="https://doi.org/g9t2rq">https://doi.org/g9t2rq</a> <div class="csl-block">DOI: <a href="https://doi.org/10.3390/publications13020019">10.3390/publications13020019</a></div></div>
</div>
<div id="ref-1AAscRzye" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">103. </div><div class="csl-right-inline"><strong>Covariance Selection</strong> <div class="csl-block">AP Dempster</div> <em>Biometrics</em> (1972-03) <a href="https://doi.org/d5t49s">https://doi.org/d5t49s</a> <div class="csl-block">DOI: <a href="https://doi.org/10.2307/2528966">10.2307/2528966</a></div></div>
</div>
<div id="ref-moNSKvBk" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">104. </div><div class="csl-right-inline"><strong>Graphical Models</strong> <div class="csl-block">Steffen L Lauritzen</div> <em>Oxford University PressOxford</em> (1996-05-02) <a href="https://doi.org/g958zb">https://doi.org/g958zb</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1093/oso/9780198522195.001.0001">10.1093/oso/9780198522195.001.0001</a></div></div>
</div>
<div id="ref-eFgbj4Kw" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">105. </div><div class="csl-right-inline"><strong>High-dimensional graphs and variable selection with the Lasso</strong> <div class="csl-block">Nicolai Meinshausen, Peter Bühlmann</div> <em>The Annals of Statistics</em> (2006-06-01) <a href="https://doi.org/fwt5kt">https://doi.org/fwt5kt</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1214/009053606000000281">10.1214/009053606000000281</a></div></div>
</div>
<div id="ref-m4KsbXUW" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">106. </div><div class="csl-right-inline"><strong>Sparse inverse covariance estimation with the graphical lasso</strong> <div class="csl-block">Jerome Friedman, Trevor Hastie, Robert Tibshirani</div> <em>Biostatistics</em> (2007-12-12) <a href="https://doi.org/db7svr">https://doi.org/db7svr</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1093/biostatistics/kxm045">10.1093/biostatistics/kxm045</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/18079126">18079126</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3019769">PMC3019769</a></div></div>
</div>
<div id="ref-hxZIBmjM" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">107. </div><div class="csl-right-inline"><strong>Joint estimation of multiple graphical models</strong> <div class="csl-block">J Guo, E Levina, G Michailidis, J Zhu</div> <em>Biometrika</em> (2011-02-09) <a href="https://doi.org/fqvbh2">https://doi.org/fqvbh2</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1093/biomet/asq060">10.1093/biomet/asq060</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/23049124">23049124</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3412604">PMC3412604</a></div></div>
</div>
<div id="ref-JDoK9thg" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">108. </div><div class="csl-right-inline"><strong>The Joint Graphical Lasso for Inverse Covariance Estimation Across Multiple Classes</strong> <div class="csl-block">Patrick Danaher, Pei Wang, Daniela M Witten</div> <em>Journal of the Royal Statistical Society Series B: Statistical Methodology</em> (2013-08-12) <a href="https://doi.org/f5sj9g">https://doi.org/f5sj9g</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1111/rssb.12033">10.1111/rssb.12033</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/24817823">24817823</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4012833">PMC4012833</a></div></div>
</div>
<div id="ref-3EfUtiJg" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">109. </div><div class="csl-right-inline"><strong>Fast spatio-temporally varying coefficient modeling with reluctant interaction selection</strong> <div class="csl-block">Daisuke Murakami, Shinichiro Shirota, Seiji Kajita, Mami Kajita</div> <em>arXiv</em> (2024) <a href="https://doi.org/g9582j">https://doi.org/g9582j</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2410.07229">10.48550/arxiv.2410.07229</a></div></div>
</div>
<div id="ref-LUKqKQYa" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">110. </div><div class="csl-right-inline"><strong>Spatially Varying Coefficient Models for Estimating Heterogeneous Mixture Effects</strong> <div class="csl-block">Jacob Englert, Howard Chang</div> <em>arXiv</em> (2025) <a href="https://doi.org/g9582q">https://doi.org/g9582q</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2502.14651">10.48550/arxiv.2502.14651</a></div></div>
</div>
<div id="ref-10QS2bf0y" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">111. </div><div class="csl-right-inline"><strong>Network Varying Coefficient Model</strong> <div class="csl-block">Xinyan Fan, Kuangnan Fang, Wei Lan, Chih-Ling Tsai</div> <em>Journal of the American Statistical Association</em> (2025-04-11) <a href="https://doi.org/g9t2rm">https://doi.org/g9t2rm</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1080/01621459.2025.2470481">10.1080/01621459.2025.2470481</a></div></div>
</div>
<div id="ref-721WoKJr" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">112. </div><div class="csl-right-inline"><strong>Bayesian Inference for General Gaussian Graphical Models With Application to Multivariate Lattice Data</strong> <div class="csl-block">Adrian Dobra, Alex Lenkoski, Abel Rodriguez</div> <em>Journal of the American Statistical Association</em> (2011-12) <a href="https://doi.org/fxq5wh">https://doi.org/fxq5wh</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1198/jasa.2011.tm10465">10.1198/jasa.2011.tm10465</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/26924867">26924867</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4767185">PMC4767185</a></div></div>
</div>
<div id="ref-1Da8QJneg" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">113. </div><div class="csl-right-inline"><strong>Bayesian Inference of Multiple Gaussian Graphical Models</strong> <div class="csl-block">Christine Peterson, Francesco C Stingo, Marina Vannucci</div> <em>Journal of the American Statistical Association</em> (2015-01-02) <a href="https://doi.org/f69dnj">https://doi.org/f69dnj</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1080/01621459.2014.896806">10.1080/01621459.2014.896806</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/26078481">26078481</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4465207">PMC4465207</a></div></div>
</div>
<div id="ref-x5cbX2Cv" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">114. </div><div class="csl-right-inline"><strong>Bayesian covariate-dependent graph learning with a dual group spike-and-slab prior</strong> <div class="csl-block">Zijian Zeng, Meng Li, Marina Vannucci</div> <em>Biometrics</em> (2025-04-02) <a href="https://doi.org/g95bkg">https://doi.org/g95bkg</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1093/biomtc/ujaf053">10.1093/biomtc/ujaf053</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/40322851">40322851</a></div></div>
</div>
<div id="ref-LJ5esEGZ" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">115. </div><div class="csl-right-inline"><strong>Tree Boosted Varying Coefficient Models</strong> <div class="csl-block">Yichen Zhou, Giles Hooker</div> <em>arXiv</em> (2019) <a href="https://doi.org/g958zt">https://doi.org/g958zt</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.1904.01058">10.48550/arxiv.1904.01058</a></div></div>
</div>
<div id="ref-121VRWHxY" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">116. </div><div class="csl-right-inline"><strong>A tree-based varying coefficient model</strong> <div class="csl-block">Henning Zakrisson, Mathias Lindholm</div> <em>arXiv</em> (2024) <a href="https://doi.org/g958z8">https://doi.org/g958z8</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2401.05982">10.48550/arxiv.2401.05982</a></div></div>
</div>
<div id="ref-pxvmdyAr" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">117. </div><div class="csl-right-inline"><strong>VCBART: Bayesian Trees for Varying Coefficients</strong> <div class="csl-block">Sameer K Deshpande, Ray Bai, Cecilia Balocchi, Jennifer E Starling, Jordan Weiss</div> <em>Bayesian Analysis</em> (2024-01-01) <a href="https://doi.org/g977nk">https://doi.org/g977nk</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1214/24-ba1470">10.1214/24-ba1470</a></div></div>
</div>
<div id="ref-Z6951tJe" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">118. </div><div class="csl-right-inline"><strong>Penalized Spline Estimation for Varying-Coefficient Models</strong> <div class="csl-block">Yiqiang Lu, Riquan Zhang, Liping Zhu</div> <em>Communications in Statistics - Theory and Methods</em> (2008-05-27) <a href="https://doi.org/fpj5gm">https://doi.org/fpj5gm</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1080/03610920801931887">10.1080/03610920801931887</a></div></div>
</div>
<div id="ref-Q3DSEqgH" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">119. </div><div class="csl-right-inline"><strong>Deep Multimodal Learning with Missing Modality: A Survey</strong> <div class="csl-block">Renjie Wu, Hu Wang, Hsiang-Ting Chen, Gustavo Carneiro</div> <em>arXiv</em> (2024) <a href="https://doi.org/g9582g">https://doi.org/g9582g</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2409.07825">10.48550/arxiv.2409.07825</a></div></div>
</div>
<div id="ref-lLSU1HNL" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">120. </div><div class="csl-right-inline"><strong>Domain Adaptation under Missingness Shift</strong> <div class="csl-block">Helen Zhou, Sivaraman Balakrishnan, Zachary C Lipton</div> <em>arXiv</em> (2022) <a href="https://doi.org/g958z3">https://doi.org/g958z3</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2211.02093">10.48550/arxiv.2211.02093</a></div></div>
</div>
<div id="ref-p6OGob17" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">121. </div><div class="csl-right-inline"><strong>Variational Autoencoder with Arbitrary Conditioning</strong> <div class="csl-block">Oleg Ivanov, Michael Figurnov, Dmitry Vetrov</div> <em>arXiv</em> (2018) <a href="https://doi.org/g958zp">https://doi.org/g958zp</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.1806.02382">10.48550/arxiv.1806.02382</a></div></div>
</div>
<div id="ref-twzF9qcj" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">122. </div><div class="csl-right-inline"><strong>GAIN: Missing Data Imputation using Generative Adversarial Nets</strong> <div class="csl-block">Jinsung Yoon, James Jordon, Mihaela van der Schaar</div> <em>arXiv</em> (2018) <a href="https://doi.org/g958zq">https://doi.org/g958zq</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.1806.02920">10.48550/arxiv.1806.02920</a></div></div>
</div>
<div id="ref-DfpD095S" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">123. </div><div class="csl-right-inline"><strong>A class of pattern-mixture models for normal incomplete data</strong> <div class="csl-block">RODERICK JA LITTLE</div> <em>Biometrika</em> (1994) <a href="https://doi.org/bqw3x9">https://doi.org/bqw3x9</a> <div class="csl-block">DOI: <a href="https://doi.org/10.2307/2337120">10.2307/2337120</a></div></div>
</div>
<div id="ref-11UHeGMyc" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">124. </div><div class="csl-right-inline"><strong>Multiple imputation of incomplete multilevel data using Heckman selection models</strong> <div class="csl-block">Johanna Muñoz, Matthias Egger, Orestis Efthimiou, Vincent Audigier, Valentijn MT de Jong, ThomasPA Debray</div> <em>arXiv</em> (2023) <a href="https://doi.org/g958z5">https://doi.org/g958z5</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2301.05043">10.48550/arxiv.2301.05043</a></div></div>
</div>
<div id="ref-pTzEjoO6" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">125. </div><div class="csl-right-inline"><strong>XGBoost: A Scalable Tree Boosting System</strong> <div class="csl-block">Tianqi Chen, Carlos Guestrin</div> <em>arXiv</em> (2016) <a href="https://doi.org/g958zc">https://doi.org/g958zc</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.1603.02754">10.48550/arxiv.1603.02754</a></div></div>
</div>
<div id="ref-T8dO7ymx" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">126. </div><div class="csl-right-inline"><strong>The Missing Indicator Method: From Low to High Dimensions</strong> <div class="csl-block">Mike Van Ness, Tomas M Bosschieter, Roberto Halpin-Gregorio, Madeleine Udell</div> <em>arXiv</em> (2022) <a href="https://doi.org/g958z4">https://doi.org/g958z4</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2211.09259">10.48550/arxiv.2211.09259</a></div></div>
</div>
<div id="ref-HZddfIob" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">127. </div><div class="csl-right-inline"><strong>Recurrent Neural Networks for Multivariate Time Series with Missing Values</strong> <div class="csl-block">Zhengping Che, Sanjay Purushotham, Kyunghyun Cho, David Sontag, Yan Liu</div> <em>arXiv</em> (2016) <a href="https://doi.org/g958zd">https://doi.org/g958zd</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.1606.01865">10.48550/arxiv.1606.01865</a></div></div>
</div>
<div id="ref-jPxFquZY" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">128. </div><div class="csl-right-inline"><strong>BRITS: Bidirectional Recurrent Imputation for Time Series</strong> <div class="csl-block">Wei Cao, Dong Wang, Jian Li, Hao Zhou, Lei Li, Yitan Li</div> <em>arXiv</em> (2018) <a href="https://doi.org/g958zn">https://doi.org/g958zn</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.1805.10572">10.48550/arxiv.1805.10572</a></div></div>
</div>
<div id="ref-8w9fI63O" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">129. </div><div class="csl-right-inline"><strong>XGBoost</strong> <div class="csl-block">Tianqi Chen, Carlos Guestrin</div> <em>Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em> (2016-08-13) <a href="https://doi.org/gdp84q">https://doi.org/gdp84q</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1145/2939672.2939785">10.1145/2939672.2939785</a></div></div>
</div>
<div id="ref-qfuiP7Hi" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">130. </div><div class="csl-right-inline"><strong>An Overview of Multi-Task Learning in Deep Neural Networks</strong> <div class="csl-block">Sebastian Ruder</div> <em>arXiv</em> (2017) <a href="https://doi.org/g958zh">https://doi.org/g958zh</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.1706.05098">10.48550/arxiv.1706.05098</a></div></div>
</div>
<div id="ref-rh7nCPVE" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">131. </div><div class="csl-right-inline"><strong>Attention Is All You Need</strong> <div class="csl-block">Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, Illia Polosukhin</div> <em>arXiv</em> (2017) <a href="https://doi.org/gpnmtv">https://doi.org/gpnmtv</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.1706.03762">10.48550/arxiv.1706.03762</a></div></div>
</div>
<div id="ref-EOUjThUk" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">132. </div><div class="csl-right-inline"><strong>Auto-Encoding Variational Bayes</strong> <div class="csl-block">Diederik P Kingma, Max Welling</div> <em>arXiv</em> (2013) <a href="https://doi.org/gpp5xv">https://doi.org/gpp5xv</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.1312.6114">10.48550/arxiv.1312.6114</a></div></div>
</div>
<div id="ref-GYqEQJ4V" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">133. </div><div class="csl-right-inline"><strong>Meta-Learning in Neural Networks: A Survey</strong> <div class="csl-block">Timothy Hospedales, Antreas Antoniou, Paul Micaelli, Amos Storkey</div> <em>arXiv</em> (2020) <a href="https://doi.org/g958zx">https://doi.org/g958zx</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2004.05439">10.48550/arxiv.2004.05439</a></div></div>
</div>
<div id="ref-qvjULhAd" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">134. </div><div class="csl-right-inline"><strong>MetaICL: Learning to Learn In Context</strong> <div class="csl-block">Sewon Min, Mike Lewis, Luke Zettlemoyer, Hannaneh Hajishirzi</div> <em>arXiv</em> (2021) <a href="https://doi.org/g96dmj">https://doi.org/g96dmj</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2110.15943">10.48550/arxiv.2110.15943</a></div></div>
</div>
<div id="ref-16Xv40Ngd" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">135. </div><div class="csl-right-inline"><strong>Why Can GPT Learn In-Context? Language Models Implicitly Perform Gradient Descent as Meta-Optimizers</strong> <div class="csl-block">Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Shuming Ma, Zhifang Sui, Furu Wei</div> <em>arXiv</em> (2022) <a href="https://doi.org/gtkkf9">https://doi.org/gtkkf9</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2212.10559">10.48550/arxiv.2212.10559</a></div></div>
</div>
<div id="ref-gy3ao6P6" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">136. </div><div class="csl-right-inline"><strong>Transformers as Support Vector Machines</strong> <div class="csl-block">Davoud Ataee Tarzanagh, Yingcong Li, Christos Thrampoulidis, Samet Oymak</div> <em>arXiv</em> (2023) <a href="https://doi.org/g958z6">https://doi.org/g958z6</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2308.16898">10.48550/arxiv.2308.16898</a></div></div>
</div>
<div id="ref-LybHVzmd" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">137. </div><div class="csl-right-inline"><strong>An Explanation of In-context Learning as Implicit Bayesian Inference</strong> <div class="csl-block">Sang Michael Xie, Aditi Raghunathan, Percy Liang, Tengyu Ma</div> <em>arXiv</em> (2021) <a href="https://doi.org/gtkkfs">https://doi.org/gtkkfs</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2111.02080">10.48550/arxiv.2111.02080</a></div></div>
</div>
<div id="ref-Hhj8Woby" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">138. </div><div class="csl-right-inline"><strong>In-Context Learning Strategies Emerge Rationally</strong> <div class="csl-block">Daniel Wurgaft, Ekdeep Singh Lubana, Core Francisco Park, Hidenori Tanaka, Gautam Reddy, Noah D Goodman</div> <em>arXiv</em> (2025) <a href="https://doi.org/g96dmp">https://doi.org/g96dmp</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2506.17859">10.48550/arxiv.2506.17859</a></div></div>
</div>
<div id="ref-m1iAIGu4" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">139. </div><div class="csl-right-inline"><strong>In-context Learning and Induction Heads</strong> <div class="csl-block">Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, … Chris Olah</div> <em>arXiv</em> (2022) <a href="https://doi.org/g95xv8">https://doi.org/g95xv8</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2209.11895">10.48550/arxiv.2209.11895</a></div></div>
</div>
<div id="ref-IncFoA7e" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">140. </div><div class="csl-right-inline"><strong>Learning without training: The implicit dynamics of in-context learning</strong> <div class="csl-block">Benoit Dherin, Michael Munn, Hanna Mazzawi, Michael Wunder, Javier Gonzalvo</div> <em>arXiv</em> (2025) <a href="https://doi.org/g96dmq">https://doi.org/g96dmq</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2507.16003">10.48550/arxiv.2507.16003</a></div></div>
</div>
<div id="ref-16QjxmbmC" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">141. </div><div class="csl-right-inline"><strong>What learning algorithm is in-context learning? Investigations with linear models</strong> <div class="csl-block">Ekin Akyürek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, Denny Zhou</div> <em>arXiv</em> (2022) <a href="https://doi.org/grq582">https://doi.org/grq582</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2211.15661">10.48550/arxiv.2211.15661</a></div></div>
</div>
<div id="ref-V6cbeqie" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">142. </div><div class="csl-right-inline"><strong>Transformers learn in-context by gradient descent</strong> <div class="csl-block">Johannes von Oswald, Eyvind Niklasson, Ettore Randazzo, João Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, Max Vladymyrov</div> <em>arXiv</em> (2022) <a href="https://doi.org/gshbsq">https://doi.org/gshbsq</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2212.07677">10.48550/arxiv.2212.07677</a></div></div>
</div>
<div id="ref-R7y5TKp9" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">143. </div><div class="csl-right-inline"><strong>What Can Transformers Learn In-Context? A Case Study of Simple Function Classes</strong> <div class="csl-block">Shivam Garg, Dimitris Tsipras, Percy Liang, Gregory Valiant</div> <em>arXiv</em> (2022) <a href="https://doi.org/g9t22c">https://doi.org/g9t22c</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2208.01066">10.48550/arxiv.2208.01066</a></div></div>
</div>
<div id="ref-1BOhEiiMt" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">144. </div><div class="csl-right-inline"><strong>Can Transformers Learn Full Bayesian Inference in Context?</strong> <div class="csl-block">Arik Reuter, Tim GJ Rudner, Vincent Fortuin, David Rügamer</div> <em>arXiv</em> (2025) <a href="https://doi.org/g9582p">https://doi.org/g9582p</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2501.16825">10.48550/arxiv.2501.16825</a></div></div>
</div>
<div id="ref-1DBdzlHLI" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">145. </div><div class="csl-right-inline"><strong>TabICL: A Tabular Foundation Model for In-Context Learning on Large Data</strong> <div class="csl-block">Jingang Qu, David Holzmüller, Gaël Varoquaux, Marine Le Morvan</div> <em>arXiv</em> (2025) <a href="https://doi.org/g96dmn">https://doi.org/g96dmn</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2502.05564">10.48550/arxiv.2502.05564</a></div></div>
</div>
<div id="ref-1FWgzslSV" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">146. </div><div class="csl-right-inline"><strong>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</strong> <div class="csl-block">Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou</div> <em>arXiv</em> (2022) <a href="https://doi.org/gr263w">https://doi.org/gr263w</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2201.11903">10.48550/arxiv.2201.11903</a></div></div>
</div>
<div id="ref-OsqhaAcF" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">147. </div><div class="csl-right-inline"><strong>Explainable AI: A Review of Machine Learning Interpretability Methods</strong> <div class="csl-block">Pantelis Linardatos, Vasilis Papastefanopoulos, Sotiris Kotsiantis</div> <em>Entropy</em> (2020-12-25) <a href="https://doi.org/gktm9k">https://doi.org/gktm9k</a> <div class="csl-block">DOI: <a href="https://doi.org/10.3390/e23010018">10.3390/e23010018</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/33375658">33375658</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7824368">PMC7824368</a></div></div>
</div>
<div id="ref-1EfmL7jzn" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">148. </div><div class="csl-right-inline"><strong>Language Models as Knowledge Bases?</strong> <div class="csl-block">Fabio Petroni, Tim Rocktäschel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H Miller, Sebastian Riedel</div> <em>arXiv</em> (2019) <a href="https://doi.org/g958zw">https://doi.org/g958zw</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.1909.01066">10.48550/arxiv.1909.01066</a></div></div>
</div>
<div id="ref-Cq4JGOGX" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">149. </div><div class="csl-right-inline"><strong>Towards A Rigorous Science of Interpretable Machine Learning</strong> <div class="csl-block">Finale Doshi-Velez, Been Kim</div> <em>arXiv</em> (2017) <a href="https://doi.org/h3cz">https://doi.org/h3cz</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.1702.08608">10.48550/arxiv.1702.08608</a></div></div>
</div>
<div id="ref-8C0DuyuD" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">150. </div><div class="csl-right-inline"><strong>Rethinking Explainable Machine Learning as Applied Statistics</strong> <div class="csl-block">Sebastian Bordt, Eric Raidl, Ulrike von Luxburg</div> <em>arXiv</em> (2024) <a href="https://doi.org/g958z9">https://doi.org/g958z9</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2402.02870">10.48550/arxiv.2402.02870</a></div></div>
</div>
<div id="ref-uXcqmE5X" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">151. </div><div class="csl-right-inline"><strong>"Why Should I Trust You?": Explaining the Predictions of Any Classifier</strong> <div class="csl-block">Marco Tulio Ribeiro, Sameer Singh, Carlos Guestrin</div> <em>arXiv</em> (2016) <a href="https://doi.org/hsmk">https://doi.org/hsmk</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.1602.04938">10.48550/arxiv.1602.04938</a></div></div>
</div>
<div id="ref-rkxTwVjs" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">152. </div><div class="csl-right-inline"><strong>A Unified Approach to Interpreting Model Predictions</strong> <div class="csl-block">Scott Lundberg, Su-In Lee</div> <em>arXiv</em> (2017) <a href="https://doi.org/gp6hf4">https://doi.org/gp6hf4</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.1705.07874">10.48550/arxiv.1705.07874</a></div></div>
</div>
<div id="ref-11Dolfu34" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">153. </div><div class="csl-right-inline"><strong>Axiomatic Attribution for Deep Networks</strong> <div class="csl-block">Mukund Sundararajan, Ankur Taly, Qiqi Yan</div> <em>arXiv</em> (2017) <a href="https://doi.org/grx4kq">https://doi.org/grx4kq</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.1703.01365">10.48550/arxiv.1703.01365</a></div></div>
</div>
<div id="ref-BvgiYaxe" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">154. </div><div class="csl-right-inline"><strong>Learning Important Features Through Propagating Activation Differences</strong> <div class="csl-block">Avanti Shrikumar, Peyton Greenside, Anshul Kundaje</div> <em>arXiv</em> (2017) <a href="https://doi.org/g958zg">https://doi.org/g958zg</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.1704.02685">10.48550/arxiv.1704.02685</a></div></div>
</div>
<div id="ref-jO5B8YvG" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">155. </div><div class="csl-right-inline"><strong>This Looks Like That: Deep Learning for Interpretable Image Recognition</strong> <div class="csl-block">Chaofan Chen, Oscar Li, Chaofan Tao, Alina Jade Barnett, Jonathan Su, Cynthia Rudin</div> <em>arXiv</em> (2018) <a href="https://doi.org/kg6p">https://doi.org/kg6p</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.1806.10574">10.48550/arxiv.1806.10574</a></div></div>
</div>
<div id="ref-KvoFCtdS" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">156. </div><div class="csl-right-inline"><strong>Deep k-Nearest Neighbors: Towards Confident, Interpretable and Robust Deep Learning</strong> <div class="csl-block">Nicolas Papernot, Patrick McDaniel</div> <em>arXiv</em> (2018) <a href="https://doi.org/g958zm">https://doi.org/g958zm</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.1803.04765">10.48550/arxiv.1803.04765</a></div></div>
</div>
<div id="ref-167ItAuU7" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">157. </div><div class="csl-right-inline"><strong>Understanding Black-box Predictions via Influence Functions</strong> <div class="csl-block">Pang Wei Koh, Percy Liang</div> <em>arXiv</em> (2017) <a href="https://doi.org/mcsx">https://doi.org/mcsx</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.1703.04730">10.48550/arxiv.1703.04730</a></div></div>
</div>
<div id="ref-NGhi4Vf3" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">158. </div><div class="csl-right-inline"><strong>Inference Suboptimality in Variational Autoencoders</strong> <div class="csl-block">Chris Cremer, Xuechen Li, David Duvenaud</div> <em>arXiv</em> (2018) <a href="https://doi.org/g958zj">https://doi.org/g958zj</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.1801.03558">10.48550/arxiv.1801.03558</a></div></div>
</div>
<div id="ref-INMMN2Vz" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">159. </div><div class="csl-right-inline"><strong>beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework</strong> <div class="csl-block">Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, Alexander Lerchner</div> <em>International Conference on Learning Representations</em> (2017) <a href="https://openreview.net/forum?id=Sy2fzU9gl">https://openreview.net/forum?id=Sy2fzU9gl</a></div>
</div>
<div id="ref-14BVLTOJq" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">160. </div><div class="csl-right-inline"><strong>Deep Variational Information Bottleneck</strong> <div class="csl-block">Alexander A Alemi, Ian Fischer, Joshua V Dillon, Kevin Murphy</div> <em>arXiv</em> (2016) <a href="https://doi.org/gq9mrm">https://doi.org/gq9mrm</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.1612.00410">10.48550/arxiv.1612.00410</a></div></div>
</div>
<div id="ref-1Cd8cgDM6" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">161. </div><div class="csl-right-inline"><strong>Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV)</strong> <div class="csl-block">Been Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai, James Wexler, Fernanda Viegas, Rory Sayres</div> <em>arXiv</em> (2017) <a href="https://doi.org/khk9">https://doi.org/khk9</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.1711.11279">10.48550/arxiv.1711.11279</a></div></div>
</div>
<div id="ref-K4VkXXUf" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">162. </div><div class="csl-right-inline"><strong>Towards Automatic Concept-based Explanations</strong> <div class="csl-block">Amirata Ghorbani, James Wexler, James Zou, Been Kim</div> <em>arXiv</em> (2019) <a href="https://doi.org/kf7w">https://doi.org/kf7w</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.1902.03129">10.48550/arxiv.1902.03129</a></div></div>
</div>
<div id="ref-URCTSFCA" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">163. </div><div class="csl-right-inline"><strong>Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations</strong> <div class="csl-block">Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar Rätsch, Sylvain Gelly, Bernhard Schölkopf, Olivier Bachem</div> <em>arXiv</em> (2018) <a href="https://doi.org/grx79c">https://doi.org/grx79c</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.1811.12359">10.48550/arxiv.1811.12359</a></div></div>
</div>
<div id="ref-12CgohhqK" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">164. </div><div class="csl-right-inline"><strong>A Framework for the Quantitative Evaluation of Disentangled Representations</strong> <div class="csl-block">Cian Eastwood, Christopher KI Williams</div> <em>International Conference on Learning Representations</em> (2018) <a href="https://openreview.net/forum?id=By-7dz-AZ">https://openreview.net/forum?id=By-7dz-AZ</a></div>
</div>
<div id="ref-y1dj3AHA" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">165. </div><div class="csl-right-inline"><strong>Understanding intermediate layers using linear classifier probes</strong> <div class="csl-block">Guillaume Alain, Yoshua Bengio</div> <em>arXiv</em> (2016) <a href="https://doi.org/khmg">https://doi.org/khmg</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.1610.01644">10.48550/arxiv.1610.01644</a></div></div>
</div>
<div id="ref-11dRZlKVC" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">166. </div><div class="csl-right-inline"><strong>What do you learn from context? Probing for sentence structure in contextualized word representations</strong> <div class="csl-block">Ian Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, RThomas McCoy, Najoung Kim, Benjamin Van Durme, Samuel R Bowman, Dipanjan Das, Ellie Pavlick</div> <em>arXiv</em> (2019) <a href="https://doi.org/g958zv">https://doi.org/g958zv</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.1905.06316">10.48550/arxiv.1905.06316</a></div></div>
</div>
<div id="ref-32o7NfNa" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">167. </div><div class="csl-right-inline"><strong>Locating and Editing Factual Associations in GPT</strong> <div class="csl-block">Kevin Meng, David Bau, Alex Andonian, Yonatan Belinkov</div> <em>arXiv</em> (2022) <a href="https://doi.org/g958z2">https://doi.org/g958z2</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2202.05262">10.48550/arxiv.2202.05262</a></div></div>
</div>
<div id="ref-mvLZYXJQ" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">168. </div><div class="csl-right-inline"><strong>Knowledge Neurons in Pretrained Transformers</strong> <div class="csl-block">Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang, Furu Wei</div> <em>arXiv</em> (2021) <a href="https://doi.org/gs8cqf">https://doi.org/gs8cqf</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2104.08696">10.48550/arxiv.2104.08696</a></div></div>
</div>
<div id="ref-RvAOKYai" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">169. </div><div class="csl-right-inline"><strong>In-Context Explainers: Harnessing LLMs for Explaining Black Box Models</strong> <div class="csl-block">Nicholas Kroeger, Dan Ley, Satyapriya Krishna, Chirag Agarwal, Himabindu Lakkaraju</div> <em>arXiv</em> (2023) <a href="https://doi.org/g95xwd">https://doi.org/g95xwd</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2310.05797">10.48550/arxiv.2310.05797</a></div></div>
</div>
<div id="ref-gOxiOg9S" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">170. </div><div class="csl-right-inline"><strong>A Framework for Evaluating Post Hoc Feature-Additive Explainers</strong> <div class="csl-block">Zachariah Carmichael, Walter J Scheirer</div> <em>arXiv</em> (2021) <a href="https://doi.org/g958zz">https://doi.org/g958zz</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2106.08376">10.48550/arxiv.2106.08376</a></div></div>
</div>
<div id="ref-RzmVBDLx" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">171. </div><div class="csl-right-inline"><strong>Evaluation of post-hoc interpretability methods in time-series classification</strong> <div class="csl-block">Hugues Turbé, Mina Bjelogrlic, Christian Lovis, Gianmarco Mengaldo</div> <em>Nature Machine Intelligence</em> (2023-03-13) <a href="https://doi.org/grzdnk">https://doi.org/grzdnk</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1038/s42256-023-00620-w">10.1038/s42256-023-00620-w</a></div></div>
</div>
<div id="ref-b0a9ckui" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">172. </div><div class="csl-right-inline"><strong>Concept Bottleneck Models</strong> <div class="csl-block">Pang Wei Koh, Thao Nguyen, Yew Siang Tang, Stephen Mussmann, Emma Pierson, Been Kim, Percy Liang</div> <em>arXiv</em> (2020) <a href="https://doi.org/khz6">https://doi.org/khz6</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2007.04612">10.48550/arxiv.2007.04612</a></div></div>
</div>
<div id="ref-1ARNqdn4y" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">173. </div><div class="csl-right-inline"><strong>Manipulating and Measuring Model Interpretability</strong> <div class="csl-block">Forough Poursabzi-Sangdeh, Daniel G Goldstein, Jake M Hofman, Jennifer Wortman Vaughan, Hanna Wallach</div> <em>arXiv</em> (2018) <a href="https://doi.org/g958zk">https://doi.org/g958zk</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.1802.07810">10.48550/arxiv.1802.07810</a></div></div>
</div>
<div id="ref-15RY4QKp3" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">174. </div><div class="csl-right-inline"><strong>Principles of risk minimization for learning theory</strong> <div class="csl-block">V Vapnik</div> <em>Advances in Neural Information Processing Systems 5 (NIPS'91)</em> (1991) <a href="https://dl.acm.org/doi/10.5555/2986916.2987018">https://dl.acm.org/doi/10.5555/2986916.2987018</a> <div class="csl-block">ISBN: 1558602224</div></div>
</div>
<div id="ref-1DeCFT6PA" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">175. </div><div class="csl-right-inline"><strong>Invariant Risk Minimization</strong> <div class="csl-block">Martin Arjovsky, LÃ©on Bottou, Ishaan Gulrajani, David Lopez-Paz</div> <em>arXiv</em> (2020-03-31) <a href="https://arxiv.org/abs/1907.02893">https://arxiv.org/abs/1907.02893</a></div>
</div>
<div id="ref-1FLMzrLE9" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">176. </div><div class="csl-right-inline"><strong>The Risks of Invariant Risk Minimization</strong> <div class="csl-block">Elan Rosenfeld, Pradeep Ravikumar, Andrej Risteski</div> <em>arXiv</em> (2021-03-30) <a href="https://arxiv.org/abs/2010.05761">https://arxiv.org/abs/2010.05761</a></div>
</div>
<div id="ref-11IMWGprl" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">177. </div><div class="csl-right-inline"><strong>Out-of-Distribution Generalization via Risk Extrapolation (REx)</strong> <div class="csl-block">David Krueger, Ethan Caballero, Joern-Henrik Jacobsen, Amy Zhang, Jonathan Binas, Dinghuai Zhang, Remi Le Priol, Aaron Courville</div> <em>arXiv</em> (2021-02-26) <a href="https://arxiv.org/abs/2003.00688">https://arxiv.org/abs/2003.00688</a></div>
</div>
<div id="ref-L6xa6qzg" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">178. </div><div class="csl-right-inline"><strong>Conditional Variance Penalties and Domain Shift Robustness</strong> <div class="csl-block">Christina Heinze-Deml, Nicolai Meinshausen</div> <em>arXiv</em> (2019-04-16) <a href="https://arxiv.org/abs/1710.11469">https://arxiv.org/abs/1710.11469</a></div>
</div>
<div id="ref-JhS4KMR7" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">179. </div><div class="csl-right-inline"><strong>Causal inference using invariant prediction: identification and confidence intervals</strong> <div class="csl-block">Jonas Peters, Peter BÃ¼hlmann, Nicolai Meinshausen</div> <em>arXiv</em> (2024-04-26) <a href="https://arxiv.org/abs/1501.01332">https://arxiv.org/abs/1501.01332</a></div>
</div>
<div id="ref-B0L8KJ8W" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">180. </div><div class="csl-right-inline"><strong>Towards Deep Learning Models Resistant to Adversarial Attacks</strong> <div class="csl-block">Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, Adrian Vladu</div> <em>arXiv</em> (2019-09-06) <a href="https://arxiv.org/abs/1706.06083">https://arxiv.org/abs/1706.06083</a></div>
</div>
<div id="ref-ylSNfYug" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">181. </div><div class="csl-right-inline"><strong>Robustness May Be at Odds with Accuracy</strong> <div class="csl-block">Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, Aleksander Madry</div> <em>arXiv</em> (2019-09-10) <a href="https://arxiv.org/abs/1805.12152">https://arxiv.org/abs/1805.12152</a></div>
</div>
<div id="ref-l5C1P3il" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">182. </div><div class="csl-right-inline"><strong>Adversarial Examples Are Not Bugs, They Are Features</strong> <div class="csl-block">Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon Tran, Aleksander Madry</div> <em>arXiv</em> (2019-08-13) <a href="https://arxiv.org/abs/1905.02175">https://arxiv.org/abs/1905.02175</a></div>
</div>
<div id="ref-CYfSdHYg" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">183. </div><div class="csl-right-inline"><strong>The Rise and Potential of Large Language Model Based Agents: A Survey</strong> <div class="csl-block">Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, … Tao Gui</div> <em>arXiv</em> (2023-09-20) <a href="https://arxiv.org/abs/2309.07864">https://arxiv.org/abs/2309.07864</a></div>
</div>
<div id="ref-Jm8Kx8HW" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">184. </div><div class="csl-right-inline"><strong>Distributionally Robust Neural Networks for Group Shifts: On the Importance of Regularization for Worst-Case Generalization</strong> <div class="csl-block">Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, Percy Liang</div> <em>arXiv</em> (2020-04-03) <a href="https://arxiv.org/abs/1911.08731">https://arxiv.org/abs/1911.08731</a></div>
</div>
<div id="ref-EXaywYVO" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">185. </div><div class="csl-right-inline"><strong>Just Train Twice: Improving Group Robustness without Training Group Information</strong> <div class="csl-block">Evan Zheran Liu, Behzad Haghgoo, Annie S Chen, Aditi Raghunathan, Pang Wei Koh, Shiori Sagawa, Percy Liang, Chelsea Finn</div> <em>arXiv</em> (2021-09-28) <a href="https://arxiv.org/abs/2107.09044">https://arxiv.org/abs/2107.09044</a></div>
</div>
<div id="ref-10151coVE" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">186. </div><div class="csl-right-inline"><strong>Environment Inference for Invariant Learning</strong> <div class="csl-block">Elliot Creager, JÃ¶rn-Henrik Jacobsen, Richard Zemel</div> <em>arXiv</em> (2021-07-16) <a href="https://arxiv.org/abs/2010.07249">https://arxiv.org/abs/2010.07249</a></div>
</div>
<div id="ref-UqtGdFFv" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">187. </div><div class="csl-right-inline"><strong>Multilevel Statistical Models</strong> <div class="csl-block">Harvey Goldstein</div> <em>Wiley Series in Probability and Statistics</em> (2010-10-29) <a href="https://doi.org/cj8tgk">https://doi.org/cj8tgk</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1002/9780470973394">10.1002/9780470973394</a></div></div>
</div>
<div id="ref-1BXylhj0T" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">188. </div><div class="csl-right-inline"><strong>Multilevel growth curve models that incorporate a random coefficient model for the level 1 variance function</strong> <div class="csl-block">Harvey Goldstein, George Leckie, Christopher Charlton, Kate Tilling, William J Browne</div> <em>Statistical Methods in Medical Research</em> (2017-05-01) <a href="https://doi.org/f95xmx">https://doi.org/f95xmx</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1177/0962280217706728">10.1177/0962280217706728</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/28459180">28459180</a></div></div>
</div>
<div id="ref-ow7VHsto" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">189. </div><div class="csl-right-inline"><strong>A Bayesian multilevel time‐varying framework for joint modeling of hospitalization and survival in patients on dialysis</strong> <div class="csl-block">Esra Kürüm, Danh V Nguyen, Sudipto Banerjee, Yihao Li, Connie M Rhee, Damla Şentürk</div> <em>Statistics in Medicine</em> (2022-10) <a href="https://doi.org/g96dmg">https://doi.org/g96dmg</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1002/sim.9582">10.1002/sim.9582</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/36181392">36181392</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9931182">PMC9931182</a></div></div>
</div>
<div id="ref-1HdkTgLul" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">190. </div><div class="csl-right-inline"><strong>Dynamic effects of increasing heterogeneity in financial markets</strong> <div class="csl-block">Ahmad K Naimzada, Giorgio Ricchiuti</div> <em>Chaos, Solitons &amp;amp; Fractals</em> (2009-08) <a href="https://doi.org/bfbqxn">https://doi.org/bfbqxn</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1016/j.chaos.2008.07.022">10.1016/j.chaos.2008.07.022</a></div></div>
</div>
<div id="ref-167NV1YDH" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">191. </div><div class="csl-right-inline"><strong>Bayesian Forecasting in Economics and Finance: A Modern Review</strong> <div class="csl-block">Gael M Martin, David T Frazier, Worapree Maneesoonthorn, Ruben Loaiza-Maya, Florian Huber, Gary Koop, John Maheu, Didier Nibbering, Anastasios Panagiotelis</div> <em>arXiv</em> (2022) <a href="https://doi.org/g96dmk">https://doi.org/g96dmk</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2212.03471">10.48550/arxiv.2212.03471</a></div></div>
</div>
<div id="ref-4aDIdh6z" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">192. </div><div class="csl-right-inline"><strong>Bayesian Dynamic Factor Models for High-dimensional Matrix-valued Time Series</strong> <div class="csl-block">Wei Zhang</div> <em>arXiv</em> (2024) <a href="https://doi.org/g96dmm">https://doi.org/g96dmm</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2409.08354">10.48550/arxiv.2409.08354</a></div></div>
</div>
<div id="ref-E4f9DQPu" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">193. </div><div class="csl-right-inline"><strong>International Asset Allocation With Regime Shifts</strong> <div class="csl-block">Andrew Ang, Geert Bekaert</div> <em>Review of Financial Studies</em> (2002-07) <a href="https://doi.org/b535qr">https://doi.org/b535qr</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1093/rfs/15.4.1137">10.1093/rfs/15.4.1137</a></div></div>
</div>
<div id="ref-jH6SpM8M" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">194. </div><div class="csl-right-inline"><strong>A Model-Based Method for Remaining Useful Life Prediction of Machinery</strong> <div class="csl-block">Yaguo Lei, Naipeng Li, Szymon Gontarz, Jing Lin, Stanislaw Radkowski, Jacek Dybala</div> <em>IEEE Transactions on Reliability</em> (2016-09) <a href="https://doi.org/f82bw2">https://doi.org/f82bw2</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1109/tr.2016.2570568">10.1109/tr.2016.2570568</a></div></div>
</div>
<div id="ref-2CxZoj5J" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">195. </div><div class="csl-right-inline"><strong>Predictive maintenance in the Industry 4.0: A systematic literature review</strong> <div class="csl-block">Tiago Zonta, Cristiano André da Costa, Rodrigo da Rosa Righi, Miromar José de Lima, Eduardo Silveira da Trindade, Guann Pyng Li</div> <em>Computers &amp;amp; Industrial Engineering</em> (2020-12) <a href="https://doi.org/ghtwvv">https://doi.org/ghtwvv</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1016/j.cie.2020.106889">10.1016/j.cie.2020.106889</a></div></div>
</div>
<div id="ref-8MSb5kh8" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">196. </div><div class="csl-right-inline"><strong>Predictive Maintenance Approaches in Industry 4.0: A Systematic Literature Review</strong> <div class="csl-block">Fidma Mohamed Abdelillah, Hamour Nora, Ouchani Samir, Sidi Mohamed Benslimane</div> <em>2023 IEEE International Conference on Enabling Technologies: Infrastructure for Collaborative Enterprises (WETICE)</em> (2023-12-14) <a href="https://doi.org/g96qvf">https://doi.org/g96qvf</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1109/wetice57085.2023.10477842">10.1109/wetice57085.2023.10477842</a></div></div>
</div>
<div id="ref-1C8sIEO7D" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">197. </div><div class="csl-right-inline"><strong>A data-driven and context-aware approach for demand forecasting in the beverage industry</strong> <div class="csl-block">Benedict Jun Ma, Ilya Jackson, Maggie Huang, Sebastian Villegas, Jaime Macias-Aguayo</div> <em>International Journal of Logistics Research and Applications</em> (2025-10-10) <a href="https://doi.org/g96qvd">https://doi.org/g96qvd</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1080/13675567.2025.2566806">10.1080/13675567.2025.2566806</a></div></div>
</div>
<div id="ref-138XfoxXm" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">198. </div><div class="csl-right-inline"><strong>Chaotic Bayesian Inference: Strange Attractors as Risk Models for Black Swan Events</strong> <div class="csl-block">Crystal Rust</div> <em>arXiv</em> (2025-09-11) <a href="https://arxiv.org/abs/2509.08183">https://arxiv.org/abs/2509.08183</a></div>
</div>
<div id="ref-1C0w0XTvj" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">199. </div><div class="csl-right-inline"><strong>A Multi-target Bayesian Transformer Framework for Predicting Cardiovascular Disease Biomarkers during Pandemics</strong> <div class="csl-block">Trusting Inekwe, Winnie Mkandawire, Emmanuel Agu, Andres Colubri</div> <em>arXiv</em> (2025-11-07) <a href="https://arxiv.org/abs/2509.01794">https://arxiv.org/abs/2509.01794</a></div>
</div>
<div id="ref-RkqVE8TP" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">200. </div><div class="csl-right-inline"><strong>Bayesian Dynamic Factor Models for High-dimensional Matrix-valued Time Series</strong> <div class="csl-block">Wei Zhang</div> <em>arXiv</em> (2025-08-11) <a href="https://arxiv.org/abs/2409.08354">https://arxiv.org/abs/2409.08354</a></div>
</div>
<div id="ref-nTk9nk8h" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">201. </div><div class="csl-right-inline"><strong>Bayesian Models for Joint Selection of Features and Auto-Regressive Lags: Theory and Applications in Environmental and Financial Forecasting</strong> <div class="csl-block">Alokesh Manna, Sujit K Ghosh</div> <em>arXiv</em> (2025-08-18) <a href="https://arxiv.org/abs/2508.10055">https://arxiv.org/abs/2508.10055</a></div>
</div>
<div id="ref-gfPki3PM" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">202. </div><div class="csl-right-inline"><strong>SafeInfer: Context Adaptive Decoding Time Safety Alignment for Large Language Models</strong> <div class="csl-block">Somnath Banerjee, Sayan Layek, Soham Tripathy, Shanu Kumar, Animesh Mukherjee, Rima Hazra</div> <em>arXiv</em> (2024-12-17) <a href="https://arxiv.org/abs/2406.12274">https://arxiv.org/abs/2406.12274</a></div>
</div>
<div id="ref-xoOsuDeH" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">203. </div><div class="csl-right-inline"><strong>Robustness, Evaluation and Adaptation of Machine Learning Models in the Wild</strong> <div class="csl-block">Vihari Piratla</div> <em>arXiv</em> (2023-03-05) <a href="https://arxiv.org/abs/2303.02781v1">https://arxiv.org/abs/2303.02781v1</a></div>
</div>
<div id="ref-yApXu0Vp" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">204. </div><div class="csl-right-inline"><strong>A Comprehensive Survey of Retrieval-Augmented Generation (RAG): Evolution, Current Landscape and Future Directions</strong> <div class="csl-block">Shailja Gupta, Rajesh Ranjan, Surya Narayan Singh</div> <em>arXiv</em> (2024-10-18) <a href="https://arxiv.org/abs/2410.12837">https://arxiv.org/abs/2410.12837</a></div>
</div>
<div id="ref-1BPbcGQGq" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">205. </div><div class="csl-right-inline"><strong>Retrieval-Augmented Generation for AI-Generated Content: A Survey</strong> <div class="csl-block">Penghao Zhao, Hailin Zhang, Qinhan Yu, Zhengren Wang, Yunteng Geng, Fangcheng Fu, Ling Yang, Wentao Zhang, Jie Jiang, Bin Cui</div> <em>arXiv</em> (2024-06-24) <a href="https://arxiv.org/abs/2402.19473">https://arxiv.org/abs/2402.19473</a></div>
</div>
<div id="ref-1HqcEkGab" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">206. </div><div class="csl-right-inline"><strong>Billion-scale similarity search with GPUs</strong> <div class="csl-block">Jeff Johnson, Matthijs Douze, HervÃ© JÃ©gou</div> <em>arXiv</em> (2018-06-07) <a href="https://arxiv.org/abs/1702.08734">https://arxiv.org/abs/1702.08734</a></div>
</div>
<div id="ref-EsknayJ2" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">207. </div><div class="csl-right-inline"><strong>From Human Memory to AI Memory: A Survey on Memory Mechanisms in the Era of LLMs</strong> <div class="csl-block">Yaxiong Wu, Sheng Liang, Chen Zhang, Yichao Wang, Yongyue Zhang, Huifeng Guo, Ruiming Tang, Yong Liu</div> <em>arXiv</em> (2025-04-24) <a href="https://arxiv.org/abs/2504.15965">https://arxiv.org/abs/2504.15965</a></div>
</div>
<div id="ref-QciOeUHY" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">208. </div><div class="csl-right-inline"><strong>Memory OS of AI Agent</strong> <div class="csl-block">Jiazheng Kang, Mingming Ji, Zhe Zhao, Ting Bai</div> <em>arXiv</em> (2025-06-10) <a href="https://arxiv.org/abs/2506.06326">https://arxiv.org/abs/2506.06326</a></div>
</div>
<div id="ref-D4deReQm" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">209. </div><div class="csl-right-inline"><strong>Efficient Streaming Language Models with Attention Sinks</strong> <div class="csl-block">Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, Mike Lewis</div> <em>arXiv</em> (2024-04-09) <a href="https://arxiv.org/abs/2309.17453">https://arxiv.org/abs/2309.17453</a></div>
</div>
<div id="ref-PF464FGD" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">210. </div><div class="csl-right-inline"><strong>Efficient Memory Management for Large Language Model Serving with PagedAttention</strong> <div class="csl-block">Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E Gonzalez, Hao Zhang, Ion Stoica</div> <em>arXiv</em> (2023-09-13) <a href="https://arxiv.org/abs/2309.06180">https://arxiv.org/abs/2309.06180</a></div>
</div>
<div id="ref-At7PZDaG" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">211. </div><div class="csl-right-inline"><strong>On the Dangers of Stochastic Parrots</strong> <div class="csl-block">Emily M Bender, Timnit Gebru, Angelina McMillan-Major, Shmargaret Shmitchell</div> <em>Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency</em> (2021-03) <a href="https://doi.org/gh677h">https://doi.org/gh677h</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1145/3442188.3445922">10.1145/3442188.3445922</a></div></div>
</div>
<div id="ref-mFiH1ERh" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">212. </div><div class="csl-right-inline"><strong>Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer</strong> <div class="csl-block">Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, Jeff Dean</div> <em>arXiv</em> (2017) <a href="https://doi.org/g95xv5">https://doi.org/g95xv5</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.1701.06538">10.48550/arxiv.1701.06538</a></div></div>
</div>
<div id="ref-Pf2hk1xb" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">213. </div><div class="csl-right-inline"><strong>LoRA: Low-Rank Adaptation of Large Language Models</strong> <div class="csl-block">Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen</div> <em>arXiv</em> (2021) <a href="https://doi.org/gthszt">https://doi.org/gthszt</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2106.09685">10.48550/arxiv.2106.09685</a></div></div>
</div>
<div id="ref-6ALrEKtt" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">214. </div><div class="csl-right-inline"><strong>Curvature-Torsion Entropy for Twisted Curves under Curve Shortening Flow</strong> <div class="csl-block">Gabriel Khan</div> <em>arXiv</em> (2023) <a href="https://doi.org/g95xv9">https://doi.org/g95xv9</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2305.07171">10.48550/arxiv.2305.07171</a></div></div>
</div>
<div id="ref-Xtwwrjzy" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">215. </div><div class="csl-right-inline"><strong>LMPriors: Pre-Trained Language Models as Task-Specific Priors</strong> <div class="csl-block">Kristy Choi, Chris Cundy, Sanjari Srivastava, Stefano Ermon</div> <em>arXiv</em> (2022) <a href="https://doi.org/g9t22d">https://doi.org/g9t22d</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2210.12530">10.48550/arxiv.2210.12530</a></div></div>
</div>
<div id="ref-dO6oWLlh" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">216. </div><div class="csl-right-inline"><strong>AdapterFusion: Non-Destructive Task Composition for Transfer Learning</strong> <div class="csl-block">Jonas Pfeiffer, Aishwarya Kamath, Andreas Rücklé, Kyunghyun Cho, Iryna Gurevych</div> <em>arXiv</em> (2020) <a href="https://doi.org/g95xv7">https://doi.org/g95xv7</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2005.00247">10.48550/arxiv.2005.00247</a></div></div>
</div>
<div id="ref-1AEMaqZx6" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">217. </div><div class="csl-right-inline"><strong>Does Combining Parameter-efficient Modules Improve Few-shot Transfer Accuracy?</strong> <div class="csl-block">Nader Asadi, Mahdi Beitollahi, Yasser Khalil, Yinchuan Li, Guojun Zhang, Xi Chen</div> <em>arXiv</em> (2024) <a href="https://doi.org/g95xwf">https://doi.org/g95xwf</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2402.15414">10.48550/arxiv.2402.15414</a></div></div>
</div>
<div id="ref-iZG9n7mW" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">218. </div><div class="csl-right-inline"><strong>In-Context Learning through the Bayesian Prism</strong> <div class="csl-block">Madhur Panwar, Kabir Ahuja, Navin Goyal</div> <em>arXiv</em> (2023) <a href="https://doi.org/g95xwb">https://doi.org/g95xwb</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2306.04891">10.48550/arxiv.2306.04891</a></div></div>
</div>
<div id="ref-zPozLA0K" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">219. </div><div class="csl-right-inline"><strong>On Calibration of Modern Neural Networks</strong> <div class="csl-block">Chuan Guo, Geoff Pleiss, Yu Sun, Kilian Q Weinberger</div> <em>arXiv</em> (2017) <a href="https://doi.org/g95xv6">https://doi.org/g95xv6</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.1706.04599">10.48550/arxiv.1706.04599</a></div></div>
</div>
<div id="ref-XxvKtGf4" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">220. </div><div class="csl-right-inline"><strong>Holistic Evaluation of Language Models</strong> <div class="csl-block">Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, … Yuta Koreeda</div> <em>arXiv</em> (2022) <a href="https://doi.org/kh33">https://doi.org/kh33</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2211.09110">10.48550/arxiv.2211.09110</a></div></div>
</div>
<div id="ref-17lpGtuH5" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">221. </div><div class="csl-right-inline"><strong>GPT-4 Technical Report</strong> <div class="csl-block">OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, … Barret Zoph</div> <em>arXiv</em> (2023) <a href="https://doi.org/grx4cb">https://doi.org/grx4cb</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2303.08774">10.48550/arxiv.2303.08774</a></div></div>
</div>
<div id="ref-yWg7tQr1" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">222. </div><div class="csl-right-inline"><strong>The Llama 3 Herd of Models</strong> <div class="csl-block">Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, … Zhiyu Ma</div> <em>arXiv</em> (2024) <a href="https://doi.org/ndw6">https://doi.org/ndw6</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2407.21783">10.48550/arxiv.2407.21783</a></div></div>
</div>
<div id="ref-rYveVDKJ" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">223. </div><div class="csl-right-inline"><strong>TabPFN: A Transformer That Solves Small Tabular Classification Problems in a Second</strong> <div class="csl-block">Noah Hollmann, Samuel Müller, Katharina Eggensperger, Frank Hutter</div> <em>arXiv</em> (2022) <a href="https://doi.org/g9t22b">https://doi.org/g9t22b</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2207.01848">10.48550/arxiv.2207.01848</a></div></div>
</div>
<div id="ref-12nAa0T4v" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">224. </div><div class="csl-right-inline"><strong>Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing</strong> <div class="csl-block">Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, Graham Neubig</div> <em>ACM Computing Surveys</em> (2023-01-16) <a href="https://doi.org/gq5fh2">https://doi.org/gq5fh2</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1145/3560815">10.1145/3560815</a></div></div>
</div>
<div id="ref-11RvF4F7q" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">225. </div><div class="csl-right-inline"><strong>CHiLL: Zero-shot Custom Interpretable Feature Extraction from Clinical Notes with Large Language Models</strong> <div class="csl-block">Denis Jered McInerney, Geoffrey Young, Jan-Willem van de Meent, Byron C Wallace</div> <em>arXiv</em> (2023) <a href="https://doi.org/g9t22g">https://doi.org/g9t22g</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2302.12343">10.48550/arxiv.2302.12343</a></div></div>
</div>
<div id="ref-10hcHcmAG" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">226. </div><div class="csl-right-inline"><strong>Learning Interpretable Style Embeddings via Prompting LLMs</strong> <div class="csl-block">Ajay Patel, Delip Rao, Ansh Kothary, Kathleen McKeown, Chris Callison-Burch</div> <em>arXiv</em> (2023) <a href="https://doi.org/g9t22h">https://doi.org/g9t22h</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2305.12696">10.48550/arxiv.2305.12696</a></div></div>
</div>
<div id="ref-kAJDlMwy" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">227. </div><div class="csl-right-inline"><strong>Tree Prompting: Efficient Task Adaptation without Fine-Tuning</strong> <div class="csl-block">Chandan Singh, John Morris, Alexander Rush, Jianfeng Gao, Yuntian Deng</div> <em>Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</em> (2023) <a href="https://doi.org/gtgrkq">https://doi.org/gtgrkq</a> <div class="csl-block">DOI: <a href="https://doi.org/10.18653/v1/2023.emnlp-main.384">10.18653/v1/2023.emnlp-main.384</a></div></div>
</div>
<div id="ref-1AazNaZYl" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">228. </div><div class="csl-right-inline"><strong>One Embedder, Any Task: Instruction-Finetuned Text Embeddings</strong> <div class="csl-block">Hongjin Su, Weijia Shi, Jungo Kasai, Yizhong Wang, Yushi Hu, Mari Ostendorf, Wen-tau Yih, Noah A Smith, Luke Zettlemoyer, Tao Yu</div> <em>arXiv</em> (2022) <a href="https://doi.org/g9t22f">https://doi.org/g9t22f</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2212.09741">10.48550/arxiv.2212.09741</a></div></div>
</div>
<div id="ref-HQXzkG4Q" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">229. </div><div class="csl-right-inline"><strong>Augmenting interpretable models with large language models during training</strong> <div class="csl-block">Chandan Singh, Armin Askari, Rich Caruana, Jianfeng Gao</div> <em>Nature Communications</em> (2023-11-30) <a href="https://doi.org/g9t2z9">https://doi.org/g9t2z9</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1038/s41467-023-43713-1">10.1038/s41467-023-43713-1</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/38036543">38036543</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10689442">PMC10689442</a></div></div>
</div>
<div id="ref-XpHq6HEw" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">230. </div><div class="csl-right-inline"><strong>Explaining Datasets in Words: Statistical Models with Natural Language Parameters</strong> <div class="csl-block">Ruiqi Zhong, Heng Wang, Dan Klein, Jacob Steinhardt</div> <em>arXiv</em> (2024) <a href="https://doi.org/g9t22k">https://doi.org/g9t22k</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2409.08466">10.48550/arxiv.2409.08466</a></div></div>
</div>
<div id="ref-zWwbz3cX" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">231. </div><div class="csl-right-inline"><strong>Mixture-of-Experts Meets Instruction Tuning:A Winning Combination for Large Language Models</strong> <div class="csl-block">Sheng Shen, Le Hou, Yanqi Zhou, Nan Du, Shayne Longpre, Jason Wei, Hyung Won Chung, Barret Zoph, William Fedus, Xinyun Chen, … Denny Zhou</div> <em>arXiv</em> (2023) <a href="https://doi.org/g9t22j">https://doi.org/g9t22j</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2305.14705">10.48550/arxiv.2305.14705</a></div></div>
</div>
<div id="ref-R3AtGoca" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">232. </div><div class="csl-right-inline"><strong>Investigating Lane-Free Traffic with a Dynamic Driving Simulator</strong> <div class="csl-block">Maya Sekeran, Arslan Ali Syed, Johannes Lindner, Martin Margreiter, Klaus Bogenberger</div> <em>arXiv</em> (2023) <a href="https://doi.org/g93rnq">https://doi.org/g93rnq</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2311.16142">10.48550/arxiv.2311.16142</a></div></div>
</div>
<div id="ref-J9eXUymQ" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">233. </div><div class="csl-right-inline"><strong>An Overview of Perception Methods for Horticultural Robots: From Pollination to Harvest</strong> <div class="csl-block">Ho Seok Ahn, Feras Dayoub, Marija Popovic, Bruce MacDonald, Roland Siegwart, Inkyu Sa</div> <em>arXiv</em> (2018) <a href="https://doi.org/g93rnk">https://doi.org/g93rnk</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.1807.03124">10.48550/arxiv.1807.03124</a></div></div>
</div>
<div id="ref-1Ft0Gvajp" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">234. </div><div class="csl-right-inline"><strong>Agentic Context Engineering: Evolving Contexts for Self-Improving Language Models</strong> <div class="csl-block">Qizheng Zhang, Changran Hu, Shubhangi Upasani, Boyuan Ma, Fenglu Hong, Vamsidhar Kamanuru, Jay Rainton, Chen Wu, Mengmeng Ji, Hanchen Li, … Kunle Olukotun</div> <em>arXiv</em> (2025) <a href="https://doi.org/g96dmr">https://doi.org/g96dmr</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.2510.04618">10.48550/arxiv.2510.04618</a></div></div>
</div>
</div>
<h2 class="page_break_before" id="appendix-a">Appendix A</h2>
<p>This appendix gives full proofs for Proposition 1 and Corollary 1. We keep the weighted support-set notation from the Introduction and make all linear-algebra steps explicit.</p>
<hr />
<h3 id="a.0-preliminaries-and-identities">A.0 Preliminaries and identities</h3>
<ul>
<li><p><strong>Joint features.</strong> For any pair <span class="math inline">\((x,c)\)</span>, define<br />
<span class="math display">\[
\psi(x,c):=x\otimes \phi(c)\in\mathbb{R}^{d_x d_c}.
\]</span>
For each indexed training example <span class="math inline">\(a\)</span> (standing in for <span class="math inline">\((i,j)\)</span>), write <span class="math inline">\(\psi_a:=\psi(x_a,c_a)\)</span>.</p></li>
<li><p><strong>Design/labels/weights.</strong> Stack <span class="math inline">\(N=\sum_i m_i\)</span> training rows:
<span class="math display">\[
Z\in\mathbb{R}^{N\times d_x d_c}\ \text{ with rows } Z_a=\psi_a^T,\qquad
y\in\mathbb{R}^{N},\qquad
W=\mathrm{diag}(w_a)\in\mathbb{R}^{N\times N},\ w_a\ge 0.
\]</span>
Define the (unweighted) Gram matrix <span class="math inline">\(K:=ZZ^\top\)</span> and the weighted Gram
<span class="math display">\[
K_W:=W^{1/2} K\, W^{1/2} \;=\; W^{1/2} Z Z^\top W^{1/2}.
\]</span>
For a query <span class="math inline">\((x,c)\)</span>, let <span class="math inline">\(k(\cdot,(x,c)) := Z\,\psi(x,c)\in\mathbb{R}^N\)</span> and <span class="math inline">\(k_{(x,c)}:=W^{1/2}k(\cdot,(x,c))\)</span>.</p></li>
<li><p><strong>Vectorization identity.</strong> For conformable matrices <span class="math inline">\(A,B,C\)</span>,
<span class="math display">\[
\mathrm{vec}(A B C)=\big(C^\top\otimes A\big)\mathrm{vec}(B),\quad
\langle \mathrm{vec}(B),\,x\otimes z\rangle = x^\top B z.
\]</span></p></li>
<li><p><strong>Weighted ridge solution.</strong> For any <span class="math inline">\(X\in\mathbb{R}^{N\times p}\)</span>, ridge objective
<span class="math display">\[
\min_\beta \ \|W^{1/2}(y-X\beta)\|_2^2+\lambda\|\beta\|_2^2
\]</span>
has unique minimizer <span class="math inline">\(\widehat\beta=(X^\top W X+\lambda I)^{-1}X^\top W y\)</span> and equivalent dual form
<span class="math display">\[
\widehat\beta = X^\top W^{1/2}\big(W^{1/2}XX^\top W^{1/2}+\lambda I\big)^{-1}W^{1/2}y.
\]</span>
Predictions for a new feature vector <span class="math inline">\(x_\star\)</span> equal
<span class="math display">\[
\widehat f(x_\star)=x_\star^\top \widehat\beta
\;=\;
\underbrace{\big(W^{1/2}X x_\star\big)^\top}_{k_\star^\top}
\big(W^{1/2}XX^\top W^{1/2}+\lambda I\big)^{-1}
W^{1/2}y.
\]</span>
This is <strong>kernel ridge regression</strong> (KRR) with kernel <span class="math inline">\(K_W=W^{1/2}XX^\top W^{1/2}\)</span> and query vector <span class="math inline">\(k_\star=W^{1/2}X x_\star\)</span>.</p></li>
</ul>
<hr />
<h3 id="a.1-proof-of-proposition-1a-explicit-varying-coefficients-weighted-krr-on-joint-features">A.1 Proof of Proposition 1(A): explicit varying-coefficients ⇔ weighted KRR on joint features</h3>
<p>Assume the linear, squared-loss setting with <span class="math inline">\(y=\langle \theta(c),x\rangle+\varepsilon\)</span> and <span class="math inline">\(\mathbb{E}[\varepsilon]=0\)</span>.<br />
Let the varying-coefficients model be <span class="math inline">\(\theta(c)=B\,\phi(c)\)</span> with <span class="math inline">\(B\in\mathbb{R}^{d_x\times d_c}\)</span> and ridge penalty <span class="math inline">\(\lambda\|B\|_F^2\)</span>.</p>
<p><strong>Step 1 (reduce to ridge in joint-feature space).</strong><br />
Vectorize <span class="math inline">\(B\)</span> as <span class="math inline">\(\beta=\mathrm{vec}(B)\in\mathbb{R}^{d_x d_c}\)</span>.<br />
By the identity above,
<span class="math display">\[
x_a^T B\,\phi(c_a)
= \langle \beta,\, x_a\otimes \phi(c_a)\rangle
= \langle \beta,\,\psi_a\rangle.
\]</span>
Thus the weighted objective specialized from (★) is
<span class="math display">\[
\min_{\beta\in\mathbb{R}^{d_x d_c}}
\ \big\|W^{1/2}\big(y - Z\beta\big)\big\|_2^2 + \lambda \|\beta\|_2^2,
\]</span>
which is exactly weighted ridge with design <span class="math inline">\(X\equiv Z\)</span>.</p>
<p><strong>Step 2 (closed form and prediction).</strong><br />
By the ridge solution,
<span class="math display">\[
\widehat\beta=(Z^T W Z+\lambda I)^{-1} Z^T W y,
\]</span>
and the prediction at a query <span class="math inline">\((x,c)\)</span> with joint feature <span class="math inline">\(\psi(x,c)\)</span> is
<span class="math display">\[
\widehat y(x,c)=\psi(x,c)^T \widehat\beta
= \underbrace{\big(W^{1/2} Z\, \psi(x,c)\big)}_{k_{(x,c)}}^\top
\big(W^{1/2} Z Z^\top W^{1/2}+\lambda I\big)^{-1} W^{1/2} y.
\]</span></p>
<p><strong>Step 3 (kernel form).</strong><br />
Since <span class="math inline">\(K:=ZZ^T\)</span> and <span class="math inline">\(K_W:=W^{1/2} K W^{1/2}\)</span>,
<span class="math display">\[
\boxed{\ \widehat y(x,c)\;=\; k_{(x,c)}^T \big(K_W+\lambda I\big)^{-1} W^{1/2}y\ }.
\]</span>
Moreover, the <span class="math inline">\((a,b)\)</span>-th entry of the kernel matrix <span class="math inline">\(K\)</span> is
<span class="math display">\[
K_{ab}=\langle \psi_a,\psi_b\rangle
=\big\langle x_a\otimes \phi(c_a),\,x_b\otimes \phi(c_b)\big\rangle
=\langle x_a,x_b\rangle\cdot \langle \phi(c_a),\phi(c_b)\rangle,
\]</span>
so (A) is precisely <strong>KRR on joint features</strong> with sample weights <span class="math inline">\(W\)</span>.<br />
This proves part (A). ■</p>
<hr />
<h3 id="a.2-proof-of-proposition-1b-linear-icl-kernel-regression">A.2 Proof of Proposition 1(B): linear ICL ⇒ kernel regression</h3>
<p>We analyze a single attention layer operating on the weighted support set <span class="math inline">\(S(c)\)</span>, using <strong>linear</strong> maps for queries, keys, and values:
<span class="math display">\[
q(x,c)=Q\,\psi(x,c),\qquad k_a = K\,\psi_a,\qquad v_a=V\,\psi_a,
\]</span>
with <span class="math inline">\(Q\in\mathbb{R}^{d_q\times d_\psi}\)</span>, <span class="math inline">\(K\in\mathbb{R}^{d_k\times d_\psi}\)</span>, <span class="math inline">\(V\in\mathbb{R}^{d_v\times d_\psi}\)</span>, <span class="math inline">\(d_\psi=d_x d_c\)</span>. Let the <strong>unnormalized</strong> attention score for index <span class="math inline">\(a\)</span> be
<span class="math display">\[
s_a(x,c):=w_a\,\langle q(x,c),k_a\rangle \;=\; w_a\,\psi(x,c)^T Q^T K\,\psi_a .
\]</span>
Define normalized weights <span class="math inline">\(\alpha_a(x,c):=s_a(x,c)/\sum_b s_b(x,c)\)</span> (or any fixed positive normalization; the form below is pointwise in <span class="math inline">\(\{\alpha_a\}\)</span>). The context representation and scalar prediction are
<span class="math display">\[
z(x,c)=\sum_a \alpha_a(x,c)\, v_a,\qquad \widehat y(x,c)=u^T z(x,c).
\]</span></p>
<p>We prove two statements: <strong>(B1)</strong> exact KRR if the attention maps are fixed and only the readout is trained, and <strong>(B2)</strong> kernel regression with the NTK if the attention parameters are trained in the linearized regime.</p>
<h4 id="a.2.1-b1-fixed-attention-trained-linear-head-exact-krr">A.2.1 (B1) Fixed attention, trained linear head ⇒ exact KRR</h4>
<p>Assume <span class="math inline">\(Q,K,V\)</span> are fixed functions (pretrained or chosen a priori), hence <span class="math inline">\(\alpha_a(x,c)\)</span> are <strong>deterministic</strong> functions of <span class="math inline">\((x,c)\)</span> and the support set. Define the induced <strong>feature map</strong>
<span class="math display">\[
\varphi(x,c):=\sum_a \alpha_a(x,c)\, v_a \;\in\; \mathbb{R}^{d_v}.
\]</span>
Stack <span class="math inline">\(\varphi_a:=\varphi(x_a,c_a)\)</span> row-wise into <span class="math inline">\(\Phi\in\mathbb{R}^{N\times d_v}\)</span>. Training only the readout <span class="math inline">\(u\)</span> with weighted ridge,
<span class="math display">\[
\widehat u \in \arg\min_u \ \|W^{1/2}(y-\Phi u)\|_2^2+\lambda \|u\|_2^2
\]</span>
yields <span class="math inline">\(\widehat u=(\Phi^T W \Phi + \lambda I)^{-1}\Phi^T W y\)</span> and predictions
<span class="math display">\[
\widehat y(x,c)=\varphi(x,c)^T \widehat u
= \underbrace{\big(W^{1/2}\Phi\,\varphi(x,c)\big)}_{k_{(x,c)}}^T
\big(W^{1/2}\Phi\Phi^T W^{1/2}+\lambda I\big)^{-1} W^{1/2} y.
\]</span>
Therefore,
<span class="math display">\[
\boxed{\ \widehat y(x,c)=k_{(x,c)}^T \big(K_W+\lambda I\big)^{-1}W^{1/2}y\ },
\quad K_W:=W^{1/2}\underbrace{(\Phi\Phi^T)}_{=:K}W^{1/2},
\]</span>
which is exactly <strong>kernel ridge regression</strong> with kernel
<span class="math display">\[
k\big((x,c),(x&#39;,c&#39;)\big)=\langle \varphi(x,c),\varphi(x&#39;,c&#39;)\rangle.
\]</span>
Because <span class="math inline">\(v_a=V\psi_a\)</span> and <span class="math inline">\(\alpha_a(x,c)\propto w_a\,\psi(x,c)^T Q^T K \psi_a\)</span>, <span class="math inline">\(\varphi\)</span> is a linear transform of a <strong>weighted average of joint features</strong>; hence the kernel is a dot-product on linear transforms of <span class="math inline">\(\{\psi_a\}\)</span>. This proves (B1). ■</p>
<h4 id="a.2.2-b2-training-attention-in-the-linearizedntk-regime-kernel-regression-with-ntk">A.2.2 (B2) Training attention in the linearized/NTK regime ⇒ kernel regression with NTK</h4>
<p>Now let <span class="math inline">\(\theta=(Q,K,V,u)\)</span> be trainable, and suppose training uses squared loss with gradient flow (or sufficiently small steps) starting from initialization <span class="math inline">\(\theta_0\)</span>. The <strong>linearized model</strong> around <span class="math inline">\(\theta_0\)</span> is the first-order Taylor expansion
<span class="math display">\[
\widehat y_\theta(x,c)\;\approx\;\widehat y_{\theta_0}(x,c)+\nabla_\theta \widehat y_{\theta_0}(x,c)^T (\theta-\theta_0)
=: \widehat y_{\theta_0}(x,c) + \phi_{\mathrm{NTK}}(x,c)^T (\theta-\theta_0),
\]</span>
where <span class="math inline">\(\phi_{\mathrm{NTK}}(x,c):=\nabla_\theta \widehat y_{\theta_0}(x,c)\)</span> are the <strong>tangent features</strong>. Standard NTK results (for squared loss, gradient flow, and linearization-validity conditions) imply that the learned function equals <strong>kernel regression with the NTK</strong>:
<span class="math display">\[
k_{\mathrm{NTK}}\big((x,c),(x&#39;,c&#39;)\big)
:= \big\langle \phi_{\mathrm{NTK}}(x,c),\,\phi_{\mathrm{NTK}}(x&#39;,c&#39;)\big\rangle,
\]</span>
i.e., predictions have the KRR form with kernel <span class="math inline">\(K_{\mathrm{NTK}}\)</span> on the training set (and explicit ridge if used, or implicit regularization via early stopping).</p>
<p>It remains to identify the structure of <span class="math inline">\(\phi_{\mathrm{NTK}}\)</span> for our <strong>linear attention</strong> block and show it lies in the span of <strong>linear transforms of joint features</strong>. Differentiating
<span class="math inline">\(\widehat y(x,c)=u^T \sum_a \alpha_a(x,c)\, V\psi_a\)</span> at <span class="math inline">\(\theta_0\)</span> yields four groups of terms:</p>
<ul>
<li><p><strong>Readout path (<span class="math inline">\(u\)</span>).</strong> <span class="math inline">\(\partial \widehat y/\partial u = \sum_a \alpha_a(x,c)\, V\psi_a = \varphi_0(x,c)\)</span>. This is linear in <span class="math inline">\(\{\psi_a\}\)</span>.</p></li>
<li><p><strong>Value path (<span class="math inline">\(V\)</span>).</strong> <span class="math inline">\(\partial \widehat y/\partial V = \sum_a \alpha_a(x,c)\, u\,\psi_a^T\)</span>. This contributes terms of the form <span class="math inline">\((u\otimes I)\sum_a \alpha_a(x,c)\psi_a\)</span>, i.e., linear in <span class="math inline">\(\{\psi_a\}\)</span>.</p></li>
<li><p><strong>Query/key paths (<span class="math inline">\(Q,K\)</span>).</strong> For linear attention with scores <span class="math inline">\(s_a=w_a\,\psi(x,c)^T Q^T K \psi_a\)</span> and normalized <span class="math inline">\(\alpha_a=s_a/\sum_b s_b\)</span>, derivatives of <span class="math inline">\(\alpha_a\)</span> w.r.t. <span class="math inline">\(Q\)</span> and <span class="math inline">\(K\)</span> are linear combinations of <span class="math inline">\(\psi(x,c)\)</span> and <span class="math inline">\(\{\psi_a\}\)</span>:
<span class="math display">\[
\frac{\partial \alpha_a}{\partial Q}\propto
\sum_b \big[\delta_{ab}-\alpha_b(x,c)\big]\,
w_a w_b \big( K\psi_a\,\psi(x,c)^T \big),
\qquad
\frac{\partial \alpha_a}{\partial K}\propto
\sum_b \big[\delta_{ab}-\alpha_b(x,c)\big]\,
w_a w_b \big( \psi(x,c)\,\psi_a^T Q^T \big),
\]</span>
and hence <span class="math inline">\(\partial \widehat y/\partial Q\)</span>, <span class="math inline">\(\partial \widehat y/\partial K\)</span> are finite linear combinations of tensors each bilinear in <span class="math inline">\(\psi(x,c)\)</span> and some <span class="math inline">\(\psi_a\)</span>. Contracting with <span class="math inline">\(u\)</span> and <span class="math inline">\(V\)</span> produces terms <em>linear</em> in <span class="math inline">\(\psi(x,c)\)</span> and linear in the set <span class="math inline">\(\{\psi_a\}\)</span>.</p></li>
</ul>
<p>Collecting all components, the tangent feature map can be written as
<span class="math display">\[
\phi_{\mathrm{NTK}}(x,c)=\mathcal{L}\big(\psi(x,c),\{\psi_a\}\big),
\]</span>
where <span class="math inline">\(\mathcal{L}\)</span> is a fixed linear operator determined by <span class="math inline">\(\theta_0\)</span>, <span class="math inline">\(W\)</span>, and the normalization rule for attention. Consequently, the NTK takes the <strong>dot-product</strong> form
<span class="math display">\[
k_{\mathrm{NTK}}\big((x,c),(x&#39;,c&#39;)\big)=
\Psi(x,c)^T\, \mathcal{M}\, \Psi(x&#39;,c&#39;),
\]</span>
for some positive semidefinite matrix <span class="math inline">\(\mathcal{M}\)</span> and a finite-dimensional feature stack <span class="math inline">\(\Psi\)</span> that concatenates linear transforms of <span class="math inline">\(\psi(x,c)\)</span> and of the support-set <span class="math inline">\(\{\psi_a\}\)</span>. In particular, <span class="math inline">\(k_{\mathrm{NTK}}\)</span> is a dot-product kernel on <strong>linear transforms of the joint features</strong> (possibly augmented by normalization-dependent combinations). Therefore, training the linear-attention ICL model in the linearized regime equals kernel regression with such a kernel—completing (B2). ■</p>
<p><strong>Assumptions for A.2.2.</strong> Squared loss; gradient flow (or sufficiently small steps); initialization independent of the data; and a regime where the linearization error stays controlled over training (e.g., small learning rate, sufficient width/depth so that the NTK remains close to its initialization).</p>
<hr />
<h3 id="a.3-proof-of-corollary-1-retrievalgatingweighting-as-kernelmeasure-choices">A.3 Proof of Corollary 1: retrieval/gating/weighting as kernel/measure choices</h3>
<p>In both A.1 and A.2, predictions have the KRR form
<span class="math display">\[
\widehat y(x,c)=k_{(x,c)}^T \big(K^\sharp + \lambda I\big)^{-1} \mu,
\]</span>
where <span class="math inline">\(K^{\sharp}\)</span> is a positive semidefinite kernel matrix computed over the support set (e.g., <span class="math inline">\(K_W=W^{1/2}ZZ^T W^{1/2}\)</span> in A.1 or <span class="math inline">\(W^{1/2}\Phi\Phi^T W^{1/2}\)</span> / <span class="math inline">\(K_{\mathrm{NTK}}\)</span> in A.2), <span class="math inline">\(k_{(x,c)}\)</span> is the associated query vector, and <span class="math inline">\(\mu=W^{1/2}y\)</span> (or an equivalent reweighting).</p>
<ul>
<li><p><strong>Retrieval <span class="math inline">\(R(c)\)</span> / gating.</strong> Changing the support set <span class="math inline">\(S(c)\)</span> (e.g., via a retriever or a gating policy) <strong>removes or adds rows/columns</strong> in <span class="math inline">\(K^\sharp\)</span> and entries in <span class="math inline">\(k_{(x,c)}\)</span>. This is equivalent to changing the <strong>empirical measure</strong> over which the kernel smoother is computed (i.e., which samples contribute and how).</p></li>
<li><p><strong>Weights <span class="math inline">\(w_{ij}(c)\)</span>.</strong> Changing the weights modifies <span class="math inline">\(W\)</span> and hence replaces <span class="math inline">\(K\)</span> by <span class="math inline">\(K_W=W^{1/2} K W^{1/2}\)</span> and <span class="math inline">\(k\)</span> by <span class="math inline">\(k_{(x,c)}=W^{1/2}k\)</span>. This is standard <strong>importance weighting</strong> in kernel regression.</p></li>
<li><p><strong>Induced kernels.</strong> Attention, value projections, or learned encoders change the <strong>feature map</strong> (e.g., <span class="math inline">\(\psi\mapsto V\psi\)</span> or <span class="math inline">\(\psi\mapsto Q\psi\)</span>), thereby changing the kernel <span class="math inline">\(k((x,c),(x&#39;,c&#39;))=\langle \Phi(x,c),\Phi(x&#39;,c&#39;)\rangle\)</span>.</p></li>
</ul>
<p>Thus retrieval/gating instantiate <strong>neighborhood selection</strong> (measure choice), and value/query/key processing instantiate <strong>kernel choice</strong>. ■</p>
<hr />
<h3 id="a.4-remarks">A.4 Remarks</h3>
<ul>
<li><strong>No Gaussianity is required.</strong> Part (A) only uses squared loss and linear algebra; the noise model <span class="math inline">\(y=f(x,c)+\varepsilon\)</span> with <span class="math inline">\(\mathbb{E}[\varepsilon]=0\)</span> suffices.</li>
<li><strong>Early stopping vs. explicit ridge.</strong> If training uses early stopping rather than explicit <span class="math inline">\(\lambda\)</span>, the resulting predictor is still a kernel regressor with an <em>implicit</em> regularization parameter controlled by stopping time (for gradient flow on squared loss).</li>
<li><strong>Multiple layers / nonlinear value stacks.</strong> With deeper nonlinear stacks, the exact identities above become local/first-order (linearized) approximations; the NTK statement continues to apply under its usual conditions.</li>
</ul>
<!-- default theme -->

<style>
  /* import google fonts */
  @import url("https://fonts.googleapis.com/css?family=Open+Sans:400,600,700");
  @import url("https://fonts.googleapis.com/css?family=Source+Code+Pro");

  /* -------------------------------------------------- */
  /* global */
  /* -------------------------------------------------- */

  /* all elements */
  * {
    /* force sans-serif font unless specified otherwise */
    font-family: "Open Sans", "Helvetica", sans-serif;

    /* prevent text inflation on some mobile browsers */
    -webkit-text-size-adjust: none !important;
    -moz-text-size-adjust: none !important;
    -o-text-size-adjust: none !important;
    text-size-adjust: none !important;
  }

  @media only screen {
    /* "page" element */
    body {
      position: relative;
      box-sizing: border-box;
      font-size: 12pt;
      line-height: 1.5;
      max-width: 8.5in;
      margin: 20px auto;
      padding: 40px;
      border-radius: 5px;
      border: solid 1px #bdbdbd;
      box-shadow: 0 0 20px rgba(0, 0, 0, 0.05);
      background: #ffffff;
    }
  }

  /* when on screen < 8.5in wide */
  @media only screen and (max-width: 8.5in) {
    /* "page" element */
    body {
      padding: 20px;
      margin: 0;
      border-radius: 0;
      border: none;
      box-shadow: 0 0 20px rgba(0, 0, 0, 0.05) inset;
      background: none;
    }
  }

  /* -------------------------------------------------- */
  /* headings */
  /* -------------------------------------------------- */

  /* all headings */
  h1,
  h2,
  h3,
  h4,
  h5,
  h6 {
    margin: 20px 0;
    padding: 0;
    font-weight: bold;
  }

  /* biggest heading */
  h1 {
    margin: 40px 0;
    text-align: center;
  }

  /* second biggest heading */
  h2 {
    margin-top: 30px;
    padding-bottom: 5px;
    border-bottom: solid 1px #bdbdbd;
  }

  /* heading font sizes */
  h1 {
    font-size: 2em;
  }
  h2 {
    font-size: 1.5em;
  }
  h3 {
    font-size: 1.35em;
  }
  h4 {
    font-size: 1.25em;
  }
  h5 {
    font-size: 1.15em;
  }
  h6 {
    font-size: 1em;
  }

  /* -------------------------------------------------- */
  /* manuscript header */
  /* -------------------------------------------------- */

  /* manuscript title */
  header > h1 {
    margin: 0;
  }

  /* manuscript title caption text (ie "automatically generated on") */
  header + p {
    text-align: center;
    margin-top: 10px;
  }

  /* -------------------------------------------------- */
  /* text elements */
  /* -------------------------------------------------- */

  /* links */
  a {
    color: #2196f3;
    overflow-wrap: break-word;
  }

  /* superscripts and subscripts */
  sub,
  sup {
    /* prevent from affecting line height */
    line-height: 0;
  }

  /* unordered and ordered lists*/
  ul,
  ol {
    padding-left: 20px;
  }

  /* class for styling text semibold */
  .semibold {
    font-weight: 600;
  }

  /* class for styling elements horizontally left aligned */
  .left {
    display: block;
    text-align: left;
    margin-left: auto;
    margin-right: 0;
    justify-content: left;
  }

  /* class for styling elements horizontally centered */
  .center {
    display: block;
    text-align: center;
    margin-left: auto;
    margin-right: auto;
    justify-content: center;
  }

  /* class for styling elements horizontally right aligned */
  .right {
    display: block;
    text-align: right;
    margin-left: 0;
    margin-right: auto;
    justify-content: right;
  }

  /* -------------------------------------------------- */
  /* section elements */
  /* -------------------------------------------------- */

  /* horizontal divider line */
  hr {
    border: none;
    height: 1px;
    background: #bdbdbd;
  }

  /* paragraphs, horizontal dividers, figures, tables, code */
  p,
  hr,
  figure,
  table,
  pre {
    /* treat all as "paragraphs", with consistent vertical margins */
    margin-top: 20px;
    margin-bottom: 20px;
  }

  /* -------------------------------------------------- */
  /* figures */
  /* -------------------------------------------------- */

  /* figure */
  figure {
    max-width: 100%;
    margin-left: auto;
    margin-right: auto;
  }

  /* figure caption */
  figcaption {
    padding: 0;
    padding-top: 10px;
  }

  /* figure image element */
  figure > img,
  figure > svg {
    max-width: 100%;
    display: block;
    margin-left: auto;
    margin-right: auto;
  }

  /* figure auto-number */
  img + figcaption > span:first-of-type,
  svg + figcaption > span:first-of-type {
    font-weight: bold;
    margin-right: 5px;
  }

  /* -------------------------------------------------- */
  /* tables */
  /* -------------------------------------------------- */

  /* table */
  table {
    border-collapse: collapse;
    border-spacing: 0;
    width: 100%;
    margin-left: auto;
    margin-right: auto;
  }

  /* table cells */
  th,
  td {
    border: solid 1px #bdbdbd;
    padding: 10px;
    /* squash table if too wide for page by forcing line breaks */
    overflow-wrap: break-word;
    word-break: break-word;
  }

  /* header row and even rows */
  th,
  tr:nth-child(2n) {
    background-color: #fafafa;
  }

  /* odd rows */
  tr:nth-child(2n + 1) {
    background-color: #ffffff;
  }

  /* table caption */
  caption {
    text-align: left;
    padding: 0;
    padding-bottom: 10px;
  }

  /* table auto-number */
  table > caption > span:first-of-type {
    font-weight: bold;
    margin-right: 5px;
  }

  /* -------------------------------------------------- */
  /* code */
  /* -------------------------------------------------- */

  /* multi-line code block */
  pre {
    padding: 10px;
    background-color: #eeeeee;
    color: #000000;
    border-radius: 5px;
    break-inside: avoid;
    text-align: left;
  }

  /* inline code, ie code within normal text */
  :not(pre) > code {
    padding: 0 4px;
    background-color: #eeeeee;
    color: #000000;
    border-radius: 5px;
  }

  /* code text */
  /* apply all children, to reach syntax highlighting sub-elements */
  code,
  code * {
    /* force monospace font */
    font-family: "Source Code Pro", "Courier New", monospace;
  }

  /* -------------------------------------------------- */
  /* quotes */
  /* -------------------------------------------------- */

  /* quoted text */
  blockquote {
    margin: 0;
    padding: 0;
    border-left: 4px solid #bdbdbd;
    padding-left: 16px;
    break-inside: avoid;
  }

  /* -------------------------------------------------- */
  /* banners */
  /* -------------------------------------------------- */

  /* info banners */
  .banner {
    box-sizing: border-box;
    display: block;
    position: relative;
    width: 100%;
    margin-top: 20px;
    margin-bottom: 20px;
    padding: 20px;
    text-align: center;
  }

  /* paragraph in banner */
  .banner > p {
    margin: 0;
  }

  /* -------------------------------------------------- */
  /* highlight colors */
  /* -------------------------------------------------- */

  .white {
    background: #ffffff;
  }
  .lightgrey {
    background: #eeeeee;
  }
  .grey {
    background: #757575;
  }
  .darkgrey {
    background: #424242;
  }
  .black {
    background: #000000;
  }
  .lightred {
    background: #ffcdd2;
  }
  .lightyellow {
    background: #ffecb3;
  }
  .lightgreen {
    background: #dcedc8;
  }
  .lightblue {
    background: #e3f2fd;
  }
  .lightpurple {
    background: #f3e5f5;
  }
  .red {
    background: #f44336;
  }
  .orange {
    background: #ff9800;
  }
  .yellow {
    background: #ffeb3b;
  }
  .green {
    background: #4caf50;
  }
  .blue {
    background: #2196f3;
  }
  .purple {
    background: #9c27b0;
  }
  .white,
  .lightgrey,
  .lightred,
  .lightyellow,
  .lightgreen,
  .lightblue,
  .lightpurple,
  .orange,
  .yellow,
  .white a,
  .lightgrey a,
  .lightred a,
  .lightyellow a,
  .lightgreen a,
  .lightblue a,
  .lightpurple a,
  .orange a,
  .yellow a {
    color: #000000;
  }
  .grey,
  .darkgrey,
  .black,
  .red,
  .green,
  .blue,
  .purple,
  .grey a,
  .darkgrey a,
  .black a,
  .red a,
  .green a,
  .blue a,
  .purple a {
    color: #ffffff;
  }

  /* -------------------------------------------------- */
  /* buttons */
  /* -------------------------------------------------- */

  /* class for styling links like buttons */
  .button {
    display: inline-flex;
    justify-content: center;
    align-items: center;
    margin: 5px;
    padding: 10px 20px;
    font-size: 0.75em;
    font-weight: 600;
    text-transform: uppercase;
    text-decoration: none;
    letter-spacing: 1px;
    background: none;
    color: #2196f3;
    border: solid 1px #bdbdbd;
    border-radius: 5px;
  }

  /* buttons when hovered */
  .button:hover:not([disabled]),
  .icon_button:hover:not([disabled]) {
    cursor: pointer;
    background: #f5f5f5;
  }

  /* buttons when disabled */
  .button[disabled],
  .icon_button[disabled] {
    opacity: 0.35;
    pointer-events: none;
  }

  /* class for styling buttons containg only single icon */
  .icon_button {
    display: inline-flex;
    justify-content: center;
    align-items: center;
    text-decoration: none;
    margin: 0;
    padding: 0;
    background: none;
    border-radius: 5px;
    border: none;
    width: 20px;
    height: 20px;
    min-width: 20px;
    min-height: 20px;
  }

  /* icon button inner svg image */
  .icon_button > svg {
    height: 16px;
  }

  /* -------------------------------------------------- */
  /* icons */
  /* -------------------------------------------------- */

  /* class for styling icons inline with text */
  .inline_icon {
    height: 1em;
    position: relative;
    top: 0.125em;
  }

  /* -------------------------------------------------- */
  /* references */
  /* -------------------------------------------------- */

  .csl-entry {
    margin-top: 15px;
    margin-bottom: 15px;
  }

  /* -------------------------------------------------- */
  /* print control */
  /* -------------------------------------------------- */

  @media print {
    @page {
      /* suggested printing margin */
      margin: 0.5in;
    }

    /* document and "page" elements */
    html,
    body {
      margin: 0;
      padding: 0;
      width: 100%;
      height: 100%;
    }

    /* "page" element */
    body {
      font-size: 11pt !important;
      line-height: 1.35;
    }

    /* all headings */
    h1,
    h2,
    h3,
    h4,
    h5,
    h6 {
      margin: 15px 0;
    }

    /* figures and tables */
    figure,
    table {
      font-size: 0.85em;
    }

    /* table cells */
    th,
    td {
      padding: 5px;
    }

    /* shrink font awesome icons */
    i.fas,
    i.fab,
    i.far,
    i.fal {
      transform: scale(0.85);
    }

    /* decrease banner margins */
    .banner {
      margin-top: 15px;
      margin-bottom: 15px;
      padding: 15px;
    }

    /* class for centering an element vertically on its own page */
    .page_center {
      margin: auto;
      width: 100%;
      height: 100%;
      display: flex;
      align-items: center;
      vertical-align: middle;
      break-before: page;
      break-after: page;
    }

    /* always insert a page break before the element */
    .page_break_before {
      break-before: page;
    }

    /* always insert a page break after the element */
    .page_break_after {
      break-after: page;
    }

    /* avoid page break before the element */
    .page_break_before_avoid {
      break-before: avoid;
    }

    /* avoid page break after the element */
    .page_break_after_avoid {
      break-after: avoid;
    }

    /* avoid page break inside the element */
    .page_break_inside_avoid {
      break-inside: avoid;
    }
  }

  /* -------------------------------------------------- */
  /* override pandoc css quirks */
  /* -------------------------------------------------- */

  .sourceCode {
    /* prevent unsightly overflow in wide code blocks */
    overflow: auto !important;
  }

  div.sourceCode {
    /* prevent background fill on top-most code block  container */
    background: none !important;
  }

  .sourceCode * {
    /* force consistent line spacing */
    line-height: 1.5 !important;
  }

  div.sourceCode {
    /* style code block margins same as <pre> element */
    margin-top: 20px;
    margin-bottom: 20px;
  }

  /* -------------------------------------------------- */
  /* tablenos */
  /* -------------------------------------------------- */

  /* tablenos wrapper */
  .tablenos {
    width: 100%;
    margin: 20px 0;
  }

  .tablenos > table {
    /* move margins from table to table_wrapper to allow margin collapsing */
    margin: 0;
  }

  @media only screen {
    /* tablenos wrapper */
    .tablenos {
      /* show scrollbar on tables if necessary to prevent overflow */
      overflow-x: auto !important;
    }

    .tablenos th,
    .tablenos td {
      overflow-wrap: unset !important;
      word-break: unset !important;
    }

    /* table in wrapper */
    .tablenos table,
    .tablenos table * {
      /* don't break table words */
      overflow-wrap: normal !important;
    }
  }
</style>
<!-- 
    Plugin Core

    Functions needed for and shared across all first-party plugins.
-->

<script>
  // get element that is target of hash (from link element or url)
  function getHashTarget(link) {
    const hash = link ? link.hash : window.location.hash;
    const id = hash.slice(1);
    let target = document.querySelector(`[id="${id}"]`);
    if (!target) return;

    // if figure or table, modify target to get expected element
    if (id.indexOf("fig:") === 0) target = target.querySelector("figure");
    if (id.indexOf("tbl:") === 0) target = target.querySelector("table");

    return target;
  }

  // get position/dimensions of element or viewport
  function getRectInView(element) {
    let rect = {};
    rect.left = 0;
    rect.top = 0;
    rect.right = document.documentElement.clientWidth;
    rect.bottom = document.documentElement.clientHeight;
    let style = {};

    if (element instanceof HTMLElement) {
      rect = element.getBoundingClientRect();
      style = window.getComputedStyle(element);
    }

    const margin = {};
    margin.left = parseFloat(style.marginLeftWidth) || 0;
    margin.top = parseFloat(style.marginTopWidth) || 0;
    margin.right = parseFloat(style.marginRightWidth) || 0;
    margin.bottom = parseFloat(style.marginBottomWidth) || 0;

    const border = {};
    border.left = parseFloat(style.borderLeftWidth) || 0;
    border.top = parseFloat(style.borderTopWidth) || 0;
    border.right = parseFloat(style.borderRightWidth) || 0;
    border.bottom = parseFloat(style.borderBottomWidth) || 0;

    const newRect = {};
    newRect.left = rect.left + margin.left + border.left;
    newRect.top = rect.top + margin.top + border.top;
    newRect.right = rect.right + margin.right + border.right;
    newRect.bottom = rect.bottom + margin.bottom + border.bottom;
    newRect.width = newRect.right - newRect.left;
    newRect.height = newRect.bottom - newRect.top;

    return newRect;
  }

  // get position of element relative to page
  function getRectInPage(element) {
    const rect = getRectInView(element);
    const body = getRectInView(document.body);

    const newRect = {};
    newRect.left = rect.left - body.left;
    newRect.top = rect.top - body.top;
    newRect.right = rect.right - body.left;
    newRect.bottom = rect.bottom - body.top;
    newRect.width = rect.width;
    newRect.height = rect.height;

    return newRect;
  }

  // get closest element before specified element that matches query
  function firstBefore(element, query) {
    while (element && element !== document.body && !element.matches(query))
      element = element.previousElementSibling || element.parentNode;

    return element;
  }

  // check if element is part of collapsed heading
  function isCollapsed(element) {
    while (element && element !== document.body) {
      if (element.dataset.collapsed === "true") return true;
      element = element.parentNode;
    }
    return false;
  }

  // expand any collapsed parent containers of element if necessary
  function expandElement(element) {
    if (isCollapsed(element)) {
      // accordion plugin
      const heading = firstBefore(element, "h2");
      if (heading) heading.click();
      // details/summary HTML element
      const summary = firstBefore(element, "summary");
      if (summary) summary.click();
    }
  }

  // scroll to and focus element
  function goToElement(element, offset) {
    // expand accordion section if collapsed
    expandElement(element);
    const y =
      getRectInView(element).top -
      getRectInView(document.documentElement).top -
      (offset || 0);

    // trigger any function listening for "onscroll" event
    window.dispatchEvent(new Event("scroll"));
    window.scrollTo(0, y);
    document.activeElement.blur();
    element.focus();
  }

  // get list of elements after a start element up to element matching query
  function nextUntil(element, query, exclude) {
    const elements = [];
    while (((element = element.nextElementSibling), element)) {
      if (element.matches(query)) break;
      if (!element.matches(exclude)) elements.push(element);
    }
    return elements;
  }
</script>
<!--
  Accordion Plugin

  Allows sections of content under h2 headings to be collapsible.
-->

<script type="module">
  // whether to always start expanded ('false'), always start collapsed
  // ('true'), or start collapsed when screen small ('auto')
  const startCollapsed = "auto";

  // start script
  function start() {
    // run through each <h2> heading
    const headings = document.querySelectorAll("h2");
    for (const heading of headings) {
      addArrow(heading);

      // start expanded/collapsed based on option
      if (
        startCollapsed === "true" ||
        (startCollapsed === "auto" && isSmallScreen()) ||
        heading.dataset.collapsed === "true"
      )
        collapseHeading(heading);
      else expandElement(heading);
    }

    // attach hash change listener to window
    window.addEventListener("hashchange", onHashChange);
  }

  // when hash (eg manuscript.html#introduction) changes
  function onHashChange() {
    const target = getHashTarget();
    if (target) goToElement(target);
  }

  // add arrow to heading
  function addArrow(heading) {
    // add arrow button
    const arrow = document.createElement("button");
    arrow.innerHTML = document.querySelector(".icon_angle_down").innerHTML;
    arrow.classList.add("icon_button", "accordion_arrow");
    heading.insertBefore(arrow, heading.firstChild);

    // attach click listener to heading and button
    heading.addEventListener("click", onHeadingClick);
    arrow.addEventListener("click", onArrowClick);
  }

  // determine if on mobile-like device with small screen
  function isSmallScreen() {
    return Math.min(window.innerWidth, window.innerHeight) < 480;
  }

  // when <h2> heading is clicked
  function onHeadingClick(event) {
    // only collapse if <h2> itself is target of click (eg, user did
    // not click on anchor within <h2>)
    if (event.target === this) toggleCollapse(this);
  }

  // when arrow button is clicked
  function onArrowClick() {
    toggleCollapse(this.parentNode);
  }

  // collapse section if expanded, expand if collapsed
  function toggleCollapse(heading) {
    if (heading.dataset.collapsed === "false") collapseHeading(heading);
    else expandElement(heading);
  }

  // elements to exclude from collapse, such as table of contents panel,
  // hypothesis panel, etc
  const exclude = "#toc_panel, div.annotator-frame, #lightbox_overlay";

  // collapse section
  function collapseHeading(heading) {
    heading.setAttribute("data-collapsed", "true");
    const children = getChildren(heading);
    for (const child of children) child.setAttribute("data-collapsed", "true");
  }

  // expand section
  function expandElement(heading) {
    heading.setAttribute("data-collapsed", "false");
    const children = getChildren(heading);
    for (const child of children) child.setAttribute("data-collapsed", "false");
  }

  // get list of elements between this <h2> and next <h2> or <h1>
  // ("children" of the <h2> section)
  function getChildren(heading) {
    return nextUntil(heading, "h2, h1", exclude);
  }

  // start script when document is finished loading
  window.addEventListener("load", start);
</script>

<!-- angle down icon -->

<template class="icon_angle_down">
  <!-- modified from: https://fontawesome.com/icons/angle-down -->
  <svg width="16" height="16" viewBox="0 0 448 512">
    <path
      fill="currentColor"
      d="M207.029 381.476L12.686 187.132c-9.373-9.373-9.373-24.569 0-33.941l22.667-22.667c9.357-9.357 24.522-9.375 33.901-.04L224 284.505l154.745-154.021c9.379-9.335 24.544-9.317 33.901.04l22.667 22.667c9.373 9.373 9.373 24.569 0 33.941L240.971 381.476c-9.373 9.372-24.569 9.372-33.942 0z"
    ></path>
  </svg>
</template>

<style>
  @media only screen {
    /* accordion arrow button */
    .accordion_arrow {
      margin-right: 10px;
    }

    /* arrow icon when <h2> data-collapsed attribute true */
    h2[data-collapsed="true"] > .accordion_arrow > svg {
      transform: rotate(-90deg);
    }

    /* all elements (except <h2>'s) when data-collapsed attribute true */
    *:not(h2)[data-collapsed="true"] {
      display: none;
    }

    /* accordion arrow button when hovered and <h2>'s when hovered */
    .accordion_arrow:hover,
    h2[data-collapsed="true"]:hover,
    h2[data-collapsed="false"]:hover {
      cursor: pointer;
    }
  }

  /* always hide accordion arrow button on print */
  @media only print {
    .accordion_arrow {
      display: none;
    }
  }
</style>
<!--
  Anchors Plugin

  Adds an anchor next to each of a certain type of element that provides a
  human-readable url to that specific item/position in the document (e.g.
  "manuscript.html#abstract"). It also makes it such that scrolling out of view
  of a target removes its identifier from the url.
-->

<script type="module">
  // which types of elements to add anchors next to, in "document.querySelector"
  // format
  const typesQuery =
    'h1, h2, h3, div[id^="fig:"], div[id^="tbl:"], span[id^="eq:"]';

  // start script
  function start() {
    // add anchor to each element of specified types
    const elements = document.querySelectorAll(typesQuery);
    for (const element of elements) addAnchor(element);

    // attach scroll listener to window
    window.addEventListener("scroll", onScroll);
  }

  // when window is scrolled
  function onScroll() {
    // if url has hash and user has scrolled out of view of hash
    // target, remove hash from url
    const tolerance = 100;
    const target = getHashTarget();
    if (target) {
      if (
        target.getBoundingClientRect().top > window.innerHeight + tolerance ||
        target.getBoundingClientRect().bottom < 0 - tolerance
      )
        history.pushState(null, null, " ");
    }
  }

  // add anchor to element
  function addAnchor(element) {
    let addTo; // element to add anchor button to

    // if figure or table, modify withId and addTo to get expected
    // elements
    if (element.id.indexOf("fig:") === 0) {
      addTo = element.querySelector("figcaption");
    } else if (element.id.indexOf("tbl:") === 0) {
      addTo = element.querySelector("caption");
    } else if (element.id.indexOf("eq:") === 0) {
      addTo = element.querySelector(".eqnos-number");
    }

    addTo = addTo || element;
    const id = element.id || null;

    // do not add anchor if element doesn't have assigned id.
    // id is generated by pandoc and is assumed to be unique and
    // human-readable
    if (!id) return;

    // create anchor button
    const anchor = document.createElement("a");
    anchor.innerHTML = document.querySelector(".icon_link").innerHTML;
    anchor.title = "Link to this part of the document";
    anchor.classList.add("icon_button", "anchor");
    anchor.dataset.ignore = "true";
    anchor.href = "#" + id;
    addTo.appendChild(anchor);
  }

  // start script when document is finished loading
  window.addEventListener("load", start);
</script>

<!-- link icon -->

<template class="icon_link">
  <!-- modified from: https://fontawesome.com/icons/link -->
  <svg width="16" height="16" viewBox="0 0 512 512">
    <path
      fill="currentColor"
      d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"
    ></path>
  </svg>
</template>

<style>
  @media only screen {
    /* anchor button */
    .anchor {
      opacity: 0;
      margin-left: 5px;
    }

    /* anchor buttons within <h2>'s */
    h2 .anchor {
      margin-left: 10px;
    }

    /* anchor buttons when hovered/focused and anything containing an anchor button when hovered */
    *:hover > .anchor,
    .anchor:hover,
    .anchor:focus {
      opacity: 1;
    }

    /* anchor button when hovered */
    .anchor:hover {
      cursor: pointer;
    }
  }

  /* always show anchor button on devices with no mouse/hover ability */
  @media (hover: none) {
    .anchor {
      opacity: 1;
    }
  }

  /* always hide anchor button on print */
  @media only print {
    .anchor {
      display: none;
    }
  }
</style>
<!-- 
    Attributes Plugin

    Allows arbitrary HTML attributes to be attached to (almost) any element.
    Place an HTML comment inside or next to the desired element with the content:
    $attribute="value"
-->

<script type="module">
  // start script
  function start() {
    // get list of comments in document
    const comments = findComments();

    for (const comment of comments)
      if (comment.parentElement)
        addAttributes(comment.parentElement, comment.nodeValue.trim());
  }

  // add html attributes to specified element based on string of
  // html attributes and values
  function addAttributes(element, text) {
    // regex's for finding attribute/value pairs in the format of
    // attribute="value" or attribute='value
    const regex2 = /\$([a-zA-Z\-]+)?=\"(.+?)\"/;
    const regex1 = /\$([a-zA-Z\-]+)?=\'(.+?)\'/;

    // loop through attribute/value pairs
    let match;
    while ((match = text.match(regex2) || text.match(regex1))) {
      // get attribute and value from regex capture groups
      let attribute = match[1];
      let value = match[2];

      // remove from string
      text = text.substring(match.index + match[0].length);

      if (!attribute || !value) break;

      // set attribute of parent element
      try {
        element.setAttribute(attribute, value);
      } catch (error) {
        console.log(error);
      }

      // special case for colspan
      if (attribute === "colspan") removeTableCells(element, value);
    }
  }

  // get list of comment elements in document
  function findComments() {
    const comments = [];

    // iterate over comment nodes in document
    function acceptNode(node) {
      return NodeFilter.FILTER_ACCEPT;
    }
    const iterator = document.createNodeIterator(
      document.body,
      NodeFilter.SHOW_COMMENT,
      acceptNode
    );
    let node;
    while ((node = iterator.nextNode())) comments.push(node);

    return comments;
  }

  // remove certain number of cells after specified cell
  function removeTableCells(cell, number) {
    number = parseInt(number);
    if (!number) return;

    // remove elements
    for (; number > 1; number--) {
      if (cell.nextElementSibling) cell.nextElementSibling.remove();
    }
  }

  // start script on DOMContentLoaded instead of load to ensure this plugins
  // runs before other plugins
  window.addEventListener("DOMContentLoaded", start);
</script>
<!--
  Jump to First Plugin

  Adds a button next to each reference entry, figure, and table that jumps the
  page to the first occurrence of a link to that item in the manuscript.
-->

<script type="module">
  // whether to add buttons next to reference entries
  const references = "true";
  // whether to add buttons next to figures
  const figures = "true";
  // whether to add buttons next to tables
  const tables = "true";

  // start script
  function start() {
    if (references !== "false")
      makeButtons(`div[id^="ref-"]`, ".csl-left-margin", "reference");
    if (figures !== "false")
      makeButtons(`div[id^="fig:"]`, "figcaption", "figure");
    if (tables !== "false") makeButtons(`div[id^="tbl:"]`, "caption", "table");
  }

  // when jump button clicked
  function onButtonClick() {
    const first = getFirstOccurrence(this.dataset.id);
    if (!first) return;

    // update url hash so navigating "back" in history will return user to button
    window.location.hash = this.dataset.id;
    // scroll to link
    const timeout = function () {
      goToElement(first, window.innerHeight * 0.5);
    };
    window.setTimeout(timeout, 0);
  }

  // get first occurrence of link to item in document
  function getFirstOccurrence(id) {
    let query = "a";
    query += '[href="#' + id + '"]';
    // exclude buttons, anchor links, toc links, etc
    query += ":not(.button):not(.icon_button):not(.anchor):not(.toc_link)";
    return document.querySelector(query);
  }

  // add button next to each reference entry, figure, or table
  function makeButtons(query, containerQuery, subject) {
    const elements = document.querySelectorAll(query);
    for (const element of elements) {
      const id = element.id;
      const buttonContainer = element.querySelector(containerQuery);
      const first = getFirstOccurrence(id);

      // if can't find link to reference or place to put button, ignore
      if (!first || !buttonContainer) continue;

      // make jump button
      let button = document.createElement("button");
      button.classList.add("icon_button", "jump_arrow");
      button.title = `Jump to the first occurrence of this ${subject} in the document`;
      const icon = document.querySelector(".icon_angle_double_up");
      button.innerHTML = icon.innerHTML;
      button.dataset.id = id;
      button.dataset.ignore = "true";
      button.addEventListener("click", onButtonClick);
      buttonContainer.prepend(button);
    }
  }

  // start script when document is finished loading
  window.addEventListener("load", start);
</script>

<!-- angle double up icon -->

<template class="icon_angle_double_up">
  <!-- modified from: https://fontawesome.com/icons/angle-double-up -->
  <svg width="16" height="16" viewBox="0 0 320 512">
    <path
      fill="currentColor"
      d="M177 255.7l136 136c9.4 9.4 9.4 24.6 0 33.9l-22.6 22.6c-9.4 9.4-24.6 9.4-33.9 0L160 351.9l-96.4 96.4c-9.4 9.4-24.6 9.4-33.9 0L7 425.7c-9.4-9.4-9.4-24.6 0-33.9l136-136c9.4-9.5 24.6-9.5 34-.1zm-34-192L7 199.7c-9.4 9.4-9.4 24.6 0 33.9l22.6 22.6c9.4 9.4 24.6 9.4 33.9 0l96.4-96.4 96.4 96.4c9.4 9.4 24.6 9.4 33.9 0l22.6-22.6c9.4-9.4 9.4-24.6 0-33.9l-136-136c-9.2-9.4-24.4-9.4-33.8 0z"
    ></path>
  </svg>
</template>

<style>
  @media only screen {
    /* jump button */
    .jump_arrow {
      position: relative;
      top: 0.125em;
      margin-right: 5px;
    }
  }

  /* always hide jump button on print */
  @media only print {
    .jump_arrow {
      display: none;
    }
  }
</style>
<!-- 
    Lightbox Plugin

    Makes it such that when a user clicks on an image, the image fills the
    screen and the user can pan/drag/zoom the image and navigate between other
    images in the document.
-->

<script type="module">
  // list of possible zoom/scale factors
  const zooms =
    "0.1, 0.25, 0.333333, 0.5, 0.666666, 0.75, 1, 1.25, 1.5, 1.75, 2, 2.5, 3, 3.5, 4, 5, 6, 7, 8";
  // whether to fit image to view ('fit'), display at 100% and shrink if
  // necessary ('shrink'), or always display at 100% ('100')
  const defaultZoom = "fit";
  // whether to zoom in/out toward center of view ('true') or mouse ('false')
  const centerZoom = "false";

  // start script
  function start() {
    // run through each <img> element
    const imgs = document.querySelectorAll("figure > img");
    let count = 1;
    for (const img of imgs) {
      img.classList.add("lightbox_document_img");
      img.dataset.number = count;
      img.dataset.total = imgs.length;
      img.addEventListener("click", openLightbox);
      count++;
    }

    // attach mouse and key listeners to window
    window.addEventListener("mousemove", onWindowMouseMove);
    window.addEventListener("keyup", onKeyUp);
  }

  // when mouse is moved anywhere in window
  function onWindowMouseMove(event) {
    window.mouseX = event.clientX;
    window.mouseY = event.clientY;
  }

  // when key pressed
  function onKeyUp(event) {
    if (!event || !event.key) return;

    switch (event.key) {
      // trigger click of prev button
      case "ArrowLeft":
        const prevButton = document.getElementById("lightbox_prev_button");
        if (prevButton) prevButton.click();
        break;
      // trigger click of next button
      case "ArrowRight":
        const nextButton = document.getElementById("lightbox_next_button");
        if (nextButton) nextButton.click();
        break;
      // close on esc
      case "Escape":
        closeLightbox();
        break;
    }
  }

  // open lightbox
  function openLightbox() {
    const lightbox = makeLightbox(this);
    if (!lightbox) return;

    blurBody(lightbox);
    document.body.appendChild(lightbox);
  }

  // make lightbox
  function makeLightbox(img) {
    // delete lightbox if it exists, start fresh
    closeLightbox();

    // create screen overlay containing lightbox
    const overlay = document.createElement("div");
    overlay.id = "lightbox_overlay";

    // create image info boxes
    const numberInfo = document.createElement("div");
    const zoomInfo = document.createElement("div");
    numberInfo.id = "lightbox_number_info";
    zoomInfo.id = "lightbox_zoom_info";

    // create container for image
    const imageContainer = document.createElement("div");
    imageContainer.id = "lightbox_image_container";
    const lightboxImg = makeLightboxImg(
      img,
      imageContainer,
      numberInfo,
      zoomInfo
    );
    imageContainer.appendChild(lightboxImg);

    // create bottom container for caption and navigation buttons
    const bottomContainer = document.createElement("div");
    bottomContainer.id = "lightbox_bottom_container";
    const caption = makeCaption(img);
    const prevButton = makePrevButton(img);
    const nextButton = makeNextButton(img);
    bottomContainer.appendChild(prevButton);
    bottomContainer.appendChild(caption);
    bottomContainer.appendChild(nextButton);

    // attach top middle and bottom to overlay
    overlay.appendChild(numberInfo);
    overlay.appendChild(zoomInfo);
    overlay.appendChild(imageContainer);
    overlay.appendChild(bottomContainer);

    return overlay;
  }

  // make <img> object that is intuitively draggable and zoomable
  function makeLightboxImg(sourceImg, container, numberInfoBox, zoomInfoBox) {
    // create copy of source <img>
    const img = sourceImg.cloneNode(true);
    img.classList.remove("lightbox_document_img");
    img.removeAttribute("id");
    img.removeAttribute("width");
    img.removeAttribute("height");
    img.style.position = "unset";
    img.style.margin = "0";
    img.style.padding = "0";
    img.style.width = "";
    img.style.height = "";
    img.style.minWidth = "";
    img.style.minHeight = "";
    img.style.maxWidth = "";
    img.style.maxHeight = "";
    img.id = "lightbox_img";

    // build sorted list of zoomSteps
    const zoomSteps = zooms.split(/[^0-9.]/).map((step) => parseFloat(step));
    zoomSteps.sort((a, b) => a - b);

    // <img> object property variables
    let zoom = 1;
    let translateX = 0;
    let translateY = 0;
    let clickMouseX = undefined;
    let clickMouseY = undefined;
    let clickTranslateX = undefined;
    let clickTranslateY = undefined;

    updateNumberInfo();

    // update image numbers displayed in info box
    function updateNumberInfo() {
      numberInfoBox.innerHTML =
        sourceImg.dataset.number + " of " + sourceImg.dataset.total;
    }

    // update zoom displayed in info box
    function updateZoomInfo() {
      let zoomInfo = zoom * 100;
      if (!Number.isInteger(zoomInfo)) zoomInfo = zoomInfo.toFixed(2);
      zoomInfoBox.innerHTML = zoomInfo + "%";
    }

    // move to closest zoom step above current zoom
    const zoomIn = function () {
      for (const zoomStep of zoomSteps) {
        if (zoomStep > zoom) {
          zoom = zoomStep;
          break;
        }
      }
      updateTransform();
    };

    // move to closest zoom step above current zoom
    const zoomOut = function () {
      zoomSteps.reverse();
      for (const zoomStep of zoomSteps) {
        if (zoomStep < zoom) {
          zoom = zoomStep;
          break;
        }
      }
      zoomSteps.reverse();

      updateTransform();
    };

    // update display of <img> based on scale/translate properties
    const updateTransform = function () {
      // set transform
      img.style.transform =
        "translate(" +
        (translateX || 0) +
        "px," +
        (translateY || 0) +
        "px) scale(" +
        (zoom || 1) +
        ")";

      // get new width/height after scale
      const rect = img.getBoundingClientRect();
      // limit translate
      translateX = Math.max(translateX, -rect.width / 2);
      translateX = Math.min(translateX, rect.width / 2);
      translateY = Math.max(translateY, -rect.height / 2);
      translateY = Math.min(translateY, rect.height / 2);

      // set transform
      img.style.transform =
        "translate(" +
        (translateX || 0) +
        "px," +
        (translateY || 0) +
        "px) scale(" +
        (zoom || 1) +
        ")";

      updateZoomInfo();
    };

    // fit <img> to container
    const fit = function () {
      // no x/y offset, 100% zoom by default
      translateX = 0;
      translateY = 0;
      zoom = 1;

      // widths of <img> and container
      const imgWidth = img.naturalWidth;
      const imgHeight = img.naturalHeight;
      const containerWidth = parseFloat(
        window.getComputedStyle(container).width
      );
      const containerHeight = parseFloat(
        window.getComputedStyle(container).height
      );

      // how much zooming is needed to fit <img> to container
      const xRatio = imgWidth / containerWidth;
      const yRatio = imgHeight / containerHeight;
      const maxRatio = Math.max(xRatio, yRatio);
      const newZoom = 1 / maxRatio;

      // fit <img> to container according to option
      if (defaultZoom === "shrink") {
        if (maxRatio > 1) zoom = newZoom;
      } else if (defaultZoom === "fit") zoom = newZoom;

      updateTransform();
    };

    // when mouse wheel is rolled anywhere in container
    const onContainerWheel = function (event) {
      if (!event) return;

      // let ctrl + mouse wheel to zoom behave as normal
      if (event.ctrlKey) return;

      // prevent normal scroll behavior
      event.preventDefault();
      event.stopPropagation();

      // point around which to scale img
      const viewRect = container.getBoundingClientRect();
      const viewX = (viewRect.left + viewRect.right) / 2;
      const viewY = (viewRect.top + viewRect.bottom) / 2;
      const originX = centerZoom === "true" ? viewX : mouseX;
      const originY = centerZoom === "true" ? viewY : mouseY;

      // get point on image under origin
      const oldRect = img.getBoundingClientRect();
      const oldPercentX = (originX - oldRect.left) / oldRect.width;
      const oldPercentY = (originY - oldRect.top) / oldRect.height;

      // increment/decrement zoom
      if (event.deltaY < 0) zoomIn();
      if (event.deltaY > 0) zoomOut();

      // get offset between previous image point and origin
      const newRect = img.getBoundingClientRect();
      const offsetX = originX - (newRect.left + newRect.width * oldPercentX);
      const offsetY = originY - (newRect.top + newRect.height * oldPercentY);

      // translate image to keep image point under origin
      translateX += offsetX;
      translateY += offsetY;

      // perform translate
      updateTransform();
    };

    // when container is clicked
    function onContainerClick(event) {
      // if container itself is target of click, and not other
      // element above it
      if (event.target === this) closeLightbox();
    }

    // when mouse button is pressed on image
    const onImageMouseDown = function (event) {
      // store original mouse position relative to image
      clickMouseX = window.mouseX;
      clickMouseY = window.mouseY;
      clickTranslateX = translateX;
      clickTranslateY = translateY;
      event.stopPropagation();
      event.preventDefault();
    };

    // when mouse button is released anywhere in window
    const onWindowMouseUp = function (event) {
      // reset original mouse position
      clickMouseX = undefined;
      clickMouseY = undefined;
      clickTranslateX = undefined;
      clickTranslateY = undefined;

      // remove global listener if lightbox removed from document
      if (!document.body.contains(container))
        window.removeEventListener("mouseup", onWindowMouseUp);
    };

    // when mouse is moved anywhere in window
    const onWindowMouseMove = function (event) {
      if (
        clickMouseX === undefined ||
        clickMouseY === undefined ||
        clickTranslateX === undefined ||
        clickTranslateY === undefined
      )
        return;

      // offset image based on original and current mouse position
      translateX = clickTranslateX + window.mouseX - clickMouseX;
      translateY = clickTranslateY + window.mouseY - clickMouseY;
      updateTransform();
      event.preventDefault();

      // remove global listener if lightbox removed from document
      if (!document.body.contains(container))
        window.removeEventListener("mousemove", onWindowMouseMove);
    };

    // when window is resized
    const onWindowResize = function (event) {
      fit();

      // remove global listener if lightbox removed from document
      if (!document.body.contains(container))
        window.removeEventListener("resize", onWindowResize);
    };

    // attach the necessary event listeners
    img.addEventListener("dblclick", fit);
    img.addEventListener("mousedown", onImageMouseDown);
    container.addEventListener("wheel", onContainerWheel);
    container.addEventListener("mousedown", onContainerClick);
    container.addEventListener("touchstart", onContainerClick);
    window.addEventListener("mouseup", onWindowMouseUp);
    window.addEventListener("mousemove", onWindowMouseMove);
    window.addEventListener("resize", onWindowResize);

    // run fit() after lightbox atttached to document and <img> Loaded
    // so needed container and img dimensions available
    img.addEventListener("load", fit);

    return img;
  }

  // make caption
  function makeCaption(img) {
    const caption = document.createElement("div");
    caption.id = "lightbox_caption";
    const captionSource = img.nextElementSibling;
    if (captionSource.tagName.toLowerCase() === "figcaption") {
      const captionCopy = makeCopy(captionSource);
      caption.innerHTML = captionCopy.innerHTML;
    }

    caption.addEventListener("touchstart", function (event) {
      event.stopPropagation();
    });

    return caption;
  }

  // make carbon copy of html dom element
  function makeCopy(source) {
    const sourceCopy = source.cloneNode(true);

    // delete elements marked with ignore (eg anchor and jump buttons)
    const deleteFromCopy = sourceCopy.querySelectorAll('[data-ignore="true"]');
    for (const element of deleteFromCopy) element.remove();

    // delete certain element attributes
    const attributes = [
      "id",
      "data-collapsed",
      "data-selected",
      "data-highlighted",
      "data-glow",
    ];
    for (const attribute of attributes) {
      sourceCopy.removeAttribute(attribute);
      const elements = sourceCopy.querySelectorAll("[" + attribute + "]");
      for (const element of elements) element.removeAttribute(attribute);
    }

    return sourceCopy;
  }

  // make button to jump to previous image in document
  function makePrevButton(img) {
    const prevButton = document.createElement("button");
    prevButton.id = "lightbox_prev_button";
    prevButton.title = "Jump to the previous image in the document [←]";
    prevButton.classList.add("icon_button", "lightbox_button");
    prevButton.innerHTML = document.querySelector(".icon_caret_left").innerHTML;

    // attach click listeners to button
    prevButton.addEventListener("click", function () {
      getPrevImg(img).click();
    });

    return prevButton;
  }

  // make button to jump to next image in document
  function makeNextButton(img) {
    const nextButton = document.createElement("button");
    nextButton.id = "lightbox_next_button";
    nextButton.title = "Jump to the next image in the document [→]";
    nextButton.classList.add("icon_button", "lightbox_button");
    nextButton.innerHTML = document.querySelector(
      ".icon_caret_right"
    ).innerHTML;

    // attach click listeners to button
    nextButton.addEventListener("click", function () {
      getNextImg(img).click();
    });

    return nextButton;
  }

  // get previous image in document
  function getPrevImg(img) {
    const imgs = document.querySelectorAll(".lightbox_document_img");

    // find index of provided img
    let index;
    for (index = 0; index < imgs.length; index++) {
      if (imgs[index] === img) break;
    }

    // wrap index to other side if < 1
    if (index - 1 >= 0) index--;
    else index = imgs.length - 1;
    return imgs[index];
  }

  // get next image in document
  function getNextImg(img) {
    const imgs = document.querySelectorAll(".lightbox_document_img");

    // find index of provided img
    let index;
    for (index = 0; index < imgs.length; index++) {
      if (imgs[index] === img) break;
    }

    // wrap index to other side if > total
    if (index + 1 <= imgs.length - 1) index++;
    else index = 0;
    return imgs[index];
  }

  // close lightbox
  function closeLightbox() {
    focusBody();

    const lightbox = document.getElementById("lightbox_overlay");
    if (lightbox) lightbox.remove();
  }

  // make all elements behind lightbox non-focusable
  function blurBody(overlay) {
    const all = document.querySelectorAll("*");
    for (const element of all) element.tabIndex = -1;
    document.body.classList.add("body_no_scroll");
  }

  // make all elements focusable again
  function focusBody() {
    const all = document.querySelectorAll("*");
    for (const element of all) element.removeAttribute("tabIndex");
    document.body.classList.remove("body_no_scroll");
  }

  // start script when document is finished loading
  window.addEventListener("load", start);
</script>

<!-- caret left icon -->

<template class="icon_caret_left">
  <!-- modified from: https://fontawesome.com/icons/caret-left -->
  <svg width="16" height="16" viewBox="0 0 192 512">
    <path
      fill="currentColor"
      d="M192 127.338v257.324c0 17.818-21.543 26.741-34.142 14.142L29.196 270.142c-7.81-7.81-7.81-20.474 0-28.284l128.662-128.662c12.599-12.6 34.142-3.676 34.142 14.142z"
    ></path>
  </svg>
</template>

<!-- caret right icon -->

<template class="icon_caret_right">
  <!-- modified from: https://fontawesome.com/icons/caret-right -->
  <svg width="16" height="16" viewBox="0 0 192 512">
    <path
      fill="currentColor"
      d="M0 384.662V127.338c0-17.818 21.543-26.741 34.142-14.142l128.662 128.662c7.81 7.81 7.81 20.474 0 28.284L34.142 398.804C21.543 411.404 0 402.48 0 384.662z"
    ></path>
  </svg>
</template>

<style>
  @media only screen {
    /* regular <img> in document when hovered */
    img.lightbox_document_img:hover {
      cursor: pointer;
    }

    .body_no_scroll {
      overflow: hidden !important;
    }

    /* screen overlay */
    #lightbox_overlay {
      display: flex;
      flex-direction: column;
      position: fixed;
      left: 0;
      top: 0;
      right: 0;
      bottom: 0;
      background: rgba(0, 0, 0, 0.75);
      z-index: 3;
    }

    /* middle area containing lightbox image */
    #lightbox_image_container {
      flex-grow: 1;
      display: flex;
      justify-content: center;
      align-items: center;
      overflow: hidden;
      position: relative;
      padding: 20px;
    }

    /* bottom area containing caption */
    #lightbox_bottom_container {
      display: flex;
      justify-content: center;
      align-items: center;
      height: 100px;
      min-height: 100px;
      max-height: 100px;
      background: rgba(0, 0, 0, 0.5);
    }

    /* image number info text box */
    #lightbox_number_info {
      position: absolute;
      color: #ffffff;
      font-weight: 600;
      left: 2px;
      top: 0;
      z-index: 4;
    }

    /* zoom info text box */
    #lightbox_zoom_info {
      position: absolute;
      color: #ffffff;
      font-weight: 600;
      right: 2px;
      top: 0;
      z-index: 4;
    }

    /* copy of image caption */
    #lightbox_caption {
      box-sizing: border-box;
      display: inline-block;
      width: 100%;
      max-height: 100%;
      padding: 10px 0;
      text-align: center;
      overflow-y: auto;
      color: #ffffff;
    }

    /* navigation previous/next button */
    .lightbox_button {
      width: 100px;
      height: 100%;
      min-width: 100px;
      min-height: 100%;
      color: #ffffff;
    }

    /* navigation previous/next button when hovered */
    .lightbox_button:hover {
      background: none !important;
    }

    /* navigation button icon */
    .lightbox_button > svg {
      height: 25px;
    }

    /* figure auto-number */
    #lightbox_caption > span:first-of-type {
      font-weight: bold;
      margin-right: 5px;
    }

    /* lightbox image when hovered */
    #lightbox_img:hover {
      cursor: grab;
    }

    /* lightbox image when grabbed */
    #lightbox_img:active {
      cursor: grabbing;
    }
  }

  /* when on screen < 480px wide */
  @media only screen and (max-width: 480px) {
    /* make navigation buttons skinnier on small screens to make more room for caption text */
    .lightbox_button {
      width: 50px;
      min-width: 50px;
    }
  }

  /* always hide lightbox on print */
  @media only print {
    #lightbox_overlay {
      display: none;
    }
  }
</style>
<!-- 
  Link Highlight Plugin

  Makes it such that when a user hovers or focuses a link, other links that have
  the same target will be highlighted. It also makes it such that when clicking
  a link, the target of the link (eg reference, figure, table) is briefly
  highlighted.
-->

<script type="module">
  // whether to also highlight links that go to external urls
  const externalLinks = "false";
  // whether user must click off to unhighlight instead of just
  // un-hovering
  const clickUnhighlight = "false";
  // whether to also highlight links that are unique
  const highlightUnique = "true";

  // start script
  function start() {
    const links = getLinks();
    for (const link of links) {
      // attach mouse and focus listeners to link
      link.addEventListener("mouseenter", onLinkFocus);
      link.addEventListener("focus", onLinkFocus);
      link.addEventListener("mouseleave", onLinkUnhover);
    }

    // attach click and hash change listeners to window
    window.addEventListener("click", onClick);
    window.addEventListener("touchstart", onClick);
    window.addEventListener("hashchange", onHashChange);

    // run hash change on window load in case user has navigated
    // directly to hash
    onHashChange();
  }

  // when link is focused (tabbed to) or hovered
  function onLinkFocus() {
    highlight(this);
  }

  // when link is unhovered
  function onLinkUnhover() {
    if (clickUnhighlight !== "true") unhighlightAll();
  }

  // when the mouse is clicked anywhere in window
  function onClick(event) {
    unhighlightAll();
  }

  // when hash (eg manuscript.html#introduction) changes
  function onHashChange() {
    const target = getHashTarget();
    if (target) glowElement(target);
  }

  // start glow sequence on an element
  function glowElement(element) {
    const startGlow = function () {
      onGlowEnd();
      element.dataset.glow = "true";
      element.addEventListener("animationend", onGlowEnd);
    };
    const onGlowEnd = function () {
      element.removeAttribute("data-glow");
      element.removeEventListener("animationend", onGlowEnd);
    };
    startGlow();
  }

  // highlight link and all others with same target
  function highlight(link) {
    // force unhighlight all to start fresh
    unhighlightAll();

    // get links with same target
    if (!link) return;
    const sameLinks = getSameLinks(link);

    // if link unique and option is off, exit and don't highlight
    if (sameLinks.length <= 1 && highlightUnique !== "true") return;

    // highlight all same links, and "select" (special highlight) this
    // one
    for (const sameLink of sameLinks) {
      if (sameLink === link) sameLink.setAttribute("data-selected", "true");
      else sameLink.setAttribute("data-highlighted", "true");
    }
  }

  // unhighlight all links
  function unhighlightAll() {
    const links = getLinks();
    for (const link of links) {
      link.setAttribute("data-selected", "false");
      link.setAttribute("data-highlighted", "false");
    }
  }

  // get links with same target
  function getSameLinks(link) {
    const results = [];
    const links = getLinks();
    for (const otherLink of links) {
      if (otherLink.getAttribute("href") === link.getAttribute("href"))
        results.push(otherLink);
    }
    return results;
  }

  // get all links of types we wish to handle
  function getLinks() {
    let query = "a";
    if (externalLinks !== "true") query += '[href^="#"]';
    // exclude buttons, anchor links, toc links, etc
    query += ":not(.button):not(.icon_button):not(.anchor):not(.toc_link)";
    return document.querySelectorAll(query);
  }

  // start script when document is finished loading
  window.addEventListener("load", start);
</script>

<style>
  @media only screen {
    /* anything with data-highlighted attribute true */
    [data-highlighted="true"] {
      background: #ffeb3b;
    }

    /* anything with data-selected attribute true */
    [data-selected="true"] {
      background: #ff8a65 !important;
    }

    /* animation definition for glow */
    @keyframes highlight_glow {
      0% {
        background: none;
      }
      10% {
        background: #bbdefb;
      }
      100% {
        background: none;
      }
    }

    /* anything with data-glow attribute true */
    [data-glow="true"] {
      animation: highlight_glow 2s;
    }
  }
</style>
<!--
  Table of Contents Plugin

  Provides a "table of contents" (toc) panel on the side of the document that
  allows the user to conveniently navigate between sections of the document.
-->

<script type="module">
  // which types of elements to add links for, in "document.querySelector" format
  const typesQuery = "h1, h2, h3";
  // whether toc starts open. use 'true' or 'false', or 'auto' to
  // use 'true' behavior when screen wide enough and 'false' when not
  const startOpen = "false";
  // whether toc closes when clicking on toc link. use 'true' or
  // 'false', or 'auto' to use 'false' behavior when screen wide
  // enough and 'true' when not
  const clickClose = "auto";
  // if list item is more than this many characters, text will be
  // truncated
  const charLimit = "50";
  // whether or not to show bullets next to each toc item
  const bullets = "false";

  // start script
  function start() {
    // make toc panel and populate with entries (links to document
    // sections)
    const panel = makePanel();
    if (!panel) return;
    makeEntries(panel);
    // attach panel to document after making entries, so 'toc' heading
    // in panel isn't included in toc
    document.body.insertBefore(panel, document.body.firstChild);

    // initial panel state
    if (startOpen === "true" || (startOpen === "auto" && !isSmallScreen()))
      openPanel();
    else closePanel();

    // attach click, scroll, and hash change listeners to window
    window.addEventListener("click", onClick);
    window.addEventListener("scroll", onScroll);
    window.addEventListener("hashchange", onScroll);
    window.addEventListener("keyup", onKeyUp);
    onScroll();

    // add class to push document body down out of way of toc button
    document.body.classList.add("toc_body_nudge");
  }

  // determine if screen wide enough to fit toc panel
  function isSmallScreen() {
    // in default theme:
    // 816px = 8.5in = width of "page" (<body>) element
    // 260px = min width of toc panel (*2 for both sides of <body>)
    return window.innerWidth < 816 + 260 * 2;
  }

  // when mouse is clicked anywhere in window
  function onClick() {
    if (isSmallScreen()) closePanel();
  }

  // when window is scrolled or hash changed
  function onScroll() {
    highlightViewed();
  }

  // when key pressed
  function onKeyUp(event) {
    if (!event || !event.key) return;

    // close on esc
    if (event.key === "Escape") closePanel();
  }

  // find entry of currently viewed document section in toc and highlight
  function highlightViewed() {
    const firstId = getFirstInView(typesQuery);

    // get toc entries (links), unhighlight all, then highlight viewed
    const list = document.getElementById("toc_list");
    if (!firstId || !list) return;
    const links = list.querySelectorAll("a");
    for (const link of links) link.dataset.viewing = "false";
    const link = list.querySelector('a[href="#' + firstId + '"]');
    if (!link) return;
    link.dataset.viewing = "true";
  }

  // get first or previous toc listed element in top half of view
  function getFirstInView(query) {
    // get all elements matching query and with id
    const elements = document.querySelectorAll(query);
    const elementsWithIds = [];
    for (const element of elements) {
      if (element.id) elementsWithIds.push(element);
    }

    // get first or previous element in top half of view
    for (let i = 0; i < elementsWithIds.length; i++) {
      const element = elementsWithIds[i];
      const prevElement = elementsWithIds[Math.max(0, i - 1)];
      if (element.getBoundingClientRect().top >= 0) {
        if (element.getBoundingClientRect().top < window.innerHeight / 2)
          return element.id;
        else return prevElement.id;
      }
    }
  }

  // make panel
  function makePanel() {
    // create panel
    const panel = document.createElement("div");
    panel.id = "toc_panel";
    if (bullets === "true") panel.dataset.bullets = "true";

    // create header
    const header = document.createElement("div");
    header.id = "toc_header";

    // create toc button
    const button = document.createElement("button");
    button.id = "toc_button";
    button.innerHTML = document.querySelector(".icon_th_list").innerHTML;
    button.title = "Table of Contents";
    button.classList.add("icon_button");

    // create header text
    const text = document.createElement("h4");
    text.innerHTML = "Table of Contents";

    // create container for toc list
    const list = document.createElement("div");
    list.id = "toc_list";

    // attach click listeners
    panel.addEventListener("click", onPanelClick);
    header.addEventListener("click", onHeaderClick);
    button.addEventListener("click", onButtonClick);

    // attach elements
    header.appendChild(button);
    header.appendChild(text);
    panel.appendChild(header);
    panel.appendChild(list);

    return panel;
  }

  // create toc entries (links) to each element of the specified types
  function makeEntries(panel) {
    const elements = document.querySelectorAll(typesQuery);
    for (const element of elements) {
      // do not add link if element doesn't have assigned id
      if (!element.id) continue;

      // create link/list item
      const link = document.createElement("a");
      link.classList.add("toc_link");
      switch (element.tagName.toLowerCase()) {
        case "h1":
          link.dataset.level = "1";
          break;
        case "h2":
          link.dataset.level = "2";
          break;
        case "h3":
          link.dataset.level = "3";
          break;
        case "h4":
          link.dataset.level = "4";
          break;
      }
      link.title = element.innerText;
      let text = element.innerText;
      if (text.length > charLimit) text = text.slice(0, charLimit) + "...";
      link.innerHTML = text;
      link.href = "#" + element.id;
      link.addEventListener("click", onLinkClick);

      // attach link
      panel.querySelector("#toc_list").appendChild(link);
    }
  }

  // when panel is clicked
  function onPanelClick(event) {
    // stop click from propagating to window/document and closing panel
    event.stopPropagation();
  }

  // when header itself is clicked
  function onHeaderClick(event) {
    togglePanel();
  }

  // when button is clicked
  function onButtonClick(event) {
    togglePanel();
    // stop header underneath button from also being clicked
    event.stopPropagation();
  }

  // when link is clicked
  function onLinkClick(event) {
    if (clickClose === "true" || (clickClose === "auto" && isSmallScreen()))
      closePanel();
    else openPanel();
  }

  // open panel if closed, close if opened
  function togglePanel() {
    const panel = document.getElementById("toc_panel");
    if (!panel) return;

    if (panel.dataset.open === "true") closePanel();
    else openPanel();
  }

  // open panel
  function openPanel() {
    const panel = document.getElementById("toc_panel");
    if (panel) panel.dataset.open = "true";
  }

  // close panel
  function closePanel() {
    const panel = document.getElementById("toc_panel");
    if (panel) panel.dataset.open = "false";
  }

  // start script when document is finished loading
  window.addEventListener("load", start);
</script>

<!-- th list icon -->

<template class="icon_th_list">
  <!-- modified from: https://fontawesome.com/icons/th-list -->
  <svg width="16" height="16" viewBox="0 0 512 512" tabindex="-1">
    <path
      fill="currentColor"
      d="M96 96c0 26.51-21.49 48-48 48S0 122.51 0 96s21.49-48 48-48 48 21.49 48 48zM48 208c-26.51 0-48 21.49-48 48s21.49 48 48 48 48-21.49 48-48-21.49-48-48-48zm0 160c-26.51 0-48 21.49-48 48s21.49 48 48 48 48-21.49 48-48-21.49-48-48-48zm96-236h352c8.837 0 16-7.163 16-16V76c0-8.837-7.163-16-16-16H144c-8.837 0-16 7.163-16 16v40c0 8.837 7.163 16 16 16zm0 160h352c8.837 0 16-7.163 16-16v-40c0-8.837-7.163-16-16-16H144c-8.837 0-16 7.163-16 16v40c0 8.837 7.163 16 16 16zm0 160h352c8.837 0 16-7.163 16-16v-40c0-8.837-7.163-16-16-16H144c-8.837 0-16 7.163-16 16v40c0 8.837 7.163 16 16 16z"
      tabindex="-1"
    ></path>
  </svg>
</template>

<style>
  @media only screen {
    /* toc panel */
    #toc_panel {
      box-sizing: border-box;
      position: fixed;
      top: 0;
      left: 0;
      background: #ffffff;
      box-shadow: 0 0 20px rgba(0, 0, 0, 0.05);
      z-index: 2;
    }

    /* toc panel when closed */
    #toc_panel[data-open="false"] {
      min-width: 60px;
      width: 60px;
      height: 60px;
      border-right: solid 1px #bdbdbd;
      border-bottom: solid 1px #bdbdbd;
    }

    /* toc panel when open */
    #toc_panel[data-open="true"] {
      min-width: 260px;
      max-width: 480px;
      /* keep panel edge consistent distance away from "page" edge */
      width: calc(((100vw - 8.5in) / 2) - 30px - 40px);
      bottom: 0;
      border-right: solid 1px #bdbdbd;
    }

    /* toc panel header */
    #toc_header {
      box-sizing: border-box;
      display: flex;
      flex-direction: row;
      align-items: center;
      height: 60px;
      margin: 0;
      padding: 20px;
    }

    /* toc panel header when hovered */
    #toc_header:hover {
      cursor: pointer;
    }

    /* toc panel header when panel open */
    #toc_panel[data-open="true"] > #toc_header {
      border-bottom: solid 1px #bdbdbd;
    }

    /* toc open/close header button */
    #toc_button {
      margin-right: 20px;
    }

    /* hide toc list and header text when closed */
    #toc_panel[data-open="false"] > #toc_header > *:not(#toc_button),
    #toc_panel[data-open="false"] > #toc_list {
      display: none;
    }

    /* toc list of entries */
    #toc_list {
      box-sizing: border-box;
      width: 100%;
      padding: 20px;
      position: absolute;
      top: calc(60px + 1px);
      bottom: 0;
      overflow: auto;
    }

    /* toc entry, link to section in document */
    .toc_link {
      display: block;
      padding: 5px;
      position: relative;
      font-weight: 600;
      text-decoration: none;
    }

    /* toc entry when hovered or when "viewed" */
    .toc_link:hover,
    .toc_link[data-viewing="true"] {
      background: #f5f5f5;
    }

    /* toc entry, level 1 indentation */
    .toc_link[data-level="1"] {
      margin-left: 0;
    }

    /* toc entry, level 2 indentation */
    .toc_link[data-level="2"] {
      margin-left: 20px;
    }

    /* toc entry, level 3 indentation */
    .toc_link[data-level="3"] {
      margin-left: 40px;
    }

    /* toc entry, level 4 indentation */
    .toc_link[data-level="4"] {
      margin-left: 60px;
    }

    /* toc entry bullets */
    #toc_panel[data-bullets="true"] .toc_link[data-level]:before {
      position: absolute;
      left: -15px;
      top: -1px;
      font-size: 1.5em;
    }

    /* toc entry, level 2 bullet */
    #toc_panel[data-bullets="true"] .toc_link[data-level="2"]:before {
      content: "\2022";
    }

    /* toc entry, level 3 bullet */
    #toc_panel[data-bullets="true"] .toc_link[data-level="3"]:before {
      content: "\25AB";
    }

    /* toc entry, level 4 bullet */
    #toc_panel[data-bullets="true"] .toc_link[data-level="4"]:before {
      content: "-";
    }
  }

  /* when on screen < 8.5in wide */
  @media only screen and (max-width: 8.5in) {
    /* push <body> ("page") element down to make room for toc icon */
    .toc_body_nudge {
      padding-top: 60px;
    }

    /* toc icon when panel closed and not hovered */
    #toc_panel[data-open="false"]:not(:hover) {
      background: rgba(255, 255, 255, 0.75);
    }
  }

  /* always hide toc panel on print */
  @media only print {
    #toc_panel {
      display: none;
    }
  }
</style>
<!-- 
  Tooltips Plugin

  Makes it such that when the user hovers or focuses a link to a citation or
  figure, a tooltip appears with a preview of the reference content, along with
  arrows to navigate between instances of the same reference in the document.
-->

<script type="module">
  // whether user must click off to close tooltip instead of just un-hovering
  const clickClose = "false";
  // delay (in ms) between opening and closing tooltip
  const delay = "100";

  // start script
  function start() {
    const links = getLinks();
    for (const link of links) {
      // attach hover and focus listeners to link
      link.addEventListener("mouseover", onLinkHover);
      link.addEventListener("mouseleave", onLinkUnhover);
      link.addEventListener("focus", onLinkFocus);
      link.addEventListener("touchend", onLinkTouch);
    }

    // attach mouse, key, and resize listeners to window
    window.addEventListener("mousedown", onClick);
    window.addEventListener("touchstart", onClick);
    window.addEventListener("keyup", onKeyUp);
    window.addEventListener("resize", onResize);
  }

  // when link is hovered
  function onLinkHover() {
    // function to open tooltip
    const delayOpenTooltip = function () {
      openTooltip(this);
    }.bind(this);

    // run open function after delay
    this.openTooltipTimer = window.setTimeout(delayOpenTooltip, delay);
  }

  // when mouse leaves link
  function onLinkUnhover() {
    // cancel opening tooltip
    window.clearTimeout(this.openTooltipTimer);

    // don't close on unhover if option specifies
    if (clickClose === "true") return;

    // function to close tooltip
    const delayCloseTooltip = function () {
      // if tooltip open and if mouse isn't over tooltip, close
      const tooltip = document.getElementById("tooltip");
      if (tooltip && !tooltip.matches(":hover")) closeTooltip();
    };

    // run close function after delay
    this.closeTooltipTimer = window.setTimeout(delayCloseTooltip, delay);
  }

  // when link is focused (tabbed to)
  function onLinkFocus(event) {
    openTooltip(this);
  }

  // when link is touched on touch screen
  function onLinkTouch(event) {
    // attempt to force hover state on first tap always, and trigger
    // regular link click (and navigation) on second tap
    if (event.target === document.activeElement) event.target.click();
    else {
      document.activeElement.blur();
      event.target.focus();
    }
    if (event.cancelable) event.preventDefault();
    event.stopPropagation();
    return false;
  }

  // when mouse is clicked anywhere in window
  function onClick(event) {
    closeTooltip();
  }

  // when key pressed
  function onKeyUp(event) {
    if (!event || !event.key) return;

    switch (event.key) {
      // trigger click of prev button
      case "ArrowLeft":
        const prevButton = document.getElementById("tooltip_prev_button");
        if (prevButton) prevButton.click();
        break;
      // trigger click of next button
      case "ArrowRight":
        const nextButton = document.getElementById("tooltip_next_button");
        if (nextButton) nextButton.click();
        break;
      // close on esc
      case "Escape":
        closeTooltip();
        break;
    }
  }

  // when window is resized or zoomed
  function onResize() {
    closeTooltip();
  }

  // get all links of types we wish to handle
  function getLinks() {
    const queries = [];
    // exclude buttons, anchor links, toc links, etc
    const exclude =
      ":not(.button):not(.icon_button):not(.anchor):not(.toc_link)";
    queries.push('a[href^="#ref-"]' + exclude); // citation links
    queries.push('a[href^="#fig:"]' + exclude); // figure links
    const query = queries.join(", ");
    return document.querySelectorAll(query);
  }

  // get links with same target, get index of link in set, get total
  // same links
  function getSameLinks(link) {
    const sameLinks = [];
    const links = getLinks();
    for (const otherLink of links) {
      if (otherLink.getAttribute("href") === link.getAttribute("href"))
        sameLinks.push(otherLink);
    }

    return {
      elements: sameLinks,
      index: sameLinks.indexOf(link),
      total: sameLinks.length,
    };
  }

  // open tooltip
  function openTooltip(link) {
    // delete tooltip if it exists, start fresh
    closeTooltip();

    // make tooltip element
    const tooltip = makeTooltip(link);

    // if source couldn't be found and tooltip not made, exit
    if (!tooltip) return;

    // make navbar elements
    const navBar = makeNavBar(link);
    if (navBar) tooltip.firstElementChild.appendChild(navBar);

    // attach tooltip to page
    document.body.appendChild(tooltip);

    // position tooltip
    const position = function () {
      positionTooltip(link);
    };
    position();

    // if tooltip contains images, position again after they've loaded
    const imgs = tooltip.querySelectorAll("img");
    for (const img of imgs) img.addEventListener("load", position);
  }

  // close (delete) tooltip
  function closeTooltip() {
    const tooltip = document.getElementById("tooltip");
    if (tooltip) tooltip.remove();
  }

  // make tooltip
  function makeTooltip(link) {
    // get target element that link points to
    const source = getSource(link);

    // if source can't be found, exit
    if (!source) return;

    // create new tooltip
    const tooltip = document.createElement("div");
    tooltip.id = "tooltip";
    const tooltipContent = document.createElement("div");
    tooltipContent.id = "tooltip_content";
    tooltip.appendChild(tooltipContent);

    // make copy of source node and put in tooltip
    const sourceCopy = makeCopy(source);
    tooltipContent.appendChild(sourceCopy);

    // attach mouse event listeners
    tooltip.addEventListener("click", onTooltipClick);
    tooltip.addEventListener("mousedown", onTooltipClick);
    tooltip.addEventListener("touchstart", onTooltipClick);
    tooltip.addEventListener("mouseleave", onTooltipUnhover);

    // (for interaction with lightbox plugin)
    // transfer click on tooltip copied img to original img
    const sourceImg = source.querySelector("img");
    const sourceCopyImg = sourceCopy.querySelector("img");
    if (sourceImg && sourceCopyImg) {
      const clickImg = function () {
        sourceImg.click();
        closeTooltip();
      };
      sourceCopyImg.addEventListener("click", clickImg);
    }

    return tooltip;
  }

  // make carbon copy of html dom element
  function makeCopy(source) {
    const sourceCopy = source.cloneNode(true);

    // delete elements marked with ignore (eg anchor and jump buttons)
    const deleteFromCopy = sourceCopy.querySelectorAll('[data-ignore="true"]');
    for (const element of deleteFromCopy) element.remove();

    // delete certain element attributes
    const attributes = [
      "id",
      "data-collapsed",
      "data-selected",
      "data-highlighted",
      "data-glow",
      "class",
    ];
    for (const attribute of attributes) {
      sourceCopy.removeAttribute(attribute);
      const elements = sourceCopy.querySelectorAll("[" + attribute + "]");
      for (const element of elements) element.removeAttribute(attribute);
    }

    return sourceCopy;
  }

  // when tooltip is clicked
  function onTooltipClick(event) {
    // when user clicks on tooltip, stop click from transferring
    // outside of tooltip (eg, click off to close tooltip, or eg click
    // off to unhighlight same refs)
    event.stopPropagation();
  }

  // when tooltip is unhovered
  function onTooltipUnhover(event) {
    if (clickClose === "true") return;

    // make sure new mouse/touch/focus no longer over tooltip or any
    // element within it
    const tooltip = document.getElementById("tooltip");
    if (!tooltip) return;
    if (this.contains(event.relatedTarget)) return;

    closeTooltip();
  }

  // make nav bar to go betwen prev/next instances of same reference
  function makeNavBar(link) {
    // find other links to the same source
    const sameLinks = getSameLinks(link);

    // don't show nav bar when singular reference
    if (sameLinks.total <= 1) return;

    // find prev/next links with same target
    const prevLink = getPrevLink(link, sameLinks);
    const nextLink = getNextLink(link, sameLinks);

    // create nav bar
    const navBar = document.createElement("div");
    navBar.id = "tooltip_nav_bar";
    const text = sameLinks.index + 1 + " of " + sameLinks.total;

    // create nav bar prev/next buttons
    const prevButton = document.createElement("button");
    const nextButton = document.createElement("button");
    prevButton.id = "tooltip_prev_button";
    nextButton.id = "tooltip_next_button";
    prevButton.title =
      "Jump to the previous occurence of this item in the document [←]";
    nextButton.title =
      "Jump to the next occurence of this item in the document [→]";
    prevButton.classList.add("icon_button");
    nextButton.classList.add("icon_button");
    prevButton.innerHTML = document.querySelector(".icon_caret_left").innerHTML;
    nextButton.innerHTML =
      document.querySelector(".icon_caret_right").innerHTML;
    navBar.appendChild(prevButton);
    navBar.appendChild(document.createTextNode(text));
    navBar.appendChild(nextButton);

    // attach click listeners to buttons
    prevButton.addEventListener("click", function () {
      onPrevNextClick(link, prevLink);
    });
    nextButton.addEventListener("click", function () {
      onPrevNextClick(link, nextLink);
    });

    return navBar;
  }

  // get previous link with same target
  function getPrevLink(link, sameLinks) {
    if (!sameLinks) sameLinks = getSameLinks(link);
    // wrap index to other side if < 1
    let index;
    if (sameLinks.index - 1 >= 0) index = sameLinks.index - 1;
    else index = sameLinks.total - 1;
    return sameLinks.elements[index];
  }

  // get next link with same target
  function getNextLink(link, sameLinks) {
    if (!sameLinks) sameLinks = getSameLinks(link);
    // wrap index to other side if > total
    let index;
    if (sameLinks.index + 1 <= sameLinks.total - 1) index = sameLinks.index + 1;
    else index = 0;
    return sameLinks.elements[index];
  }

  // get element that is target of link or url hash
  function getSource(link) {
    const hash = link ? link.hash : window.location.hash;
    const id = hash.slice(1);
    let target = document.querySelector('[id="' + id + '"]');
    if (!target) return;

    // if ref or figure, modify target to get expected element
    if (id.indexOf("ref-") === 0) target = target.querySelector(":nth-child(2)");
    else if (id.indexOf("fig:") === 0) target = target.querySelector("figure");

    return target;
  }

  // when prev/next arrow button is clicked
  function onPrevNextClick(link, prevNextLink) {
    if (link && prevNextLink)
      goToElement(prevNextLink, window.innerHeight * 0.5);
  }

  // scroll to and focus element
  function goToElement(element, offset) {
    // expand accordion section if collapsed
    expandElement(element);
    const y =
      getRectInView(element).top -
      getRectInView(document.documentElement).top -
      (offset || 0);
    // trigger any function listening for "onscroll" event
    window.dispatchEvent(new Event("scroll"));
    window.scrollTo(0, y);
    document.activeElement.blur();
    element.focus();
  }

  // determine position to place tooltip based on link position in
  // viewport and tooltip size
  function positionTooltip(link, left, top) {
    const tooltipElement = document.getElementById("tooltip");
    if (!tooltipElement) return;

    // get convenient vars for position/dimensions of
    // link/tooltip/page/view
    link = getRectInPage(link);
    const tooltip = getRectInPage(tooltipElement);
    const view = getRectInPage();

    // horizontal positioning
    if (left)
      // use explicit value
      left = left;
    else if (link.left + tooltip.width < view.right)
      // fit tooltip to right of link
      left = link.left;
    else if (link.right - tooltip.width > view.left)
      // fit tooltip to left of link
      left = link.right - tooltip.width;
    // center tooltip in view
    else left = (view.right - view.left) / 2 - tooltip.width / 2;

    // vertical positioning
    if (top)
      // use explicit value
      top = top;
    else if (link.top - tooltip.height > view.top)
      // fit tooltip above link
      top = link.top - tooltip.height;
    else if (link.bottom + tooltip.height < view.bottom)
      // fit tooltip below link
      top = link.bottom;
    else {
      // center tooltip in view
      top = view.top + view.height / 2 - tooltip.height / 2;
      // nudge off of link to left/right if possible
      if (link.right + tooltip.width < view.right) left = link.right;
      else if (link.left - tooltip.width > view.left)
        left = link.left - tooltip.width;
    }

    tooltipElement.style.left = left + "px";
    tooltipElement.style.top = top + "px";
  }

  // start script when document is finished loading
  window.addEventListener("load", start);
</script>

<!-- caret left icon -->

<template class="icon_caret_left">
  <!-- modified from: https://fontawesome.com/icons/caret-left -->
  <svg width="16" height="16" viewBox="0 0 192 512">
    <path
      fill="currentColor"
      d="M192 127.338v257.324c0 17.818-21.543 26.741-34.142 14.142L29.196 270.142c-7.81-7.81-7.81-20.474 0-28.284l128.662-128.662c12.599-12.6 34.142-3.676 34.142 14.142z"
    ></path>
  </svg>
</template>

<!-- caret right icon -->

<template class="icon_caret_right">
  <!-- modified from: https://fontawesome.com/icons/caret-right -->
  <svg width="16" height="16" viewBox="0 0 192 512">
    <path
      fill="currentColor"
      d="M0 384.662V127.338c0-17.818 21.543-26.741 34.142-14.142l128.662 128.662c7.81 7.81 7.81 20.474 0 28.284L34.142 398.804C21.543 411.404 0 402.48 0 384.662z"
    ></path>
  </svg>
</template>

<style>
  @media only screen {
    /* tooltip container */
    #tooltip {
      position: absolute;
      width: 50%;
      min-width: 240px;
      max-width: 75%;
      z-index: 1;
    }

    /* tooltip content */
    #tooltip_content {
      margin-bottom: 5px;
      padding: 20px;
      border-radius: 5px;
      border: solid 1px #bdbdbd;
      box-shadow: 0 0 20px rgba(0, 0, 0, 0.05);
      background: #ffffff;
      overflow-wrap: break-word;
    }

    /* tooltip copy of paragraphs and figures */
    #tooltip_content > p,
    #tooltip_content > figure {
      margin: 0;
      max-height: 320px;
      overflow-y: auto;
    }

    /* tooltip copy of <img> */
    #tooltip_content > figure > img,
    #tooltip_content > figure > svg {
      max-height: 260px;
    }

    /* navigation bar */
    #tooltip_nav_bar {
      margin-top: 10px;
      text-align: center;
    }

    /* navigation bar previous/next buton */
    #tooltip_nav_bar > .icon_button {
      position: relative;
      top: 3px;
    }

    /* navigation bar previous button */
    #tooltip_nav_bar > .icon_button:first-of-type {
      margin-right: 5px;
    }

    /* navigation bar next button */
    #tooltip_nav_bar > .icon_button:last-of-type {
      margin-left: 5px;
    }
  }

  /* always hide tooltip on print */
  @media only print {
    #tooltip {
      display: none;
    }
  }
</style>
<!--
  Analytics Plugin (third-party) 
  
  Copy and paste code from Google Analytics or similar service here.
-->
<!-- 
  Annotations Plugin

  Allows public annotation of the  manuscript. See https://web.hypothes.is/.
-->

<script type="module">
  // configuration
  window.hypothesisConfig = function () {
    return {
      branding: {
        accentColor: "#2196f3",
        appBackgroundColor: "#f8f8f8",
        ctaBackgroundColor: "#f8f8f8",
        ctaTextColor: "#000000",
        selectionFontFamily: "Open Sans, Helvetica, sans serif",
        annotationFontFamily: "Open Sans, Helvetica, sans serif",
      },
    };
  };

  // hypothesis client script
  const embed = "https://hypothes.is/embed.js";
  // hypothesis annotation count query url
  const query = "https://api.hypothes.is/api/search?limit=0&url=";

  // start script
  function start() {
    const button = makeButton();
    document.body.insertBefore(button, document.body.firstChild);
    insertCount(button);
  }

  // make button
  function makeButton() {
    // create button
    const button = document.createElement("button");
    button.id = "hypothesis_button";
    button.innerHTML = document.querySelector(".icon_hypothesis").innerHTML;
    button.title = "Hypothesis annotations";
    button.classList.add("icon_button");

    function onClick(event) {
      onButtonClick(event, button);
    }

    // attach click listeners
    button.addEventListener("click", onClick);

    return button;
  }

  // insert annotations count
  async function insertCount(button) {
    // get annotation count from Hypothesis based on url
    let count = "-";
    try {
      const canonical = document.querySelector('link[rel="canonical"]');
      const location = window.location;
      const url = encodeURIComponent((canonical || location).href);
      const response = await fetch(query + url);
      const json = await response.json();
      count = json.total || "-";
    } catch (error) {
      console.log(error);
    }

    // put count into button
    const counter = document.createElement("span");
    counter.id = "hypothesis_count";
    counter.innerHTML = count;
    button.title = "View " + count + " Hypothesis annotations";
    button.append(counter);
  }

  // when button is clicked
  function onButtonClick(event, button) {
    const script = document.createElement("script");
    script.src = embed;
    document.body.append(script);
    button.remove();
  }

  // start script when document is finished loading
  window.addEventListener("load", start);
</script>

<!-- hypothesis icon -->

<template class="icon_hypothesis">
  <!-- modified from: https://simpleicons.org/icons/hypothesis.svg / https://git.io/Jf1VB -->
  <svg width="16" height="16" viewBox="0 0 24 24" tabindex="-1">
    <path
      fill="currentColor"
      d="M3.43 0C2.5 0 1.72 .768 1.72 1.72V18.86C1.72 19.8 2.5 20.57 3.43 20.57H9.38L12 24L14.62 20.57H20.57C21.5 20.57 22.29 19.8 22.29 18.86V1.72C22.29 .77 21.5 0 20.57 0H3.43M5.14 3.43H7.72V9.43S8.58 7.72 10.28 7.72C12 7.72 13.74 8.57 13.74 11.24V17.14H11.16V12C11.16 10.61 10.28 10.07 9.43 10.29C8.57 10.5 7.72 11.41 7.72 13.29V17.14H5.14V3.43M18 13.72C18.95 13.72 19.72 14.5 19.72 15.42A1.71 1.71 0 0 1 18 17.13A1.71 1.71 0 0 1 16.29 15.42C16.29 14.5 17.05 13.71 18 13.71Z"
      tabindex="-1"
    ></path>
  </svg>
</template>

<style>
  /* hypothesis activation button */
  #hypothesis_button {
    box-sizing: border-box;
    position: fixed;
    top: 0;
    right: 0;
    width: 60px;
    height: 60px;
    background: #ffffff;
    border-radius: 0;
    border-left: solid 1px #bdbdbd;
    border-bottom: solid 1px #bdbdbd;
    box-shadow: 0 0 20px rgba(0, 0, 0, 0.05);
    z-index: 2;
  }

  /* hypothesis button svg */
  #hypothesis_button > svg {
    position: relative;
    top: -4px;
  }

  /* hypothesis annotation count */
  #hypothesis_count {
    position: absolute;
    left: 0;
    right: 0;
    bottom: 5px;
  }

  /* side panel */
  .annotator-frame {
    width: 280px !important;
  }

  /* match highlight color to rest of theme */
  .annotator-highlights-always-on .annotator-hl {
    background-color: #ffeb3b !important;
  }

  /* match focused color to rest of theme */
  .annotator-hl.annotator-hl-focused {
    background-color: #ff8a65 !important;
  }

  /* match bucket bar color to rest of theme */
  .annotator-bucket-bar {
    background: #f5f5f5 !important;
  }

  /* always hide button, toolbar, and tooltip on print */
  @media only print {
    #hypothesis_button {
      display: none;
    }

    .annotator-frame {
      display: none !important;
    }

    hypothesis-adder {
      display: none !important;
    }
  }
</style>
<!-- 
  Mathjax Plugin (third-party) 

  Allows the proper rendering of math/equations written in LaTeX.
  See https://www.mathjax.org/.
-->

<script type="text/x-mathjax-config">
  // configuration
  MathJax.Hub.Config({
    "CommonHTML": { linebreaks: { automatic: true } },
    "HTML-CSS": { linebreaks: { automatic: true } },
    "SVG": { linebreaks: { automatic: true } },
    "fast-preview": { disabled: true }
  });
</script>

<script
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"
  integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A=="
  crossorigin="anonymous"
></script>

<style>
  /* mathjax containers */
  .math.display > span:not(.MathJax_Preview) {
    /* turn inline element (no dimensions) into block (allows fixed width and thus scrolling) */
    display: flex !important;
    overflow-x: auto !important;
    overflow-y: hidden !important;
    justify-content: center;
    align-items: center;
    margin: 0 !important;
  }

  /* right click menu */
  .MathJax_Menu {
    border-radius: 5px !important;
    border: solid 1px #bdbdbd !important;
    box-shadow: none !important;
  }

  /* equation auto-number */
  span[id^="eq:"] > span.math.display + span {
    font-weight: 600;
  }

  /* equation */
  span[id^="eq:"] > span.math.display > span {
    /* nudge to make room for equation auto-number and anchor */
    margin-right: 60px !important;
  }
</style>
</body>
</html>
